{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22ecf72b",
   "metadata": {},
   "source": [
    "# Auto-encodeurs\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Un autoencodeur est algorithme entraîné de manière non supervisée à\n",
    "reproduire son entrée $\\mathbf{x}\\in \\mathcal{X}$. Il peut être vu\n",
    "({numref}`ae1`) comme\n",
    "composé de deux parties : un *encodeur* `E` qui transforme $\\mathbf{x}$\n",
    "en un code déterministe\n",
    "$\\mathbf{h} = f(\\mathbf{x} ; \\mathbf{w_E})\\in \\mathcal{H}$ ou une\n",
    "distribution\n",
    "$\\textbf p_{encodeur} (\\mathbf{h}|\\mathbf{x},\\mathbf{w_E})$,\n",
    "qui représente l'entrée ; et un *décodeur* `D` qui produit une\n",
    "reconstruction déterministe\n",
    "$\\mathbf{\\hat{x}} = g(\\mathbf{h} ; \\mathbf{w_D})$ de $\\mathbf{x}$ ou une\n",
    "distribution\n",
    "$\\textbf p_{decodeur} (\\mathbf{x}|\\mathbf{h},\\mathbf{w_D})$.\n",
    "Les vecteurs $\\mathbf{w_E}$ et $\\mathbf{w_D}$ sont les paramètres de `E`\n",
    "et `D`. Le plus souvent, l'encodeur et le décodeur sont des réseaux de\n",
    "neurones (perceptrons multicouches plus ou moins profonds, réseaux\n",
    "convolutifs ou récurrents,...) et les paramètres sont donc les poids de ces réseaux. À ce\n",
    "titre, l'entraînement peut être réalisé avec les mêmes algorithmes que\n",
    "ceux utilisés dans les réseaux de neurones classiques.\\\n",
    "Entraîner un autoencodeur à reconstruire\n",
    "$g\\circ f(\\mathbf{x})=\\mathbf{x}$ pour tout $\\mathbf{x}$ n'est pas utile\n",
    "(on apprend l'identité). On contraint donc le réseau à ne pas reproduire\n",
    "parfaitement l'entrée, et à ne s'intéresser qu'à certains aspects de la\n",
    "reconstruction, ce qui lui permet d'apprendre des propriétés utilies des\n",
    "données.\n",
    "\n",
    "\n",
    "```{figure} ./images/ae1.png\n",
    ":name: ae1\n",
    "Architecture générale d’un autoencodeur\n",
    "```\n",
    "\n",
    "\n",
    "## Influence de la taille de l'espace de codage\n",
    "\n",
    "### Le cas $|\\mathcal{H}|<|\\mathcal{X}|$\n",
    "\n",
    "Lorsque la dimension du code $\\mathbf{h}$ est inférieure à celle de\n",
    "$\\mathbf{x}$, l'encodeur `E` apprend à réduire la dimension. Le\n",
    "décodeur, une fois appris, permet de créer une donnée dans $\\mathcal{X}$\n",
    "à partir d'un point de $\\mathcal{H}$ : il agit donc comme un modèle\n",
    "génératif.\\\n",
    "L'apprentissage (la recherche des valeurs de $\\mathbf{w_E}$ et\n",
    "$\\mathbf{w_D}$) s'effectue par minimisation d'une fonction de perte\n",
    "\n",
    "$$\\ell(\\mathbf{x} , g\\left [f(\\mathbf{x} ; \\mathbf{w_E}); \\mathbf{w_D})\\right ]$$\n",
    "\n",
    "Si $g$ est linéaire et $\\ell$ est la fonction de perte quadratique,\n",
    "alors l'autoencodeur agit comme l'analyse en composantes principales.\n",
    "Dans le cas plus général, l'autoencodeur apprend une représentation plus\n",
    "complexe des données. Il faut cependant prendre garde à ce que $f$ et\n",
    "$g$ ne soient pas trop complexes, auquel cas l'autoencodeur ne saura\n",
    "faire que copier exactement l'entrée, sans extraire dans $\\mathcal{H}$\n",
    "d'information utile sur les données.\n",
    "\n",
    "L'espace $\\mathcal{H}$ peut être utilisé pour de la visualisation en\n",
    "dimension réduite des données, pour des tâches de classification, ou\n",
    "plus simplement pour un espace de représentation plus compact des\n",
    "données de $\\mathcal{X}$ ({numref}`ae2`).\n",
    "\n",
    "\n",
    "```{figure} ./images/ae2.png\n",
    ":name: ae2\n",
    "Utilisation d'un autoencodeur pour la compression et la génération de\n",
    "chiffres manuscrits. Les images MNIST (28$\\times$28,\n",
    "ligne du haut) sont encodées par un simple perceptron multicouche à\n",
    "activation sigmoïde et une seule couche cachée de taille 36. Le code est\n",
    "visualisé (ligne du milieu) sous la forme d'images\n",
    "6$\\times$6. Le décodeur produit des images\n",
    "reconstruites (ligne du bas) à partir de ce\n",
    "code\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "### Le cas $|\\mathcal{H}|\\geq|\\mathcal{X}|$\n",
    "\n",
    "Si la taille de l'espace de représentation $\\mathcal{H}$ est supérieure\n",
    "à celle de l'espace d'entrée, on comprend assez facilement qu'il est\n",
    "très aisé pour l'autoencodeur d'apprendre l'identité, sans extraire\n",
    "d'information utile des données initiales (il suffit de propager\n",
    "$\\mathcal{X}$ dans $\\mathcal{H}$). Il est donc nécessaire de contraindre\n",
    "le modèle.\n",
    "\n",
    "## Autoencodeurs régularisés\n",
    "\n",
    "Régulariser un autoencodeur permet d'entraîner efficacement\n",
    "l'algorithme, en choisissant de plus la dimension de $\\mathcal H$ et la\n",
    "complexité de $f$ et $g$ en fonction de la complexité de la distribution\n",
    "à modéliser. Plutôt que de limiter la capacité du modèle (en imposant\n",
    "par exemple que `E` et `D` soient des réseaux multicouches à faible\n",
    "profondeur et/ou que $\\mathcal H$ soit de faible dimension), la\n",
    "régularisation construit une fonction de perte qui encourage\n",
    "l'autoencodeur à avoir des propriétés supplémentaires, en plus de celle\n",
    "de reproduire son entrée.\n",
    "\n",
    "Un autoencodeur régularisé minimise la fonction\n",
    "\n",
    "$$\\ell(\\mathbf{x} , g\\left [f(\\mathbf{x} ; \\mathbf{w_E}); \\mathbf{w_D})\\right ] + \\beta \\Omega(\\mathbf{h})$$\n",
    "\n",
    "où $\\Omega(\\mathbf{h})$ est un terme de pénalisation permettant de\n",
    "contraindre les paramètres du modèle et $\\beta\\in\\mathbb{R}$ contrôle le\n",
    "poids du terme de pénalité dans l'optimisation.\n",
    "\n",
    "### Autoencodeurs parcimonieux\n",
    "\n",
    "Les autoencodeurs parcimonieux (ou épars) sont typiquement utilisés pour\n",
    "apprendre des caractéristiques pertinentes des données d'entrée, qui\n",
    "sont ensuite utilisées comme entrées d'algorithmes de classification ou\n",
    "de régression.\n",
    "\n",
    "Supposons que `E` et `D` soient des perceptrons multicouches. Il est\n",
    "alors par exemple possible d'imposer aux neurones d'être \\\"inactifs\\\" la\n",
    "plupart du temps, en définissant l'inactivité comme une valeur de sortie\n",
    "du neurone proche de zéro (pour une sigmoïde, ou -1 pour une tangente\n",
    "hyperbolique). Pour cela, on dispose de $m$ exemples\n",
    "$\\mathcal{S} =  \\{\\mathbf{x_1}\\cdots \\mathbf{x_m}\\}$. On note\n",
    "$y^{(l)}_j(\\mathbf{x})$ l'activation du neurone caché $j$ de la couche\n",
    "$l$ lorsque l'entrée $\\mathbf{x}$ est présentée au réseau. On note\n",
    "également\n",
    "\n",
    "$$\\hat\\rho_j = \\frac{1}{m} \\sum_{i=1}^m \\left[ y^{(l)}_j(\\mathbf{x_i}) \\right]$$\n",
    "\n",
    "l'activation moyenne du neurone caché $j$ sur présentation de\n",
    "$\\mathcal{S}$. L'objectif est alors d'imposer $\\hat\\rho_j = \\rho$, où\n",
    "$\\rho$ est une valeur proche de zéro (ainsi l'activation moyenne de\n",
    "chaque neurone caché doit être faible), par l'intermédiaire d'une\n",
    "définition adaptée de $\\Omega$. De nombreux choix sont possibles. Par\n",
    "exemple pour un réseau à une couche cachée :\n",
    "\n",
    "$$\\Omega(\\mathbf{h}) = \\displaystyle\\sum_{j=1}^{n^{(2)}} \\rho \\log \\frac{\\rho}{\\hat\\rho_j} + (1-\\rho) \\log \\frac{1-\\rho}{1-\\hat\\rho_j} = \\displaystyle\\sum_{j=1}^{n^{(2)}} {KL}(\\rho || \\hat\\rho_j),$$\n",
    "\n",
    "où ${KL}(\\rho || \\hat\\rho_j)$ est la divergence de Kullback-Leibler (KL)\n",
    "entre une variable aléatoire de loi de Bernoulli de moyenne $\\rho$ et\n",
    "une variable aléatoire de loi de Bernoulli de moyenne $\\hat\\rho_j$.\\\n",
    "On peut alors montrer que ${KL}(\\rho || \\hat\\rho_j) = 0$ si\n",
    "$\\hat\\rho_j = \\rho$, et ${KL}$ croît de façon monotone lorsque\n",
    "$\\hat\\rho_j$ s'éloigne de $\\rho$.\n",
    "\n",
    "Le calcul des dérivées partielles et la descente de gradient changent\n",
    "peu pour l'algorithme d'optimisation. Il faut cependant connaître au\n",
    "préalable les $\\textstyle \\hat\\rho_j$ et donc faire dans un premier\n",
    "temps une propagation avant sur tous les exemples de $\\mathcal{S}$\n",
    "permettant de calculer les activations moyennes.\\\n",
    "Dans le cas où la base d'apprentissage est suffisamment petite, elle\n",
    "tient entièrement en mémoire et les activations peuvent être stockées\n",
    "pour calculer $\\textstyle \\hat\\rho_j$. Les activations stockées peuvent\n",
    "alors être utilisées dans l'étape de rétropropagation sur l'ensemble des\n",
    "exemples.\\\n",
    "Dans le cas contraire, le calcul de $\\textstyle \\hat\\rho_j$ peut être\n",
    "fait en accumulant les activations calculées exemple par exemple, mais\n",
    "sans sauvegarder les valeurs de ces activations. Une seconde propagation\n",
    "sur chaque exemple sera alors nécessaire pour permettre la\n",
    "rétropropagation. \n",
    "\n",
    "Les autoencodeurs parcimonieux peuvent également être\n",
    "vus d'un point de vue probabiliste comme des algorithmes maximisant la\n",
    "vraisemblance maximale d'un modèle génératif à variables latentes\n",
    "$\\mathbf{h}$. Supposons disposer d'une distribution jointe explicite\n",
    "\n",
    "$$\\textbf{p}_{modele}(\\mathbf{x},\\mathbf{h}) = \\textbf{p}_{modele}( \\mathbf{h})\\textbf{p}_{modele}(\\mathbf{x}|\\mathbf{h})$$\n",
    "\n",
    "La log vraisemblance peut alors s'écrire\n",
    "\n",
    "$$log(\\textbf{p}_{modele}(\\mathbf{x})) = log \\left (\\displaystyle\\sum_{\\mathbf{h}}  \\textbf{p}_{modele}(\\mathbf{x},\\mathbf{h})\\right )$$\n",
    "\n",
    "L'autoencodeur approche cette somme juste pour une une valeur de\n",
    "$\\mathbf{h}$ fortement probable. Pour cette valeur, on maximise alors\n",
    "$$log(\\textbf{p}_{modele}(\\mathbf{x},\\mathbf{h})) = log(\\textbf{p}_{modele}(\\mathbf{h})) + log(\\textbf{p}_{modele}(\\mathbf{x}|\\mathbf{h}))$$\n",
    "et $log(\\textbf{p}_{modele}(\\mathbf{h}))$ peut être\n",
    "utilisée pour introduire de la parcimonie.\n",
    "\n",
    "Par exemple si\n",
    "$\\textbf{p}_{modele}(h_i) = \\frac{\\lambda}{2} e^{-\\beta|h_i| }$\n",
    "(Laplace prior), alors\n",
    "\n",
    "$$-log(\\textbf{p}_{modele}(\\mathbf{h})) = \\displaystyle\\sum_{i=1}^{|\\mathcal{H}|}\\left (\\lambda |h_i| -log\\frac{\\lambda}{2} \\right) = \\Omega(\\mathbf{h})  + c$$\n",
    "\n",
    "et l'on retrouve une régularisation $\\ell_1$ (méthode Lasso).\n",
    "\n",
    "### Autoencodeurs contractifs\n",
    "\n",
    "Une autre stratégie de régularisation consiste à faire dépendre $\\Omega$\n",
    "du gradient du code en fonction des entrées :\n",
    "\n",
    "$$\\Omega(\\mathbf{x},\\mathbf{h}) = \\displaystyle\\sum_{i=1}^{|\\mathcal{H}|}\\|\\nabla_{\\mathbf{x}} h_i\\|^2 =\\left  \\|\\frac{\\partial f(\\mathbf{x},\\mathbf{w_E})}{\\partial \\mathbf{x}}\\right \\|_F^2$$\n",
    "\n",
    "où $\\|.\\|_F$ est la norme de Frobenius. Le modèle apprend alors une\n",
    "fonction qui change peu lorsque $\\mathbf{x}$ varie peu. Puisque la\n",
    "pénalité n'est appliquée que sur les exemples de $\\mathcal{S}$, les\n",
    "informations capturées dans le code concernent la distribution des\n",
    "données d'entraînement, et plus précisément la variété sur laquelle\n",
    "vivent les données de $\\mathcal{S}$. En ce sens, ces autoencodeurs sont\n",
    "à rapprocher des méthodes de *Manifold Learning*.\n",
    "\n",
    "### Autoencodeurs de débruitage\n",
    "\n",
    "Plutôt que d'ajouter un terme à la fonction de perte, on peut\n",
    "directement changer cette dernière pour apprendre des caractéristiques\n",
    "utiles des données.\\\n",
    "Un autoencodeur de débruitage considère la fonction de perte\n",
    "\n",
    "$$\\ell(\\mathbf{x} , g\\left [f(\\mathbf{\\tilde{x}} ; \\mathbf{w_E}); \\mathbf{w_D})\\right ]$$\n",
    "\n",
    "où $\\mathbf{\\tilde{x}}$ est une version de $\\mathbf{{x}}$ bruitée par\n",
    "une distribution conditionnelle $C(\\mathbf{\\tilde{x}},\\mathbf{x})$.\n",
    "L'autoencodeur apprend alors une distribution de reconstruction\n",
    "$\\textbf{p}_R(\\mathbf{x}\\mid \\mathbf{\\tilde{x}})$\n",
    "selon l'{prf:ref}`AEB` ({numref}`ae3`).\n",
    "\n",
    "```{prf:algorithm} Algorithe d'apprentissage d'un autoencodeur de débruitage\n",
    ":label: AEB\n",
    "**Entrée :**  $\\mathcal{S}$, $C(\\mathbf{\\tilde{x}},\\mathbf{x})$, un autoencodeur $(f,g)$\n",
    "\n",
    "**Sortie :** Un autoencodeur de débruitage\n",
    "\n",
    "1. Tant que (non stop)\n",
    "    1. Tirer un exemple $\\mathbf x$ de $\\mathcal{S}$\n",
    "    2. Tirer $\\mathbf {\\tilde{x}}$ selon $C(\\mathbf{\\tilde{x}},\\mathbf{x})$\n",
    "    3. Estimer $\\textbf{p}_R(\\mathbf{x}\\mid \\mathbf{\\tilde{x}}) = \\textbf{p}_{decodeur}(\\mathbf{x}\\mid \\mathbf{h},\\mathbf{w_D})=g(\\mathbf{h},\\mathbf{w_D})$ où $\\mathbf{h} = f(\\mathbf{\\tilde{x}},\\mathbf{w_E})$\n",
    "```\n",
    "\n",
    "L'apprentissage peut être vu comme une descente de gradient stochastique\n",
    "de\n",
    "\n",
    "$$-\\mathbb{E}_{\\mathbf{x}\\sim \\textbf{p}_{\\mathcal{S}}(\\mathbf{x})} \\mathbb{E}_{\\mathbf{\\tilde{x}}\\sim C(\\mathbf{\\tilde{x}},\\mathbf{x})} \\left (log\\;\\textbf{p}_{decodeur} (\\mathbf{x}|\\mathbf{h}=f(\\mathbf{\\tilde{x}},\\mathbf{w_E}),\\mathbf{w_D})\\right )$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "```{figure} ./images/ae3.png\n",
    ":name: ae3\n",
    "Autoencodeur de débruitage sur les données MNIST. Les images\n",
    "$\\mathbf{x}$ (ligne du haut) sont corrompues par un bruit gaussien\n",
    "centré de variance unité (deuxième ligne). Un autoencodeur de débruitage\n",
    "est ensuite entraîné. Le code $\\mathbf{h}$ de taille 32 est visualisé\n",
    "(troisième ligne) sous la forme d'images 8$\\times$4. Le\n",
    "décodeur produit les images débruitées de la dernière\n",
    "ligne.\n",
    "```\n",
    "\n",
    "\n",
    "## Autoencodeurs variationnels\n",
    "\n",
    "Le dernier modèle d'autoencodeurs que nous abordons fait le lien avec le chapitre consacré aux réseaux antagonistes générateurs.\n",
    "\n",
    "Les autoencodeurs variationnels (VAE) {cite:p}`Kingma13` sont des modèles\n",
    "génératifs. Ce ne sont pas à proprement parler des autoencodeurs tels\n",
    "que nous les avons abordés dans les paragraphes précédents, ils\n",
    "empruntent juste une architecture similaire ({numref}`ae4`), d'où leur nom.\n",
    "\n",
    "```{figure} ./images/ae4.png\n",
    ":name: ae4\n",
    "Architecture générale d’un autoencodeur\n",
    "variationnel.\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "Au lieu d'apprendre $f(.,\\mathbf{w_E})$ et $g(.,\\mathbf{w_D})$, un\n",
    "autoencodeur variationnel apprend des distributions de probabilité\n",
    "$\\textbf{p}_{encodeur} (\\mathbf{h}|\\mathbf{x},\\mathbf{w_E})$\n",
    "et\n",
    "$\\textbf{p}_{decodeur} (\\mathbf{x}|\\mathbf{h},\\mathbf{w_D})$.\\\n",
    "Apprendre des distributions plutôt que des fonctions déterministes\n",
    "présente plusieurs avantages, et notamment :\n",
    "\n",
    "-   les données d'entrée peuvent être bruitées, et un modèle de\n",
    "    distribution $\\textbf{p}_\\mathbf{x}$ peut être\n",
    "    plus utile\n",
    "\n",
    "-   il est possible d'utiliser\n",
    "    $\\textbf{p}_{decodeur} (\\mathbf{x}|\\mathbf{h},\\mathbf{w_D})$\n",
    "    pour échantillonner $\\mathbf{h}$ puis $\\mathbf{x}$, et donc de\n",
    "    générer des données ayant des statistiques similaires aux éléments\n",
    "    de $\\mathcal{S}$ ({numref}`ae5`).\n",
    "\n",
    "Abordons ces autoencodeurs sous l'angle des modèles génératifs.\n",
    "Supposons que nous voulions générer des points suivant la distribution\n",
    "$\\textbf{p}_\\mathbf{x}$. Plutôt que d'inférer\n",
    "directement sur cette distribution, nous pouvons utiliser des *variables\n",
    "latentes* (le code des autoencodeurs). Les modèles à variables latentes\n",
    "font l'hypothèse que les données $\\mathbf{x}$ sont issues d'une variable\n",
    "non observée $\\mathbf{h}$. S'il peut être difficile de modéliser\n",
    "directement $\\textbf{p}_\\mathbf{x}$, il peut être plus\n",
    "facile de choisir*a priori* une distribution\n",
    "$\\textbf{p}_\\mathbf{h}$ et chercher à modéliser\n",
    "$\\textbf{p}_{\\mathbf{x}|\\mathbf{h}}$.\\\n",
    "Pour générer $\\mathbf{x}\\sim \\textbf{p}_\\mathbf{x}$,\n",
    "un autoencodeur variationnel tire donc tout d'abord\n",
    "$\\mathbf{h}\\sim \\textbf{p}_\\mathbf{h}$. $\\mathbf{h}$\n",
    "est ensuite passé à un réseau de neurones et $\\mathbf{x}$ est finalement\n",
    "tiré selon\n",
    "$\\textbf{p}_{decodeur} (\\mathbf{x}|\\mathbf{h},\\mathbf{w_D})$.\n",
    "L'entraînement est réalisé en maximisant la borne inférieure\n",
    "variationnelle :\n",
    "\n",
    "$$\\mathcal{L}(q) = \\mathbb{E}_{\\mathbf{h}\\sim q(\\mathbf{h}|\\mathbf{x})} log \\left (\\textbf{p}_{decodeur} (\\mathbf{x}|\\mathbf{h},\\mathbf{w_D})\\right ) -KL\\left (q(\\mathbf{h}|\\mathbf{x}) ||\\textbf{p}_\\mathbf{h}\\right )$$\n",
    "\n",
    "où $KL$ est la divergence de Kullback Leibler déjà rencontrée dans les\n",
    "autoencodeurs parcimonieux. Le premier terme de $\\mathcal{L}(q)$ est la\n",
    "log vraisemblance de la reconstruction trouvée dans les autoencodeurs\n",
    "classiques, tandis que le second terme tend à rapprocher la distribution\n",
    "*a posteriori* $q(\\mathbf{h}|\\mathbf{x})$ et le modèle *a priori*\n",
    "$\\textbf{p}_\\mathbf{h}$. Dans les techniques\n",
    "classiques d'inférence, $q$ est approché par optimisation. Dans les\n",
    "autoencodeurs variationnels, on entraîne un encodeur paramétrique (un\n",
    "réseau de neurones paramétré par $\\mathbf{w_E}$) qui produit les\n",
    "paramètres de $q$. Tant que $\\mathbf{h}$ est continue, il est donc\n",
    "possible de rétropropager à travers les tirages de $\\mathbf{h}$\n",
    "effectués selon\n",
    "$q(\\mathbf{h}|\\mathbf{x})  = q(\\mathbf{h}| f(\\mathbf{x} ; \\mathbf{w_E}))$\n",
    "pour obtenir le gradient par rapport à $\\mathbf{w_E}$. L'apprentissage\n",
    "consiste alors simplement à maximiser $\\mathcal{L}$ par rapport à\n",
    "$(\\mathbf{w_E},\\mathbf{w_D})$.\n",
    "\n",
    "Il est courant de choisir comme prior\n",
    "$\\textbf{p}_\\mathbf{h}$ une loi normale centrée\n",
    "réduite $\\mathcal{N}(\\mathbf{0},\\mathbf{I})$. Cette simplicité apparente\n",
    "ne réduit pas le pouvoir d'expression du modèle si l'effort est fait sur\n",
    "l'optimisation de la distribution\n",
    "$\\textbf{p}_{decodeur} (\\mathbf{x}|\\mathbf{h},\\mathbf{w_D})$.\n",
    "L'encodeur `E` est alors un réseau de neurones générant des paramètres\n",
    "de distribution de $q$ dans $\\mathcal{H}$, soit un vecteur de moyenne\n",
    "$\\mu$ et une matrice de covariance $\\mathbf{\\Sigma}$.\\\n",
    "Notons enfin que la rétropropagation du gradient nécessite une astuce de\n",
    "calcul dans $\\mathcal{H}$, dite astuce de reparamétrisation : la\n",
    "génération de\n",
    "$\\mathbf{h}\\sim \\textbf{p}_{encodeur} (\\mathbf{h}|\\mathbf{x},\\mathbf{w_E})$\n",
    "se fait effectivement en tirant une variable aléatoire\n",
    "$\\epsilon\\sim\\mathcal{N}(\\mathbf{0},\\mathbf{I})$, puis en calculant\n",
    "$\\mathbf{h}$=$\\mu$ + $\\Sigma^{1/2} \\epsilon$. L'échantillonnage se fait\n",
    "alors seulement pour $\\epsilon$, qui n'a pas besoin d'être rétropropagé.\n",
    "\n",
    "\n",
    "\n",
    "```{figure} ./images/ae5.png\n",
    ":name: ae5\n",
    "Visualisation de l’espace latent $\\mathcal H = \\mathbb{R}^2$ appris par un autoencodeur variationnel sur les donneés MNIST. Pour chaque valeur $\\mathbf h_i$ discrétisée sur $\\mathcal H$ est affichée une image $\\mathbf x ∼ \\textbf{p}_{decodeur}(\\mathbf{x}\\mid \\mathbf h_i,\\mathbf{w_D})$. Les chiffres de la même classe sont groupés dans cet espace, et les axes de $\\mathcal H$ ont une interprétation (l'axe horizontal semble souligner le caractère \"penché\" des chiffres).\n",
    "```\n",
    "\n",
    "## Implémentation\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import make_grid\n",
    "```\n",
    "\n",
    "On travaille sur les données MNIST\n",
    "\n",
    "\n",
    "```python\n",
    "# taille des batchs\n",
    "train_batch_size=128\n",
    "test_batch_size = 128\n",
    "\n",
    "# Paramètres du réseau et de l'apprentissage\n",
    "lr = 0.001\n",
    "num_epochs = 100\n",
    "num_hidden_1 = 256  \n",
    "num_hidden_2 = 128  \n",
    "num_input = 784  \n",
    "\n",
    "transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "        ])\n",
    "dataset1 = datasets.MNIST('../data', train=True, download=True,\n",
    "                       transform=transform)\n",
    "dataset2 = datasets.MNIST('../data', train=False,\n",
    "                       transform=transform)\n",
    "train_kwargs = {'batch_size': train_batch_size}\n",
    "test_kwargs = {'batch_size': test_batch_size}\n",
    "train_loader = torch.utils.data.DataLoader(dataset1,**train_kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(dataset2, **test_kwargs)\n",
    "```\n",
    "\n",
    "et on implémente un autoencodeur : \n",
    "\n",
    "```python\n",
    "class AE(nn.Module):\n",
    "    def __init__(self, x_dim, h_dim1, h_dim2):\n",
    "        super(AE, self).__init__()\n",
    "        # encodeur \n",
    "        self.fc1 = nn.Linear(x_dim, h_dim1)\n",
    "        self.fc2 = nn.Linear(h_dim1, h_dim2)\n",
    "        # decodeur part\n",
    "        self.fc3 = nn.Linear(h_dim2, h_dim1)\n",
    "        self.fc4 = nn.Linear(h_dim1, x_dim)\n",
    "\n",
    "    def encodeur(self, x):\n",
    "        x = torch.sigmoid(self.fc1(x))\n",
    "        x = torch.sigmoid(self.fc2(x))\n",
    "        return x\n",
    "\n",
    "    def decodeur(self, x):\n",
    "        x = torch.sigmoid(self.fc3(x))\n",
    "        x = torch.sigmoid(self.fc4(x))\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encodeur(x)\n",
    "        x = self.decodeur(x)\n",
    "        return x\n",
    "```\n",
    "\n",
    "On instantie le modèle\n",
    "\n",
    "```python\n",
    "model = AE(num_input, num_hidden_1, num_hidden_2)\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "loss_function = nn.MSELoss()\n",
    "```\n",
    "\n",
    "Et on entraîne\n",
    "```python\n",
    "for i in range(num_epochs):\n",
    "    train_loss = 0\n",
    "    for batch_idx, (data, _) in enumerate(train_loader):\n",
    "        inputs = torch.reshape(data,(-1, 784)) \n",
    "        optimizer.zero_grad()\n",
    "        recons = model(inputs)\n",
    "        loss = loss_function(recons, inputs)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        \n",
    "    if i%10==0:    \n",
    "        print('Epoch: {} perte moyenne: {:.9f}'.format(i, train_loss))\n",
    "```\n",
    "\n",
    "\n",
    "La figure ({numref}`ae5b`) montre l'évolution de la reconstruction au cours des itérations.\n",
    "\n",
    "```{figure} ./images/ae5b.png\n",
    ":name: ae5b\n",
    ":align: center\n",
    "Reconstruction aux itérations 0,10,20,30 et 40 d'un sous-ensemble de l'ensemble d'apprentissage.\n",
    "```\n",
    "\n",
    "Pour finalement visualiser les résultats de la reconstruction sur un ensemble de test ({numref}`ae6`).\n",
    "\n",
    "```python\n",
    "def imshow(img):\n",
    "    npimg = img.numpy()\n",
    "    ax = plt.gca()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0))) \n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    plt.tight_layout()\n",
    "\n",
    "inputs, _ = next(iter(test_loader))\n",
    "inputs_example = make_grid(inputs[:16,:,:,:],4)\n",
    "inputs=torch.reshape(inputs,(-1,784))\n",
    "\n",
    "outputs=model(inputs)\n",
    "outputs=torch.reshape(outputs,(-1,1,28,28))\n",
    "outputs=outputs.detach().cpu()\n",
    "outputs_example = make_grid(outputs[:16,:,:,:],4)\n",
    "\n",
    "fig = plt.figure(122)\n",
    "fig.add_subplot(121)\n",
    "fig.set_title(\"Images originales\")\n",
    "imshow(inputs_example)\n",
    "fig.add_subplot(122)\n",
    "fig.set_title(\"Images reconstruites\")\n",
    "imshow(outputs_example)\n",
    "```\n",
    "\n",
    "\n",
    "```{figure} ./images/ae6.png\n",
    ":name: ae6\n",
    ":align: center\n",
    "Images originales de test (gauche) et reconstruites (droite) par l'autoencodeur.\n",
    "```\n",
    "\n",
    "L'Autoencodeur peut ensuite être utilisé par exemple en reconnaissance de chiffres. Une manière simple consiste à choisir au hasard 10 échantillons d'apprentissage de chaque classe et à leur attribuer une étiquette. Ensuite, étant donné les données de test, il est possible de prédire à quelles classes elles appartiennent en trouvant les échantillons d'apprentissage étiquetés les plus similaires dans l'espace latent $\\mathcal H$.\n",
    "\n",
    "```python\n",
    "# Données d'entraînement\n",
    "x_train, y_train = next(iter(train_loader))\n",
    "candidates = np.random.choice(train_batch_size, 10*10)\n",
    "x_train = x_train[candidates]\n",
    "y_train = y_train[candidates]\n",
    "\n",
    "# Données test à étiqueter\n",
    "x_test, y_test = next(iter(test_loader))\n",
    "candidates = np.random.choice(test_batch_size, 10*10)\n",
    "x_test = x_test[candidates_test]\n",
    "y_test = y_test[candidates]\n",
    "\n",
    "#Représentation des données dans l'espace latent\n",
    "h_train=model.encodeur(torch.reshape(x_train,(-1,784)))\n",
    "h_test=model.encodeur(torch.reshape(x_test,(-1,784)))\n",
    "\n",
    "# Données d'entraînement les plus proches (MSE) de chaque exemple de test\n",
    "MSEs = np.mean(np.power(np.expand_dims(h_test.detach().cpu(), axis=1) - np.expand_dims(h_train.detach().cpu(), axis=0), 2), axis=2)\n",
    "neighbours = MSEs.argmin(axis=1)\n",
    "predicts = y_train[neighbours]\n",
    "\n",
    "print(\"Taux de reconnaissance des chiffres manuscrits sur l'ensemble de test :  %.1f%%\" % (100 * (y_test == predicts).numpy().astype(np.float32).mean()))\n",
    "```\n",
    "\n",
    "\n",
    "```{bibliography}\n",
    ":style: unsrt\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "md:myst",
   "text_representation": {
    "extension": ".md",
    "format_name": "myst"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "source_map": [
   11
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}