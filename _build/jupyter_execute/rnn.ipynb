{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bedb71ca",
   "metadata": {},
   "source": [
    "# Réseaux récurrents\n",
    "\n",
    "-----------------------------------------------------------------------------------------------\n",
    "\n",
    "## Définition\n",
    "\n",
    "Les réseaux de neurones récurrents (RNN, *Recurrent Neural Networks*)\n",
    "sont des réseaux à propagation avant, permettant de prendre en compte le\n",
    "temps. Comme dans les réseaux classiques, il n'existe pas de cycle, mais\n",
    "les arcs ajoutés pour introduire la notion de temps (les arcs\n",
    "récurrents) peuvent en revanche former des cycles, y compris de longueur\n",
    "1 (connexion d'un neurone avec lui-même). À l'instant $t$, les neurones\n",
    "possédant des arcs récurrents reçoivent en entrée la donnée courante\n",
    "$\\mathbf{x_t}$ et les valeurs des neurones cachés ${h_{t-1}}$ informant\n",
    "sur l'état précédent du réseau. La sortie $\\hat{y}_{t}$ est calculée\n",
    "étant donné l'état $\\mathbf{x_t}$ des neurones cachés à l'instant $t$.\n",
    "La donnée $\\mathbf{x_{t-1}}$ peut influencer $\\hat{y}_{t}$ et la sortie\n",
    "aux instants suivants, à l'aide des arcs récurrents.\n",
    "\n",
    "Deux équations permettent de calculer les quantités nécessaires à\n",
    "l'instant $t$ dans la phase de propagation avant d'un réseau récurrent\n",
    "simple (comme celui de la {numref}`ae1` gauche) : \n",
    "\n",
    "$$\\begin{aligned}\n",
    "h_t&=&\\sigma\\left ( \\mathbf{W_{hx}^\\top x_t} +  \\mathbf{W_{hh}^\\top x_{t-1}} + b_h\\right)\\\\\n",
    "\\hat{y}_{t}&=&softmax\\left({W_{yh}} h_t + b_y  \\right)\n",
    "\\end{aligned}$$ \n",
    "\n",
    "où $\\mathbf{W_{hx}}$ est la matrice des poids reliant\n",
    "l'entrée à la couche cachée et $\\mathbf{W_{hh}}$ celle des poids des\n",
    "arcs récurrents. Les biais sont notés $b_h$ et $b_y$.\\\n",
    "La dynamique du réseau peut être décrite en dépliant ce réseau dans le\n",
    "temps ({numref}`ae1` droite). Le réseau devient donc un réseau profond, avec une couche par instant $t$ et un partage de poids au cours du temps. Ce dernier peut\n",
    "donc être entraîné de manière classique par l'algorithme de\n",
    "rétropropagation du gradient, indicé par le temps (*Backpropagation\n",
    "through time*, BPTT algorithm).\n",
    "\n",
    "\n",
    "```{figure} ./images/rnn1.png\n",
    ":name: ae1\n",
    "Réseau récurrent et sa version dépliée dans le\n",
    "temps.\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "Avec ces réseaux, il est possible de traiter des séquences de longueur\n",
    "quelconque, la taille du modèle étant indépendante de cette longueur.\n",
    "Plusieurs architectures peuvent être déclinées sur ce principe et le\n",
    "tableau [1.1](#T:rnn){reference-type=\"ref\" reference=\"T:rnn\"} donne un\n",
    "panorama de certaines d'entre elles, avec des exemples d'applications.\n",
    "\n",
    "::: {#T:rnn}\n",
    "  **Architecture**           **Réseau**                                              **Applications**\n",
    "  -------------------------- ------------------------------------------------------- ---------------------------------------------------------------\n",
    "  Un vers plusieurs          ![image](images/onetomany.pdf){width=\"\\\\linewidth\"}     Génération de musique, légendage d'images\n",
    "  Plusieurs vers un          ![image](images/manytoone.pdf){width=\"\\\\linewidth\"}     Classification de sentiments\n",
    "  Plusieurs vers plusieurs   ![image](images/manytomany1.pdf){width=\"\\\\linewidth\"}   Reconnaissance d'entité dans des textes, annotation de vidéos\n",
    "  Plusieurs vers plusieurs   ![image](images/manytomany2.pdf){width=\"\\\\linewidth\"}   Traduction\n",
    "\n",
    "  : Différentes architectures de réseaux récurrents et leurs\n",
    "  applications\n",
    ":::\n",
    "\n",
    "## Entraînement des réseaux récurrents\n",
    "\n",
    "L'apprentissage de dépendances long terme peut être difficile. Les\n",
    "problèmes d'évanescence (*vanishing*) ou d'explosion du gradient peuvent\n",
    "rapidement survenir, lors de la rétropropagation sur plusieurs pas de\n",
    "temps.\n",
    "\n",
    "Prenons un exemple simple pour comprendre : considérons un réseau à un\n",
    "neurone d'entrée, un neurone récurrent caché et un neurone de sortie. On\n",
    "donne au réseau une entrée à l'instant $t_0$ et on calcule l'erreur à\n",
    "l'instant $t>t_0$, en supposant des entrées nulles entre $t_0$ et $t$.\n",
    "Le lien entre les poids au cours du temps fait que le poids sur l'arc\n",
    "récurrent ne change jamais. La contribution de l'entrée au temps $t_0$ à\n",
    "la sortie au temps $t$ deviendra de plus en plus importante, ou se\n",
    "rapprochera de zéro, de manière exponentielle à mesure que $t-t_0$\n",
    "croît. Et la dérivée de l'erreur par rapport à l'entrée explosera ou\n",
    "disparaîtra, selon que le poids de l'arc récurrent a une valeur absolue\n",
    "plus grande ou plus petite que 1 et selon la fonction d'activation du\n",
    "neurone caché (le problème du gradient évanescent est très présent avec\n",
    "une sigmoïde et une activation ReLU force davantage l'explosion).\n",
    "\n",
    "Plusieurs solutions ont été proposées (régularisation, retropropagation\n",
    "tronquée, conception d'architecture et heuristiques) pour résoudre ces\n",
    "problèmes.\n",
    "\n",
    "## Quelques architectures\n",
    "\n",
    "### LSTM\n",
    "\n",
    "Les réseaux *Long Short-Term Memory* (LSTM) ont été introduits en 1997 {cite:p}`HO97` pour résoudre le problème de l'évanescence du gradient. Ce\n",
    "modèle ressemble à un réseau récurrent classique à une couche cachée,\n",
    "mais chaque neurone de la couche cachée est remplacé par une cellule de\n",
    "mémoire.\n",
    "\n",
    "Dans la suite, on note $\\mathbf{x_t}$ l'entrée de la cellule à l'instant\n",
    "$t$, ${h_{t-1}}$ la sortie de la couche cachée calculée au temps $t-1$.\n",
    "Au lieu de calculer une sortie du type\n",
    "$\\sigma\\left( \\mathbf{W^\\top x}+b\\right)$, la cellule contient plusieurs\n",
    "éléments distincts aux fonctions particulières. Les LSTM introduisent la\n",
    "notion de portes, qui sont des unités d'activation de type sigmoïde qui\n",
    "prennent comme arguments $\\mathbf{x_t}$ et ${h_{t-1}}$ et viennent\n",
    "pondérer des valeurs calculées dans la cellule. En particulier, si la\n",
    "valeur d'une porte est nulle, alors le flot est coupé dans le graphe,\n",
    "alors qu'il transite intégralement si la valeur de la porte est égale à\n",
    "1.\n",
    "\n",
    "On retrouve dans une cellule (figure\n",
    "[1.3](#F:lstmC){reference-type=\"ref\" reference=\"F:lstmC\"}) les éléments\n",
    "suivants :\n",
    "\n",
    "-   *Neurone d'entrée* : ce neurone prend en entrée $\\mathbf{x_t}$ et\n",
    "    ${h_{t-1}}$ et calcule, à la manière d'un neurone classique, une\n",
    "    sortie\n",
    "    $g^{t} = \\sigma\\left(\\mathbf{W_C^\\top} \\left[\\mathbf{x_t},  {h_{t-1}}\\right ] +b_C\\right)$.\n",
    "\n",
    "-   *Porte d'entrée* (ou de mise à jour) : la porte calcule\n",
    "    $i^{t} = \\sigma\\left(\\mathbf{W_i}^\\top \\left[\\mathbf{x_t},  {h_{t-1}}\\right ] +b_i\\right)$\n",
    "    et vient pondérer la valeur du neurone d'entrée pour décider de\n",
    "    l'importance à lui donner au temps $t$.\n",
    "\n",
    "-   Porte d'oubli : cette porte calcule\n",
    "    $f^{t}=\\sigma\\left(\\mathbf{W_f}^\\top\\left[\\mathbf{x_t} , {h_{t-1}}\\right ] +b_f\\right)$\n",
    "    et permet au réseau d'oublier son état interne.\n",
    "\n",
    "-   *État interne* : le cœur de la cellule de mémoire est son état\n",
    "    interne, noté $C^{t}$, composé d'un neurone récurrent à poids fixe\n",
    "    unité, assurant que le gradient peut passer par cet arc de\n",
    "    nombreuses fois sans disparaître ou exploser. La mise à jour de\n",
    "    l'état interne est effectuée par une opération du type\n",
    "    $C^{t} =g^{t}.i^{t} + C^{(t-1)}.f^{t}$.\n",
    "\n",
    "-   *Porte de sortie* : la valeur $h_t$ produite par la cellule de\n",
    "    mémoire est calculée comme le produit de $tanh(C^{t})$ par la valeur\n",
    "    de la porte de sortie $o^{t}$. Cette porte sélectionne la part de\n",
    "    $C^{t}$ à fournir en sortie et est calculée par\n",
    "    $o^{t} = \\sigma\\left(\\mathbf{W_o}^\\top\\left[\\mathbf{x_t} , h_{t-1}\\right ] +b_o\\right)$.\n",
    "\n",
    "<figure id=\"F:lstmC\">\n",
    "\n",
    "<figcaption>Cellule LSTM</figcaption>\n",
    "</figure>\n",
    "\n",
    "En résumé, un LSTM effectue donc les opérations suivantes à l'instant\n",
    "$t$ : $$\\begin{aligned}\n",
    "g^{t} &=& \\sigma\\left(\\mathbf{W_C}^\\top \\left[\\mathbf{x_t} , h_{t-1}\\right ] +b_C\\right)\\\\\n",
    "i^{t} &=& \\sigma\\left(\\mathbf{W_i}^\\top \\left[\\mathbf{x_t} , h_{t-1}\\right ] +b_i\\right)\\\\\n",
    "f^{t} &=& \\sigma\\left(\\mathbf{W_f}^\\top \\left[\\mathbf{x_t} , h_{t-1}\\right ] +b_f\\right)\\\\\n",
    "o^{t} &=& \\sigma\\left(\\mathbf{W_o}^\\top \\left[\\mathbf{x_t}, h_{t-1}\\right ] +b_o\\right)\\\\\n",
    "C^{t} &=&g^{t}.i^{t} + C^{t-1}.f^{t}\\\\\n",
    "h_t &=& o^{t} tanh(C^{t})\n",
    "\\end{aligned}$$\n",
    "\n",
    "### GRU\n",
    "\n",
    "En 2014 [@ChO14], une version simplifiée des réseaux LSTM a été\n",
    "introduite, qui nécessite moins de paramètres. Les GRU (*Gated Recurrent\n",
    "Units*) sont en effet des réseaux sans mémoire interne $C^{t}$, ni porte\n",
    "de sortie $o^{t}$. Ces réseaux sont composés de deux portes au lieu de\n",
    "trois :\n",
    "\n",
    "-   une *porte reset* $r^{t}$, qui détermine la manière de combiner la\n",
    "    nouvelle entrée au temps $t$ avec la mémoire provenant du temps\n",
    "    $t-1$.\n",
    "\n",
    "-   une *porte de mise à jour* $z^{t}$, qui détermine la quantité de\n",
    "    mémoire précédente qui doit être conservée. Cette porte est la\n",
    "    combinaison des portes d'entrée et d'oubli des LSTM.\n",
    "\n",
    "Formellement : $$\\begin{aligned}\n",
    "r^{t} &=& \\sigma\\left(\\mathbf{W_r}^\\top \\left[\\mathbf{x_t} , h_{t-1}\\right ] +b_r\\right)\\\\\n",
    "z^{t} &=& \\sigma\\left(\\mathbf{W_z}^\\top\\left[\\mathbf{x_t}, h_{t-1}\\right ] +b_z\\right)\\\\\n",
    "\\tilde{h}^{t} &=& tanh\\left(\\mathbf{W}^\\top\\left[\\mathbf{x_t} , r^{t} h_{t-1}\\right ] +b_h \\right)\\\\ \n",
    "h_t&=&\\left(1-z^{t}\\right)h_{t-1} + z^{t} \\tilde{h}^{t}\n",
    "\\end{aligned}$$\n",
    "\n",
    "Si, pour tout $t, r^{t}=1$ et $z^{t}=0$, alors on modélise un réseau\n",
    "récurrent classique.\n",
    "\n",
    "<figure id=\"F:lstmC\">\n",
    "\n",
    "<figcaption>Cellule GRU</figcaption>\n",
    "</figure>\n",
    "\n",
    "### Réseaux récurrents bidirectionnels\n",
    "\n",
    "Les réseaux bidirectionnels ont été décrits pour la première fois en\n",
    "1997 [@SCH97]. Dans ces réseaux, deux couches cachées sont présentes,\n",
    "chacune connectée à l'entrée et la sortie. La première couche cachée a\n",
    "des connexions récurrentes depuis le passé vers le futur, tandis que\n",
    "l'autre transmet les activations depuis le futur vers le passé\n",
    "(figure [1.4](#F:BDRN){reference-type=\"ref\" reference=\"F:BDRN\"}).\n",
    "\n",
    "<figure id=\"F:BDRN\">\n",
    "<embed src=\"images/bidirectionnel.pdf\" />\n",
    "<figcaption>Réseau bidirectionnel.</figcaption>\n",
    "</figure>\n",
    "\n",
    "Étant données une entrée et une sortie du réseau (des séquences), le\n",
    "réseau peut être entraîné par rétropropagation après avoir été déplié :\n",
    "$$\\begin{aligned}\n",
    "x_t &=&\\sigma\\left(\\mathbf{W_h}^\\top \\left [\\mathbf{x_t},h_{t-1} \\right ]+b_h \\right)\\\\\n",
    "z_{t} &=&\\sigma\\left(\\mathbf{W_z}^\\top \\left [\\mathbf{x_t},z_{t+1} \\right ]+b_z \\right)\\\\\n",
    "\\hat{y}_{t}&=& softmax\\left(\\mathbf{W_y}^\\top \\left [x_t,z_{t} \\right ]+b_y \\right)\n",
    "\\end{aligned}$$ où $h_t$ (respectivement $z_{t}$) représente la valeur\n",
    "de la couche cachée dans le sens du temps (respectivement dans le sens\n",
    "inverse). Puisque le temps doit être fini dans les deux sens de\n",
    "parcours, les réseaux bidirectionnels ne peuvent traiter que des\n",
    "séquences finies.\n",
    "\n",
    "### Machines de Turing neuronales\n",
    "\n",
    "Les réseaux récurrents sont performants pour construire une\n",
    "représentation implicite de l'information, mais restent relativement peu\n",
    "adaptés à la conservation d'informations explicites (des dates précises\n",
    "par exemple). S'inspirant des mémoires de travail, théorisées par les\n",
    "neurosciences et qui sont responsables du raisonnement inductif et de la\n",
    "création de nouveaux concepts, l'idée est alors d'ajouter à ces modèles\n",
    "une mémoire de travail externe, ce qui permet de découpler la mémoire\n",
    "(assimilable à la RAM d'un ordinateur) des opérations liées à la tâche\n",
    "effectuée par le réseau (assimilable à la CPU). Puisque la mémoire des\n",
    "LSTM est distribuée dans chaque cellule, elle est donc liée au nombre de\n",
    "cellules et à la capacité de calcul et ce modèle ne répond pas\n",
    "directement au problème posé.\n",
    "\n",
    "Graves et al. [@Graves14] proposent alors une architecture, appelée\n",
    "machine de Turing neuronale, constituée de deux éléments principaux :\n",
    "une mémoire et un contrôleur doté d'un mécanisme d'attention qui lit et\n",
    "écrit dans cette mémoire. Les accès mémoire sont ici des équivalents\n",
    "analogiques dérivables, pour permettre d'entraîner le contrôleur par\n",
    "descente de gradient. Typiquement, le contrôleur est un réseau de\n",
    "neurones ou un réseau récurrent type LSTM\n",
    "(figure [\\[F:NTM\\]](#F:NTM){reference-type=\"ref\" reference=\"F:NTM\"}).\n",
    "\n",
    "Les têtes de lecture et d'écriture interagissent avec la mémoire. Chaque\n",
    "tête est contrôlée par un vecteur de poids, chaque composante\n",
    "définissant le degré d'interaction de la tête avec la zone mémoire\n",
    "correspondante. Un *mécanisme de mise à jour de ces poids*, composé de\n",
    "quatre opérations, est mis en place pour permettre l'apprentissage du\n",
    "réseau :\n",
    "\n",
    "1.  Le réseau s'intéresse tout d'abord aux zones mémoire proches d'une\n",
    "    clé $k_t$ donnée. Cela permet au modèle de retrouver une information\n",
    "    spécifique, en recherchant si la zone mémoire $M_t(i)$ est proche de\n",
    "    la clé, au sens d'une similarité $K$. Formellement, chaque poids\n",
    "    correspondant à la zone mémoire $i$ est calculé par\n",
    "    $w_t(i) = softmax(\\beta_t K[k_t,M_t(i)])$.\n",
    "\n",
    "2.  Un mécanisme d'interpolation linéaire permet ensuite de mettre à\n",
    "    jour les poids en fonction de leur valeur précédente (pour prendre\n",
    "    plus ou moins en compte l'information issue de la clé, ou au\n",
    "    contraire la valeur précédente du poids) :\n",
    "    $w_t(i)=g_t.w_t(i) + (1-g_t).w_{t-1}(i)$.\n",
    "\n",
    "3.  Un décalage par convolution translate ensuite les poids, à la\n",
    "    manière du décalage classique de la tête dans une machine de Turing\n",
    "    classique : $w_t(i)=\\displaystyle\\sum_j w_t(j)\\mathbf{s_t}(i-j)$ où\n",
    "    $\\mathbf{s_t}$ est un vecteur qui définit un décalage des poids à\n",
    "    l'instant $t$.\n",
    "\n",
    "4.  Enfin, le vecteur de poids est focalisé :\n",
    "    $w_t(i) = w_t(i)^{\\gamma_t}$, $\\gamma_t>1$.\n",
    "\n",
    "Une fois que la tête a mis à jour les poids, elle interagit avec la\n",
    "mémoire :\n",
    "\n",
    "-   Dans le cas de la tête de lecture, elle calcule une combinaison\n",
    "    linéaire des zones mémoire, pondérées par les poids $w_t(i)$, et\n",
    "    produit le vecteur $\\mathbf{r_t}$, fourni au contrôleur de l'instant\n",
    "    suivant.\n",
    "\n",
    "-   Dans le cas de la tête d'écriture, le contenu de la mémoire est mis\n",
    "    à jour selon la formule\n",
    "    $M_t(i) = M_{t-1}(i)(1-w_t(i)\\mathbf{e_t})+w_t(i)\\mathbf{a_t}$, où\n",
    "    $\\mathbf{e_t}$ est un vecteur d'effacement, dont les composantes\n",
    "    sont dans {0,1} et $\\mathbf{a_t}$ est un vecteur d'ajout.\n",
    "\n",
    "::: SCfigure\n",
    "![image](images/turing.pdf){width=\".6\\\\linewidth\"}\n",
    ":::\n",
    "\n",
    "<figure>\n",
    "\n",
    "<figcaption>Quelques applications des réseaux récurrents.</figcaption>\n",
    "</figure>\n",
    "\n",
    "## Quelques applications\n",
    "\n",
    "Comme les réseaux convolutifs, les réseaux récurrents ont depuis leur\n",
    "introduction trouvé de nombreuses applications.\n",
    "\n",
    "**Traitement automatique du langage**\n",
    "\n",
    "Les réseaux récurrents sont utilisés en traitement automatique du\n",
    "langage, notamment à des fins génératives. Ces réseaux permettent de\n",
    "modéliser un langage (prédire la probabilité d'un mot donné étant donnés\n",
    "les mots précédents) et de générer du texte à partir du modèle appris.\n",
    "De nombreuses applications découlent de cette modélisation : génération\n",
    "de texte au style de (génération d'un texte dans le style de\n",
    "Shakespeare, à partir d'un RNN appris sur le corpus des œuvres de\n",
    "l'auteur par exemple, génération de textes manuscrits\n",
    "(figure [\\[F:gener\\]](#F:gener){reference-type=\"ref\"\n",
    "reference=\"F:gener\"}), génération de pages Wikipedia, ou même génération\n",
    "d'articles scientifiques, à partir des sources LaTeXd'un ouvrage et d'un\n",
    "LSTM multicouche.\n",
    "\n",
    "**Traduction automatique**\n",
    "\n",
    "La traduction automatique de texte procède de la même stratégie que la\n",
    "modélisation d'une langue. Deux réseaux récurrents sont entraînés,\n",
    "chacun dans une des langues, et le RNN traducteur calcule sa sortie en\n",
    "fonction de la couche cachée du premier réseau.\n",
    "\n",
    "**Analyse de sentiments**\n",
    "\n",
    "Détecter de manière automatique l'opinion du public sur un sujet donné\n",
    "intéresse de plus en plus le domaine commercial. Ce domaine, largement\n",
    "alimenté par les réseaux sociaux, les avis et recommandations déposées\n",
    "sur les sites Internet, est un champ de prédilection pour les réseaux\n",
    "profonds. Des réseaux récurrents (notamment LSTM structurés en arbres)\n",
    "sont utilisés à cet effet et servent de base à des systèmes de\n",
    "recommandation.\n",
    "\n",
    "**Résumé automatique**\n",
    "\n",
    "Les réseaux récurrents permettent de produire des résumés abstraits de\n",
    "textes (i.e. générer de nouvelles phrases, en opposition à extraire les\n",
    "mots les plus importants d'un texte). Les modèles utilisés sont des\n",
    "réseaux récurrents avec mécanisme d'attention. Un système\n",
    "d'encodage/décodage est mis en place dans le réseau, où l'encodeur est\n",
    "par exemple un GRU bidirectionnel et le décodeur un GRU dont l'état\n",
    "caché a la même taille que celui de l'encodeur. Les modèles sont appris\n",
    "et validés sur des corpus dédiés (DUC, CNN/Daily Mail par exemple).\n",
    "\n",
    "**Reconnaissance de la parole**\n",
    "\n",
    "L'utilisation de réseaux LSTM bidirectionnels, qui permettent à la fois\n",
    "d'exploiter les contextes passé et futur, et de garder trace d'un\n",
    "contexte à longue échéance, a montré de bonnes performances dans la\n",
    "tache de reconnaissance de la parole.\n",
    "\n",
    "**Annotation d'images**\n",
    "\n",
    "Couplé à un réseau convolutif, un RNN permet de générer des descriptions\n",
    "(légendes) d'images non labelisées. Le réseau convolutif produit des\n",
    "descripteurs, qui servent d'entrée à un réseau récurrent type LSTM\n",
    "(figure [\\[F:caption\\]](#F:caption){reference-type=\"ref\"\n",
    "reference=\"F:caption\"}).\n",
    "\n",
    "\n",
    "\n",
    "```{bibliography}\n",
    ":style: unsrt\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "md:myst",
   "text_representation": {
    "extension": ".md",
    "format_name": "myst"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "source_map": [
   11
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}