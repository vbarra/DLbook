{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84844573",
   "metadata": {},
   "source": [
    "# Réseaux récurrents\n",
    "\n",
    "## Définition\n",
    "\n",
    "Les réseaux de neurones récurrents (RNN, *Recurrent Neural Networks*)\n",
    "sont des réseaux à propagation avant, permettant de prendre en compte le\n",
    "temps. Comme dans les réseaux classiques, il n'existe pas de cycle, mais\n",
    "les arcs ajoutés pour introduire la notion de temps (les arcs\n",
    "récurrents) peuvent en revanche former des cycles, y compris de longueur\n",
    "1 (connexion d'un neurone avec lui-même). À l'instant $t$, les neurones\n",
    "possédant des arcs récurrents reçoivent en entrée la donnée courante\n",
    "$\\mathbf{x_t}$ et les valeurs des neurones cachés ${h_{t-1}}$ informant\n",
    "sur l'état précédent du réseau. La sortie $\\hat{y}_{t}$ est calculée\n",
    "étant donné l'état $\\mathbf{x_t}$ des neurones cachés à l'instant $t$.\n",
    "La donnée $\\mathbf{x_{t-1}}$ peut influencer $\\hat{y}_{t}$ et la sortie\n",
    "aux instants suivants, à l'aide des arcs récurrents.\n",
    "\n",
    "Deux équations permettent de calculer les quantités nécessaires à\n",
    "l'instant $t$ dans la phase de propagation avant d'un réseau récurrent\n",
    "simple (comme celui de la {numref}`rnn1` gauche) : \n",
    "\n",
    "$$\\begin{aligned}\n",
    "h_t&=&\\sigma\\left ( \\mathbf{W_{hx}^\\top x_t} +  \\mathbf{W_{hh}^\\top x_{t-1}} + b_h\\right)\\\\\n",
    "\\hat{y}_{t}&=&softmax\\left({W_{yh}} h_t + b_y  \\right)\n",
    "\\end{aligned}$$ \n",
    "\n",
    "où $\\mathbf{W_{hx}}$ est la matrice des poids reliant\n",
    "l'entrée à la couche cachée et $\\mathbf{W_{hh}}$ celle des poids des\n",
    "arcs récurrents. Les biais sont notés $b_h$ et $b_y$.\\\n",
    "La dynamique du réseau peut être décrite en dépliant ce réseau dans le\n",
    "temps ({numref}`rnn1` droite). Le réseau devient donc un réseau profond, avec une couche par instant $t$ et un partage de poids au cours du temps. Ce dernier peut\n",
    "donc être entraîné de manière classique par l'algorithme de\n",
    "rétropropagation du gradient, indicé par le temps (*Backpropagation\n",
    "through time*, BPTT algorithm).\n",
    "\n",
    "\n",
    "```{figure} ./images/rnn1.png\n",
    ":name: rnn1\n",
    "Réseau récurrent et sa version dépliée dans le\n",
    "temps.\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "Avec ces réseaux, il est possible de traiter des séquences de longueur\n",
    "quelconque, la taille du modèle étant indépendante de cette longueur.\n",
    "Plusieurs architectures peuvent être déclinées sur ce principe\n",
    "\n",
    "```{figure} ./images/onetomany.png\n",
    ":name: onetomany\n",
    "Architectures \"un vers plusieurs\" , utilisées par exemple en génération de musique ou légendage d'images.\n",
    "```\n",
    "\n",
    "```{figure} ./images/manytoone.png\n",
    ":name: onetomany\n",
    "Architectures \"un vers plusieurs\" , utilisées par exemple en classification de sentiments\n",
    "```\n",
    "```{figure} ./images/manytomany1.png\n",
    ":name: onetomany\n",
    "Architectures \"plusieurs vers plusieurs\", pour la reconnaissance d'entité dans des textes ou annotation de vidéos.\n",
    "```\n",
    "```{figure} ./images/manytomany2.png\n",
    ":name: onetomany\n",
    "Architectures \"plusieurs vers plusieurs\", pour la traduction automatique.\n",
    "```\n",
    "\n",
    "\n",
    "## Entraînement des réseaux récurrents\n",
    "\n",
    "L'apprentissage de dépendances long terme peut être difficile. Les\n",
    "problèmes d'évanescence (*vanishing*) ou d'explosion du gradient peuvent\n",
    "rapidement survenir, lors de la rétropropagation sur plusieurs pas de\n",
    "temps.\n",
    "\n",
    "Prenons un exemple simple pour comprendre : considérons un réseau à un\n",
    "neurone d'entrée, un neurone récurrent caché et un neurone de sortie. On\n",
    "donne au réseau une entrée à l'instant $t_0$ et on calcule l'erreur à\n",
    "l'instant $t>t_0$, en supposant des entrées nulles entre $t_0$ et $t$.\n",
    "Le lien entre les poids au cours du temps fait que le poids sur l'arc\n",
    "récurrent ne change jamais. La contribution de l'entrée au temps $t_0$ à\n",
    "la sortie au temps $t$ deviendra de plus en plus importante, ou se\n",
    "rapprochera de zéro, de manière exponentielle à mesure que $t-t_0$\n",
    "croît. Et la dérivée de l'erreur par rapport à l'entrée explosera ou\n",
    "disparaîtra, selon que le poids de l'arc récurrent a une valeur absolue\n",
    "plus grande ou plus petite que 1 et selon la fonction d'activation du\n",
    "neurone caché (le problème du gradient évanescent est très présent avec\n",
    "une sigmoïde et une activation ReLU force davantage l'explosion).\n",
    "\n",
    "Plusieurs solutions ont été proposées (régularisation, retropropagation\n",
    "tronquée, conception d'architecture et heuristiques) pour résoudre ces\n",
    "problèmes.\n",
    "\n",
    "## Quelques architectures\n",
    "\n",
    "### LSTM\n",
    "\n",
    "Les réseaux *Long Short-Term Memory* (LSTM) ont été introduits en 1997 {cite:p}`HO97` pour résoudre le problème de l'évanescence du gradient. Ce\n",
    "modèle ressemble à un réseau récurrent classique à une couche cachée,\n",
    "mais chaque neurone de la couche cachée est remplacé par une cellule de\n",
    "mémoire.\n",
    "\n",
    "Dans la suite, on note $\\mathbf{x_t}$ l'entrée de la cellule à l'instant\n",
    "$t$, ${h_{t-1}}$ la sortie de la couche cachée calculée au temps $t-1$.\n",
    "Au lieu de calculer une sortie du type\n",
    "$\\sigma\\left( \\mathbf{W^\\top x}+b\\right)$, la cellule contient plusieurs\n",
    "éléments distincts aux fonctions particulières. Les LSTM introduisent la\n",
    "notion de portes, qui sont des unités d'activation de type sigmoïde qui\n",
    "prennent comme arguments $\\mathbf{x_t}$ et ${h_{t-1}}$ et viennent\n",
    "pondérer des valeurs calculées dans la cellule. En particulier, si la\n",
    "valeur d'une porte est nulle, alors le flot est coupé dans le graphe,\n",
    "alors qu'il transite intégralement si la valeur de la porte est égale à\n",
    "1.\n",
    "\n",
    "On retrouve dans une cellule ({numref}`lstm1`) les éléments suivants :\n",
    "\n",
    "-   *Neurone d'entrée* : ce neurone prend en entrée $\\mathbf{x_t}$ et\n",
    "    ${h_{t-1}}$ et calcule, à la manière d'un neurone classique, une\n",
    "    sortie\n",
    "    $g^{t} = \\sigma\\left(\\mathbf{W_C^\\top} \\left[\\mathbf{x_t},  {h_{t-1}}\\right ] +b_C\\right)$.\n",
    "\n",
    "-   *Porte d'entrée* (ou de mise à jour) : la porte calcule\n",
    "    $i^{t} = \\sigma\\left(\\mathbf{W_i}^\\top \\left[\\mathbf{x_t},  {h_{t-1}}\\right ] +b_i\\right)$\n",
    "    et vient pondérer la valeur du neurone d'entrée pour décider de\n",
    "    l'importance à lui donner au temps $t$.\n",
    "\n",
    "-   Porte d'oubli : cette porte calcule\n",
    "    $f^{t}=\\sigma\\left(\\mathbf{W_f}^\\top\\left[\\mathbf{x_t} , {h_{t-1}}\\right ] +b_f\\right)$\n",
    "    et permet au réseau d'oublier son état interne.\n",
    "\n",
    "-   *État interne* : le cœur de la cellule de mémoire est son état\n",
    "    interne, noté $C^{t}$, composé d'un neurone récurrent à poids fixe\n",
    "    unité, assurant que le gradient peut passer par cet arc de\n",
    "    nombreuses fois sans disparaître ou exploser. La mise à jour de\n",
    "    l'état interne est effectuée par une opération du type\n",
    "    $C^{t} =g^{t}.i^{t} + C^{(t-1)}.f^{t}$.\n",
    "\n",
    "-   *Porte de sortie* : la valeur $h_t$ produite par la cellule de\n",
    "    mémoire est calculée comme le produit de $tanh(C^{t})$ par la valeur\n",
    "    de la porte de sortie $o^{t}$. Cette porte sélectionne la part de\n",
    "    $C^{t}$ à fournir en sortie et est calculée par\n",
    "    $o^{t} = \\sigma\\left(\\mathbf{W_o}^\\top\\left[\\mathbf{x_t} , h_{t-1}\\right ] +b_o\\right)$.\n",
    "\n",
    "```{figure} ./images/lstm1.png\n",
    ":name: lstm1\n",
    "Cellule LSTM\n",
    "```\n",
    "\n",
    "En résumé, un LSTM effectue donc les opérations suivantes à l'instant\n",
    "$t$ : \n",
    "\n",
    "$$\\begin{aligned}\n",
    "g^{t} &=& \\sigma\\left(\\mathbf{W_C}^\\top \\left[\\mathbf{x_t} , h_{t-1}\\right ] +b_C\\right)\\\\\n",
    "i^{t} &=& \\sigma\\left(\\mathbf{W_i}^\\top \\left[\\mathbf{x_t} , h_{t-1}\\right ] +b_i\\right)\\\\\n",
    "f^{t} &=& \\sigma\\left(\\mathbf{W_f}^\\top \\left[\\mathbf{x_t} , h_{t-1}\\right ] +b_f\\right)\\\\\n",
    "o^{t} &=& \\sigma\\left(\\mathbf{W_o}^\\top \\left[\\mathbf{x_t}, h_{t-1}\\right ] +b_o\\right)\\\\\n",
    "C^{t} &=&g^{t}.i^{t} + C^{t-1}.f^{t}\\\\\n",
    "h_t &=& o^{t} tanh(C^{t})\n",
    "\\end{aligned}$$\n",
    "\n",
    "### GRU\n",
    "\n",
    "En 2014 {cite:p}`SCHChO1497`, une version simplifiée des réseaux LSTM a été\n",
    "introduite, qui nécessite moins de paramètres. Les GRU (*Gated Recurrent\n",
    "Units*) ({numref}`gru`) sont en effet des réseaux sans mémoire interne $C^{t}$, ni porte\n",
    "de sortie $o^{t}$. Ces réseaux sont composés de deux portes au lieu de\n",
    "trois :\n",
    "\n",
    "-   une *porte reset* $r^{t}$, qui détermine la manière de combiner la\n",
    "    nouvelle entrée au temps $t$ avec la mémoire provenant du temps\n",
    "    $t-1$.\n",
    "\n",
    "-   une *porte de mise à jour* $z^{t}$, qui détermine la quantité de\n",
    "    mémoire précédente qui doit être conservée. Cette porte est la\n",
    "    combinaison des portes d'entrée et d'oubli des LSTM.\n",
    "\n",
    "Formellement : \n",
    "\n",
    "$$\\begin{aligned}\n",
    "r^{t} &=& \\sigma\\left(\\mathbf{W_r}^\\top \\left[\\mathbf{x_t} , h_{t-1}\\right ] +b_r\\right)\\\\\n",
    "z^{t} &=& \\sigma\\left(\\mathbf{W_z}^\\top\\left[\\mathbf{x_t}, h_{t-1}\\right ] +b_z\\right)\\\\\n",
    "\\tilde{h}^{t} &=& tanh\\left(\\mathbf{W}^\\top\\left[\\mathbf{x_t} , r^{t} h_{t-1}\\right ] +b_h \\right)\\\\ \n",
    "h_t&=&\\left(1-z^{t}\\right)h_{t-1} + z^{t} \\tilde{h}^{t}\n",
    "\\end{aligned}$$\n",
    "\n",
    "Si, pour tout $t, r^{t}=1$ et $z^{t}=0$, alors on modélise un réseau\n",
    "récurrent classique.\n",
    "\n",
    "```{figure} ./images/gru.png\n",
    ":name: gru\n",
    "Cellule GRU\n",
    "```\n",
    "\n",
    "### Réseaux récurrents bidirectionnels\n",
    "\n",
    "Les réseaux bidirectionnels ont été décrits pour la première fois en\n",
    "1997 {cite:p}`SCH97`. Dans ces réseaux, deux couches cachées sont présentes,\n",
    "chacune connectée à l'entrée et la sortie. La première couche cachée a\n",
    "des connexions récurrentes depuis le passé vers le futur, tandis que\n",
    "l'autre transmet les activations depuis le futur vers le passé\n",
    "({numref}`bidir`).\n",
    "\n",
    "\n",
    "```{figure} ./images/bidir.png\n",
    ":name: bidir\n",
    "Réseau bidirectionnel\n",
    "```\n",
    "\n",
    "\n",
    "Étant données une entrée et une sortie du réseau (des séquences), le\n",
    "réseau peut être entraîné par rétropropagation après avoir été déplié :\n",
    "\n",
    "$$\\begin{aligned}\n",
    "x_t &=&\\sigma\\left(\\mathbf{W_h}^\\top \\left [\\mathbf{x_t},h_{t-1} \\right ]+b_h \\right)\\\\\n",
    "z_{t} &=&\\sigma\\left(\\mathbf{W_z}^\\top \\left [\\mathbf{x_t},z_{t+1} \\right ]+b_z \\right)\\\\\n",
    "\\hat{y}_{t}&=& softmax\\left(\\mathbf{W_y}^\\top \\left [x_t,z_{t} \\right ]+b_y \\right)\n",
    "\\end{aligned}$$ \n",
    "\n",
    "où $h_t$ (respectivement $z_{t}$) représente la valeur\n",
    "de la couche cachée dans le sens du temps (respectivement dans le sens\n",
    "inverse). Puisque le temps doit être fini dans les deux sens de\n",
    "parcours, les réseaux bidirectionnels ne peuvent traiter que des\n",
    "séquences finies.\n",
    "\n",
    "### Machines de Turing neuronales\n",
    "\n",
    "Les réseaux récurrents sont performants pour construire une\n",
    "représentation implicite de l'information, mais restent relativement peu\n",
    "adaptés à la conservation d'informations explicites (des dates précises\n",
    "par exemple). S'inspirant des mémoires de travail, théorisées par les\n",
    "neurosciences et qui sont responsables du raisonnement inductif et de la\n",
    "création de nouveaux concepts, l'idée est alors d'ajouter à ces modèles\n",
    "une mémoire de travail externe, ce qui permet de découpler la mémoire\n",
    "(assimilable à la RAM d'un ordinateur) des opérations liées à la tâche\n",
    "effectuée par le réseau (assimilable à la CPU). Puisque la mémoire des\n",
    "LSTM est distribuée dans chaque cellule, elle est donc liée au nombre de\n",
    "cellules et à la capacité de calcul et ce modèle ne répond pas\n",
    "directement au problème posé.\n",
    "\n",
    "Graves et al. {cite:p}`Graves14` proposent alors une architecture, appelée\n",
    "machine de Turing neuronale, constituée de deux éléments principaux :\n",
    "une mémoire et un contrôleur doté d'un mécanisme d'attention qui lit et\n",
    "écrit dans cette mémoire. Les accès mémoire sont ici des équivalents\n",
    "analogiques dérivables, pour permettre d'entraîner le contrôleur par\n",
    "descente de gradient. Typiquement, le contrôleur est un réseau de\n",
    "neurones ou un réseau récurrent type LSTM\n",
    "({numref}`turing`).\n",
    "\n",
    "Les têtes de lecture et d'écriture interagissent avec la mémoire. Chaque\n",
    "tête est contrôlée par un vecteur de poids, chaque composante\n",
    "définissant le degré d'interaction de la tête avec la zone mémoire\n",
    "correspondante. Un *mécanisme de mise à jour de ces poids*, composé de\n",
    "quatre opérations, est mis en place pour permettre l'apprentissage du\n",
    "réseau :\n",
    "\n",
    "1.  Le réseau s'intéresse tout d'abord aux zones mémoire proches d'une\n",
    "    clé $k_t$ donnée. Cela permet au modèle de retrouver une information\n",
    "    spécifique, en recherchant si la zone mémoire $M_t(i)$ est proche de\n",
    "    la clé, au sens d'une similarité $K$. Formellement, chaque poids\n",
    "    correspondant à la zone mémoire $i$ est calculé par\n",
    "    $w_t(i) = softmax(\\beta_t K[k_t,M_t(i)])$.\n",
    "\n",
    "2.  Un mécanisme d'interpolation linéaire permet ensuite de mettre à\n",
    "    jour les poids en fonction de leur valeur précédente (pour prendre\n",
    "    plus ou moins en compte l'information issue de la clé, ou au\n",
    "    contraire la valeur précédente du poids) :\n",
    "    $w_t(i)=g_t.w_t(i) + (1-g_t).w_{t-1}(i)$.\n",
    "\n",
    "3.  Un décalage par convolution translate ensuite les poids, à la\n",
    "    manière du décalage de la tête dans une machine de Turing\n",
    "    classique : $w_t(i)=\\displaystyle\\sum_j w_t(j)\\mathbf{s_t}(i-j)$ où\n",
    "    $\\mathbf{s_t}$ est un vecteur qui définit un décalage des poids à\n",
    "    l'instant $t$.\n",
    "\n",
    "4.  Enfin, le vecteur de poids est focalisé :\n",
    "    $w_t(i) = w_t(i)^{\\gamma_t}$, $\\gamma_t>1$.\n",
    "\n",
    "Une fois que la tête a mis à jour les poids, elle interagit avec la\n",
    "mémoire :\n",
    "\n",
    "-   Dans le cas de la tête de lecture, elle calcule une combinaison\n",
    "    linéaire des zones mémoire, pondérées par les poids $w_t(i)$ et\n",
    "    produit le vecteur $\\mathbf{r_t}$, fourni au contrôleur de l'instant\n",
    "    suivant.\n",
    "\n",
    "-   Dans le cas de la tête d'écriture, le contenu de la mémoire est mis\n",
    "    à jour selon la formule\n",
    "    $M_t(i) = M_{t-1}(i)(1-w_t(i)\\mathbf{e_t})+w_t(i)\\mathbf{a_t}$, où\n",
    "    $\\mathbf{e_t}$ est un vecteur d'effacement, dont les composantes\n",
    "    sont dans {0,1} et $\\mathbf{a_t}$ est un vecteur d'ajout.\n",
    "\n",
    "```{figure} ./images/turing.png\n",
    ":name: turing\n",
    "Exemple de machine de Turing nuronale dépliée dans le temps, oà le contrôleur est un LSTM. Les accès en écriture du LSTM dans la mémoire sont représentés par des flèches rouges, les accès en lecture en bleu.\n",
    "```\n",
    "\n",
    "## Implémentation\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt \n",
    "import matplotlib.dates as mdates \n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "```\n",
    "\n",
    "On s'intéresse aux données financières d'Apple ({numref}`donnees`), et plus particulièrement \n",
    "- au prix d'une action à l'ouverture (open) et à la fermeture (close), par jour\n",
    "- au prix le plus bas (low) et haut (high), par jour\n",
    "- à l'ajustement de clôture (adj_close) et le volume de vente (volume), par jour.\n",
    "\n",
    "```python\n",
    "df = pd.read_csv(\"./data/finance.csv\",index_col=0)\n",
    "column_names = list(df.columns.values)\n",
    "df_plot = df.copy()\n",
    "ncols = 2\n",
    "nrows = int(round(df_plot.shape[1] / ncols, 0))\n",
    "fig, ax = plt.subplots(nrows=nrows, ncols=ncols,sharex=True, figsize=(14, 7))\n",
    "for i, ax in enumerate(fig.axes):\n",
    "    ax.plot(np.array(df_plot.iloc[:, i]))\n",
    "    ax.set_ylabel(column_names[i])\n",
    "    ax.tick_params(axis=\"x\", rotation=30, labelsize=10, length=0)\n",
    "    ax.xaxis.set_major_locator(mdates.AutoDateLocator())\n",
    "fig.tight_layout()\n",
    "```\n",
    "\n",
    "```{figure} ./images/donnees.png\n",
    ":name: donnees\n",
    "Cellule Données financières d'Apple (source : Yahoo finance)\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "```{bibliography}\n",
    ":style: unsrt\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "md:myst",
   "text_representation": {
    "extension": ".md",
    "format_name": "myst"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "source_map": [
   11
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}