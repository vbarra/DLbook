{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4919299",
   "metadata": {},
   "source": [
    "# Réseaux convolutifs\n",
    "\n",
    "## Introduction\n",
    "\n",
    "### Inspiration biologique\n",
    "\n",
    "Un réseau de neurones convolutif (CNN, *Convolutional Neural Network*) est un type de réseau de neurones artificiels acyclique à\n",
    "propagation avant, dans lequel le motif de connexion entre les neurones\n",
    "est inspiré par le cortex visuel des animaux. Les neurones de cette\n",
    "région du cerveau sont arrangés de sorte à ce qu'ils correspondent à des\n",
    "régions (appelés champs réceptifs) qui se chevauchent lors du pavage du\n",
    "champ visuel. Ils sont de plus organisés de manière hiérarchique, en\n",
    "couches (aire visuelle primaire V1, secondaire V2, puis aires V3, V4, V5\n",
    "et V6, gyrus temporal inférieur), chacune des couches étant spécialisée\n",
    "dans une tâche, de plus en plus abstraite en allant de l'entrée vers la\n",
    "sortie. En simplifiant à l'extrême, une fois que les signaux lumineux\n",
    "sont reçus par la rétine et convertis en potentiels d'action :\n",
    "\n",
    "-   L'aire primaire V1 s'intéresse principalement à la détection de\n",
    "    contours, ces contours étant définis comme des zones de fort\n",
    "    contraste de signaux visuels reçus.\n",
    "\n",
    "-   L'aire V2 reçoit les informations de V1 et extrait des informations\n",
    "    telles que la fréquence spatiale, l'orientation, ou encore la\n",
    "    couleur.\n",
    "\n",
    "-   L'aire V4, qui reçoit des informations de V2, mais aussi de V1\n",
    "    directement, détecte des caractéristiques plus complexes et\n",
    "    abstraites liées par exemple à la forme.\n",
    "\n",
    "-   Le gyrus temporal inférieur est chargé de la partie sémantique\n",
    "    (reconnaissance des objets), à partir des informations reçues des\n",
    "    aires précédentes et d'une mémoire des informations stockées sur des\n",
    "    objets.\n",
    "\n",
    "L'architecture et le fonctionnement des réseaux convolutifs sont\n",
    "inspirés par ces processus biologiques. Ces réseaux consistent en un\n",
    "empilage multicouche de perceptrons, dont le but est de prétraiter de\n",
    "petites quantités d'informations. Les réseaux convolutifs ont de larges\n",
    "applications dans la reconnaissance d'image et vidéo, les systèmes de\n",
    "recommandation et le traitement du langage naturel.\n",
    "\n",
    "Un réseau convolutif se compose de deux types de neurones, agencés en\n",
    "couches traitant successivement l'information. Dans le cas du traitement\n",
    "de données de type images, on a ainsi :\n",
    "\n",
    "-   des *neurones de traitement*, qui traitent une portion limitée de\n",
    "    l'image (le champ réceptif) au travers d'une fonction de\n",
    "    convolution;\n",
    "\n",
    "-   des *neurones* de mise en commun des sorties dits *d'agrégation\n",
    "    totale ou partielle* (*pooling*).\n",
    "\n",
    "Un traitement correctif non linéaire est appliqué entre chaque couche\n",
    "pour améliorer la pertinence du résultat. L'ensemble des sorties d'une\n",
    "couche de traitement permet de reconstituer des images intermédiaires,\n",
    "dite cartes de caractéristiques (feature maps), qui servent de base à la\n",
    "couche suivante. Les couches et leurs connexions apprennent des niveaux\n",
    "d'abstraction croissants et extraient des caractéristiques de plus en\n",
    "plus haut niveau des données d'entrée.\n",
    "\n",
    "Dans la suite, le propos sera illustré sur des images 2D en niveaux de\n",
    "gris, de taille $n_1 \\times n_2$ : \n",
    "\n",
    "$$\\begin{aligned}\n",
    "    \\mathbf{I} : [\\![1,n_1]\\!]\\times [\\![1, n_2]\\!] &\\rightarrow&  \\mathbb{R}\\\\\n",
    "     (i,j) &\\mapsto& I_{i,j}\n",
    "\\end{aligned}$$ \n",
    "\n",
    "$\\mathbf{I}$ sera indifféremment vue comme une fonction ou une matrice.\n",
    "\n",
    "### Convolution discrète \n",
    "\n",
    "Pour reproduire la notion de champ réceptif, et ainsi permettre aux\n",
    "neurones de détecter des caractéristiques de petite taille mais porteurs\n",
    "d'information, l'idée est de laisser un neurone caché voir et traiter\n",
    "seulement une petite portion de l'image qu'il prend en entrée. L'outil\n",
    "retenu dans les réseaux convolutifs est la convolution discrète.\n",
    "\n",
    "````{prf:definition} Convolution discrète\n",
    "Soient\n",
    "$h_1,h_2\\in\\mathbb{N}, \\mathbf{K} \\in \\mathbb{R}^{(2h_1+1) \\times (2h_2+1)}$.\n",
    "La convolution discrète de $\\mathbf{I}$ par le filtre $\\mathbf{K}$ est\n",
    "donnée par : \n",
    "\n",
    "$$\\begin{aligned}\n",
    "    \\label{eq:convolution}\n",
    "    \\left(\\mathbf{K} \\ast \\mathbf{I}\\right)_{r,s} = \\displaystyle\\sum _{u = -h_1} ^{h_1} \\displaystyle\\sum _{v = -h_2}^{h_2} K_{u,v} I_{r+u,s+v}\n",
    "\\end{aligned}$$ \n",
    "\n",
    "où $\\mathbf{K}$ est donné par : \n",
    "\n",
    "$$\\begin{aligned}\n",
    "    \\mathbf{K} =\n",
    "    \\begin{pmatrix}\n",
    "        K_{-h_1,-h_2} & \\ldots & K_{-h_1,h_2}\\\\\n",
    "        \\vdots & K_{0,0} & \\vdots\\\\\n",
    "        K_{h_1,-h_2} & \\ldots & K_{h_1,h_2}\\\\\n",
    "    \\end{pmatrix}.\n",
    "\\end{aligned}$$\n",
    "````\n",
    "\n",
    "La taille du filtre $(2h_1+1) \\times (2h_2+1)$ précise le champ visuel\n",
    "capturé et traité par $\\mathbf{K}$.\\\n",
    "Lorsque $\\mathbf{K}$ parcourt $\\mathbf{I}$, le déplacement du filtre est\n",
    "réglé par deux paramètres de *stride* (horizontal et vertical). Un\n",
    "stride de 1 horizontal (respectivement vertical) signifie que\n",
    "$\\mathbf{K}$ se déplace d'une position horizontale (resp. verticale) à\n",
    "chaque application de l'équation précédente. Les valeurs de stride peuvent également\n",
    "être supérieures et ainsi sous-échantillonner $\\mathbf{I}$.\n",
    "\n",
    "Le comportement du filtre sur les bords de $\\mathbf{I}$ doit également\n",
    "être précisé, par l'intermédiaire d'un paramètre de *padding*. Si\n",
    "l'image convoluée $\\left(\\mathbf{K} \\ast \\mathbf{I}\\right)$ doit\n",
    "posséder la même taille que $\\mathbf{I}$, alors $2h_1$ lignes de 0\n",
    "($h_1$ en haut et $h_1$ en bas) et $2h_2$ colonnes de 0 ($h_2$ à gauche\n",
    "et $h_2$ à droite) doivent être ajoutées. Dans le cas où la convolution\n",
    "est réalisée sans padding, l'image convoluée est de taille\n",
    "$(n_1-2h_1)\\times (n_2-2h_2)$.\n",
    "\n",
    "## Définition des couches\n",
    "\n",
    "Nous introduisons ici les principaux types de couches utilisées dans les\n",
    "réseaux convolutifs. L'assemblage de ces couches permet de construire\n",
    "des architectures complexes pour la classification ou la régression.\n",
    "\n",
    "### Couche de convolution\n",
    "\n",
    "Soit $l\\in\\mathbb{N}$ une couche de convolution. L'entrée de la couche\n",
    "$l$ est composée de $n^{(l-1)}$ cartes provenant de la couche\n",
    "précédente, de taille $n_1^{(l-1)} \\times n_2^{(l-1)}$. Dans le cas de\n",
    "la couche d'entrée du réseau ($l = 1$), l'entrée est l'image\n",
    "$\\mathbf{I}$. La sortie de la couche $l$ est formée de $n^{(l)}$ cartes\n",
    "de taille $n_1^{(l)} \\times n_2^{(l)}$. La $i^{\\text{e}}$ carte de la\n",
    "couche $l$, notée $\\mathbf{Y_i^{(l)}}$, se calcule comme :\n",
    "\n",
    "$$\\begin{aligned}\n",
    "    \\mathbf{Y_i^{(l)}} = \\mathbf{B^{(l)}_{i}} + \\displaystyle\\sum _{j = 1}^{n^{(l-1)}} \\mathbf{K^{(l)}_{i,j}} \\ast \\mathbf{Y_j^{(l-1)}}\n",
    "\\end{aligned}$$ \n",
    "\n",
    "où $\\mathbf{B_i^{(l)}}$ est une matrice de biais et\n",
    "$\\mathbf{K^{(l)}_{i,j}}$ est le filtre de taille\n",
    "$(2h_1^{(l)} + 1) \\times (2h_2^{(l)} + 1)$ connectant la $j^{\\text{e}}$\n",
    "carte de la couche $(l-1)$ à la $i^{\\text{e}}$ carte de la couche $l$\n",
    "({numref}`cnn1`).\n",
    "\n",
    "\n",
    "```{figure} ./images/cnn1.png\n",
    ":name: cnn1\n",
    ":width: 500px\n",
    ":align: center\n",
    "Illustration des calculs effectués dans une opération de convolution discrète. Le pixel (2,2) de\n",
    "l’image ${\\mathbf Y_i^{l}}$ est une combinaison linéaire des pixels $(i,j)\\in[\\![1,3]\\!]^2$ de ${\\mathbf Y_i^{l-1}}$ les coefficients de la combinaison étant portés par le filtre $\\mathbf K$.\n",
    "```\n",
    "\n",
    "\n",
    "$n_1^{(l)}$ et $n_2^{(l)}$ doivent prendre en compte les effets de bords\n",
    ": lors du calcul de la convolution, seuls les pixels dont la somme est\n",
    "définie avec des indices positifs doivent être traités. Dans le cas où\n",
    "le padding n'est pas utilisé, les cartes de sortie ont donc une taille\n",
    "de $n_1^{(l)} = n_1^{(l-1)} - 2h_1^{(l)}$ et\n",
    "$n_2^{(l)} = n_2^{(l-1)} - 2h_2^{(l)}$.\n",
    "\n",
    "Souvent, les filtres utilisés pour calculer $\\mathbf{Y_i^{(l)}}$ sont\n",
    "les mêmes, i.e. $\\mathbf{K_{i,j}^{(l)}} = \\mathbf{K _{i,k}^{(l)}}$ pour\n",
    "$j \\neq k$. De plus, la somme dans l'équation\n",
    "de la convolution peut être conduite sur un sous-ensemble des\n",
    "cartes d'entrée.\n",
    "\n",
    "Il est possible de mettre en correspondance la couche de convolution, et\n",
    "l'opération qu'elle effectue, avec un perceptron\n",
    "multicouche. Pour cela, il suffit de réécrire l'équation : chaque carte $\\mathbf{Y_i^{(l)}}$ de la\n",
    "couche $l$ est formée de $n_1^{(l)} \\cdot n_2^{(l)}$ neurones organisés\n",
    "dans un tableau à deux dimensions. Le neurone en position $(r,s)$\n",
    "calcule : \n",
    "\n",
    "$$\\begin{aligned}\n",
    "    \\left(\\mathbf{Y_i^{(l)}}\\right)_{r,s} &= \\left(\\mathbf{B_i^{(l)}}\\right)_{r,s} + \\displaystyle\\sum _{j = 1}^{n^{(l-1)}} \\left(\\mathbf{K^{(l)}_{i,j} }\\ast \\mathbf{Y_j^{(l-1)}}\\right)_{r,s}\\\\\n",
    "    &= \\left(\\mathbf{B_i^{(l)}}\\right)_{r,s} + \\displaystyle\\sum _{j = 1}^{n^{(l-1)}} \\displaystyle\\sum _{u = - h_1^{(l)}} ^{h_1^{(l)}} \\displaystyle\\sum _{v = - h_2^{(l)}} ^{h_2^{(l)}} \\left(\\mathbf{K^{(l)}_{i,j}}\\right)_{u,v} \\left(\\mathbf{Y_j^{(l-1)}}\\right)_{r+u,s+v}\n",
    "\\end{aligned}$$\n",
    "\n",
    "Les paramètres du réseau à entraîner (poids) peuvent alors être trouvés\n",
    "dans les filtres $\\mathbf{K^{(l)}_{i,j}}$ et les matrices de biais\n",
    "$\\mathbf{B_i^{(l)}}$.\n",
    "\n",
    "Comme nous le verrons plus loin, un sous-échantillonnage est utilisé\n",
    "pour diminuer l'influence du bruit et des distorsions dans les images.\n",
    "Le sous-échantillonnage peut être également réalisé simplement avec des\n",
    "paramètres de stride, en sautant un nombre fixe de pixels dans les\n",
    "dimensions horizontale (saut $s_1^{(l)}$) et verticale (saut\n",
    "$s_2^{(l)}$) avant d'appliquer de nouveau le filtre. La taille des\n",
    "images de sortie est alors : \n",
    "\n",
    "$$\\begin{aligned}\n",
    "    n_1^{(l)} = \\frac{n_1^{(l-1)} - 2h_1^{(l)}}{s_1^{(l)} + 1}\\quad \\text{ et }\\quad n_2^{(l)} = \\frac{n_2^{(l-1)} - 2h_2^{(l)}}{s_2^{(l)} + 1}.\n",
    "\\end{aligned}$$\n",
    "\n",
    "\n",
    "Un point clé des réseaux convolutifs est d'exploiter la corrélation\n",
    "spatiale des données. L'utilisation des noyaux permet d'alléger le\n",
    "modèle, plutôt que d'utiliser des couches complètement connectées.\n",
    "\n",
    "### Couche non linéaire\n",
    "\n",
    "Pour augmenter le pouvoir d'expression\n",
    "des réseaux profonds, on utilise des couches non linéaires. Les entrées\n",
    "d'une couche non linéaire sont $n^{(l-1)}$ cartes et ses sorties\n",
    "$n^{(l)} = n^{(l-1)}$ cartes $\\mathbf{Y_i^{(l)}}$, de taille\n",
    "$n_1^{(l-1)} \\times n_2^{(l-1)}$ telles que $n_1^{(l)} = n_1^{(l-1)}$ et\n",
    "$n_2^{(l)} = n_2^{(l-1)}$, données par\n",
    "$\\mathbf{Y_i^{(l)}}$ = $f \\left(\\mathbf{Y_i^{(l-1)}}\\right)$, où $f$ est\n",
    "la fonction d'activation utilisée dans la couche $l$.\n",
    "\n",
    "La {numref}`tabact` du cours sur les perceptrons multicouches présente quelques fonctions d'activation classiques.\n",
    "\n",
    "En apprentissage profond, il a été reporté que la sigmoïde et la\n",
    "tangente hyperbolique avaient des performances moindres que la fonction\n",
    "d'activation *softsign* : \n",
    "\n",
    "$$\\begin{aligned}\n",
    "    \\mathbf{Y_i^{(l)}}  = \\frac{1}{1+ \\left|\\mathbf{Y_i^{(l-1)}} \\right |}.\n",
    "\\end{aligned}$$ \n",
    "\n",
    "En effet, les valeurs des pixels des cartes\n",
    "$\\mathbf{Y_i^{(l-1)}}$ arrivant près des paliers de saturation de ces\n",
    "fonctions donnent des gradients faibles, qui ont tendance à s'annuler\n",
    "(problème du *gradient évanescent* ou *vanishing gradient*) lors de la\n",
    "phase d'apprentissage par rétropropagation du gradient. Une autre\n",
    "fonction, non saturante elle, est très largement utilisée. Il s'agit de\n",
    "la fonction ReLU (Rectified Linear Unit) {cite:p}`Nair10` : \n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\mathbf{Y_i^{(l)}} = max\\left (0,\\mathbf{Y_i^{(l-1)}}\\right )\n",
    "\\end{aligned}$$ \n",
    "\n",
    "Les neurones utilisant la fonction ReLU sont appelés neurones linéaires rectifiés. Glorot\n",
    "et Bengio {cite:p}`Glorot11` ont montré que l'utilisation d'une couche ReLU en\n",
    "tant que couche non linéaire permettait un entraînement efficace de\n",
    "réseaux profonds sans pré-entraînement non supervisé. Plusieurs\n",
    "variantes de cette fonction existent, par exemple pour assurer une\n",
    "différentiabilité en 0 ou pour proposer des valeurs non nulles pour des\n",
    "valeurs négatives de l'argument. \n",
    "\n",
    "### Couches de normalisation\n",
    "\n",
    "La normalisation prend aujourd'hui une place de plus en plus importante,\n",
    "notamment depuis les travaux de Ioffe et Szegedy {cite:p}`Ioffe15`. Les auteurs\n",
    "suggèrent qu'un changement dans la distribution des activations d'un\n",
    "réseau profond, résultant de la présentation d'un nouveau mini batch\n",
    "d'exemples, ralentit le processus d'apprentissage. Pour pallier ce\n",
    "problème, chaque activation du mini batch est centrée et normée\n",
    "(variance unité), la moyenne et la variance étant calculées sur le mini\n",
    "batch entier, indépendamment pour chaque activation. Des paramètres\n",
    "d'offset $\\beta$ et multiplicatif $\\gamma$ sont alors appliqués pour\n",
    "normaliser les données d'entrée\n",
    "({prf:ref}`norm`).\n",
    "\n",
    "```{prf:algorithm} Normalisation par batch sur la présentation d'un mini batch $\\mathcal{B}$\n",
    ":label: norm\n",
    "Entrées : valeurs de l'activation $x$ sur un mini batch $\\mathcal{B} = \\{x_1\\cdots x_m\\}$,  Paramètres $\\beta,\\gamma$ à apprendre\n",
    "\n",
    "Sortie : Données normalisées $\\{y_1\\cdots  y_m\\}=BatchNorm_{\\gamma,\\beta}(x_1\\cdots x_m)$\n",
    "\n",
    "1. $\\mu_\\mathcal{B} = \\frac{1}{m}\\displaystyle\\sum_{i=1}^m x_i$\n",
    "2. $\\sigma^2_\\mathcal{B} = \\frac{1}{m}\\displaystyle\\sum_{i=1}^m \\left (x_i-\\mu_\\mathcal{B}\\right )^2$\n",
    "3. Pour $i=1$ à $m$\n",
    "    1. $y_i = \\gamma \\frac{x_i-\\mu_\\mathcal{B}}{\\sqrt{\\sigma^2_\\mathcal{B}+\\epsilon}}+\\beta$\n",
    "```\n",
    "Lorsque la descente de gradient est achevée, un post apprentissage est\n",
    "appliqué dans lequel la moyenne et la variance sont calculées sur\n",
    "l'ensemble d'entraînement et remplacent $\\mu_\\mathcal{B}$ et\n",
    "$\\sigma^2_\\mathcal{B}$ ({prf:ref}`norm2`).\n",
    "\n",
    "```{prf:algorithm} Normalisation par batch d'un réseau\n",
    ":label: norm2\n",
    "{\n",
    "Entrées : {un réseau $N$,  un ensemble d'activations $\\{x^1\\cdots x^K\\}$\n",
    "\n",
    "1. $N_n=N$\n",
    "2. Pour $i=1$ à $K$\n",
    "    1. Calculer $y^i=BN_{\\gamma,\\beta}(x^i)$ à l'aide de l'{prf:ref}`norm`\n",
    "    2. Modifier chaque couche de $N_n$ : l'entrée $y^i$ remplace l'entrée $x^i$\n",
    "3. Entraîner $N_n$ pour optimiser les paramètres de $N$ et $(\\gamma^i,\\beta^i)_{i\\in[\\![1,K]\\!]}$\n",
    "4. $N^f = N_n$\n",
    "5. Pour $i=1$ à $K$\n",
    "    1. Utiliser $N^f$ sur des batchs $\\mathcal{B}$ de taille $m$\n",
    "    2. Calculer la moyenne des moyennes $\\bar{x^i}$ et des variances $Var(x^i)$\n",
    "    3. Remplacer dans $N^f$ la transformation $y^i=BN_{\\gamma,\\beta}(x^i)$ par \n",
    "\n",
    "$$y^i=\\frac{\\gamma^i}{\\sqrt{Var(x^i)+\\epsilon}}x^i+\\left(\\beta^i -\\frac{\\gamma^i \\bar{x^i}}{\\sqrt{Var(x^i)+\\epsilon}}\\right)$$ \n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Couche d'agrégation et de sous-échantillonnage\n",
    "\n",
    "Le sous-échantillonnage (pooling) des cartes obtenues par les couches\n",
    "précédentes a pour objectif d'assurer une robustesse au bruit et aux\n",
    "distorsions.\n",
    "\n",
    "La sortie d'une couche d'agrégation $l$\n",
    "({numref}`pooling`) est composée de $n^{(l)} = n^{(l-1)}$ cartes\n",
    "de taille réduite. En général, l'agrégation est effectuée en déplaçant\n",
    "dans les cartes d'entrée une fenêtre de taille $2p \\times 2p$ toutes les\n",
    "$q$ positions (il y a recouvrement si $q < p$ et non recouvrement\n",
    "sinon), et en calculant, pour chaque position de la fenêtre, une seule\n",
    "valeur, affectée à la position centrale dans la carte de sortie. \n",
    "\n",
    "```{figure} ./images/pooling.png\n",
    ":name: pooling\n",
    ":width: 500px\n",
    ":align: center\n",
    "Couche d’agrégation et de sous-échantillonnage $l$. Chacune des $n^{(l−1)}$ cartes de la couche $l-1$ est traitée individuellement. Chaque neurone des $n^(l) = n^{(l−1)}$ cartes de sortie est la moyenne (ou le maximum) des valeurs contenues dans une fenêtre de taille donnée dans la carte correspondante de la couche $l-1$.\n",
    "```\n",
    "\n",
    "\n",
    "On distingue généralement deux types d'agrégation :\n",
    "\n",
    "- La moyenne : on utilise un filtre $\\mathbf{K_B}$ de taille\n",
    "    $(2h_1 + 1)\\times (2h_2 + 1)$ défini par \n",
    "\n",
    "    $$\\left(\\mathbf{K_B}\\right)_{r,s} = \\frac{1}{(2h_1 + 1)(2h_2 + 1)}$$\n",
    "\n",
    "- Le maximum : la valeur maximum dans la fenêtre est retenue.\n",
    "\n",
    "Le maximum est souvent utilisé pour assurer une convergence rapide\n",
    "durant la phase d'entraînement. L'agrégation avec recouvrement, elle,\n",
    "semble assurer une réduction du phénomène de surapprentissage\n",
    "\n",
    "### Couche complètement connectée\n",
    "\n",
    "Si $l$ et $(l-1)$ sont des couches complètement connectées, l'équation :\n",
    "\n",
    "$$\\begin{aligned}\n",
    "    z_i^{(l)} = \\sum _{k = 0} ^{m^{(l-1)}} w_{i,k}^{(l)} y_k^{(l-1)}\\quad \\text{ où }\\quad \\mathbf{Z^{(l)}} = \\mathbf{W^{(l)}} \\mathbf{Y^{(l-1)}}\n",
    "\\end{aligned}$$ \n",
    "\n",
    "avec $\\mathbf{Z^{(l)}}$, $\\mathbf{W^{(l)}}$ et\n",
    "$\\mathbf{Y^{(l-1)}}$ les représentations vectorielle et matricielle des\n",
    "entrées $z_i^{(l)}$, des poids $w_{i,k}^{(l)}$ et des sorties\n",
    "$y_k^{(l-1)}$, permet de relier ces deux couches.\n",
    "\n",
    "Dans le cas contraire, la couche $l$ attend $n^{(l-1)}$ entrées de\n",
    "taille $n_1^{(l-1)} \\times n_2^{(l-1)}$ et le $i^{\\text{e}}$ neurone de\n",
    "la couche $l$ calcule : \n",
    "\n",
    "$$\\begin{aligned}\n",
    "    y_i^{(l)} = f\\left(z_i^{(l)}\\right)\\quad\\text{ avec }\\quad z_i^{(l)} = \\displaystyle\\sum _{j = 1}^{n^{(l-1)}} \\displaystyle\\sum _{r = 1} ^{n_1^{(l-1)}} \\displaystyle\\sum _{s = 1}^{n_2^{(l-1)}} w_{i,j,r,s}^{(l)} \\left(\\mathbf{ Y_j^{(l-1)}} \\right)_{r,s}\n",
    "\\end{aligned}$$ \n",
    "\n",
    "où $w_{i,j,r,s}^{(l)}$ est le poids connectant le\n",
    "neurone en position $(r,s)$ de la $j^{\\text{e}}$ carte de la couche\n",
    "$(l - 1)$ au $i^{\\text{e}}$ neurone de la couche $l$.\n",
    "\n",
    "En pratique, les réseaux convolutifs sont utilisés pour apprendre une\n",
    "hiérarchie dans les données et la (ou les) couche(s) complètement\n",
    "connectée(s) est(sont) utilisée(s) en bout de réseau pour des tâches de\n",
    "classification ou de régression.\n",
    "\n",
    "Une couche de classification classiquement mise en œuvre utilise le\n",
    "classifieur *softmax*, qui généralise la régression logistique au cas\n",
    "multiclasse ($k$ classes). L'ensemble d'apprentissage\n",
    "${\\mathcal E}_a = \\left \\{(\\mathbf{x^{(i)}}, y^{(i)}),i \\in[\\![1,m]\\!]\\right \\}$\n",
    "est donc tel que $y^{(i)}\\in[\\![1,k]\\!]$ et le classifieur estime\n",
    "la probabilité $P(y^{(i)}=j |\\mathbf{x^{(i)}})$ pour chaque classe\n",
    "$j\\in [\\![1,k]\\!]$. Le classifieur softmax calcule cette probabilité selon\n",
    ":\n",
    "\n",
    "$$\\forall j\\in[\\![1,k]\\!]\\quad P(y^{(i)}=j | \\mathbf{x^{(i)}},\\mathbf{W}) = \\frac{e^{\\mathbf{W_j^\\top x^{(i)}}}}{\\displaystyle\\sum_{l=1}^k e^{\\mathbf{W_l^\\top}\\mathbf{x^{(i)}}}}$$\n",
    "\n",
    "où $\\mathbf{W}$ est la matrice des paramètres du modèle (les poids). Ces\n",
    "paramètres sont obtenus en minimisant une fonction de coût, qui peut par\n",
    "exemple s'écrire :\n",
    "\n",
    "$$J(\\mathbf{W}) =- \\frac{1}{m}\\displaystyle\\sum_{i=1}^m \\displaystyle\\sum_{j=1}^k \\mathbb{I}_{y^{(i)}=j}log\\left ( \\frac{e^{\\mathbf{W_j^\\top x^{(i)}}}}{\\displaystyle\\sum_{l=1}^k e^{\\mathbf{W_l^\\top x^{(i)}}}}\\right ) + \\frac{\\lambda}{2}\\displaystyle\\sum_{i=1}^n \\displaystyle\\sum_{j=1}^k W_{ji}^2\n",
    "$$ \n",
    "\n",
    "où $\\lambda$ est un paramètre de régularisation contrôlant le second terme du coût qui pénalise les grandes valeurs des poids (régularisation $\\ell_2$).\n",
    "\n",
    "## Régularisation\n",
    "\n",
    "Un des enjeux principaux en apprentissage automatique est de construire\n",
    "des algorithmes ayant une bonne capacité de généralisation. Les\n",
    "stratégies mises en œuvre pour arriver à cette fin rentrent dans la\n",
    "catégorie générale de la régularisation et de nombreuses méthodes sont\n",
    "aujourd'hui proposées en ce sens. La régularisation a déjà été abordée dans le chapitre consacré aux perceptrons multicouches (voir section {ref}`content:references:reg`). Nous faisons ici un focus sur trois\n",
    "stratégies largement utilisées en apprentissage profond.\n",
    "\n",
    "### Régularisation de la fonction de coût\n",
    "\n",
    "L'équation précédente est un exemple de régularisation de la\n",
    "fonction de coût, utilisée lors de la phase d'entraînement. À la\n",
    "fonction d'erreur est ajoutée une fonction des poids du réseau, qui peut\n",
    "prendre de multiples formes. Les deux principales stratégies sont :\n",
    "\n",
    "-   La régularisation $\\ell_2$ (ou ridge regression), qui force les\n",
    "    poids à avoir une faible valeur absolue : un terme de régularisation\n",
    "    fonction de la norme $\\ell_2$ de la matrice des poids est ajouté. On parle souvent de *weight decay*.\n",
    "\n",
    "-   La régularisation $\\ell_1$, qui tend à rendre épars le réseau\n",
    "    profond, *i.e.* à imposer à un maximum de poids de s'annuler. Un\n",
    "    terme de régularisation, somme pondérée des valeurs absolues des\n",
    "    poids, est ajouté à la fonction objectif.\n",
    "\n",
    "### Dropout\n",
    "\n",
    "Les techniques de dropout se rapprochent des stratégies classiques de\n",
    "bagging en apprentissage automatique. L'objectif est d'entraîner un\n",
    "ensemble constitué de tous les sous-réseaux qui peuvent être construits\n",
    "en supprimant des neurones (hors neurones d'entrée et de sortie) du\n",
    "réseau initial. Si le réseau comporte $|\\mathbf{W}|$ neurones cachés, il\n",
    "existe ainsi $2^{|\\mathbf{W}|}$ modèles possibles. En pratique, les\n",
    "neurones cachés se voient perturbés par un bruit binomial, qui a pour\n",
    "effet de les empêcher de fonctionner en groupe et de les rendre, au\n",
    "contraire, plus indépendants. Le phénomène de surapprentissage est ainsi\n",
    "fortement réduit sur le réseau, qui doit décomposer les entrées en\n",
    "caractéristiques pertinentes, indépendamment les unes des autres. Les\n",
    "réseaux construits par dropout partagent partiellement leurs paramètres,\n",
    "ce qui diminue l'empreinte mémoire de la méthode.\n",
    "\n",
    "Lors de la phase de prédiction, le réseau complet est utilisé, mais les\n",
    "neurones cachés sont pondérés par la fraction de bruit utilisé pendant\n",
    "l'apprentissage (*i.e.* pour chaque neurone le nombre de fois où il a\n",
    "été supprimé d'un sous-réseau, rapporté au nombre total de réseaux),\n",
    "afin de conserver la valeur moyenne des activations des neurones\n",
    "identiques à celles durant l'apprentissage.\n",
    "\n",
    "Notons qu'il est également possible d'éteindre non pas un neurone, mais\n",
    "un poids. La stratégie correspondante est appelée DropConnect.\n",
    "\n",
    "### Partage de paramètres\n",
    "\n",
    "La régularisation de la fonction de coût permet d'imposer aux poids\n",
    "certaines contraintes (par exemple de rester faibles en amplitude pour\n",
    "la régularisation $\\ell_2$, ou de s'annuler pour la régularisation\n",
    "$\\ell_1$). Il peut également être intéressant d'imposer certains a\n",
    "priori sur les poids, par exemple une dépendance entre les valeurs des\n",
    "paramètres.\n",
    "\n",
    "Une dépendance classique consiste à imposer que les valeurs de certains\n",
    "poids soient proches les unes des autres (dans le cas par exemple où\n",
    "deux modèles de classification $M_1$ et $M_2$, de paramètres\n",
    "$\\mathbf{W_1}$ et $\\mathbf{W_2}$, opèrent sur des données similaires et\n",
    "sur des classes identiques) et, là encore, une stratégie de pénalisation\n",
    "de la fonction objectif peut être utilisée. Cependant, il est plus\n",
    "courant dans ce cas d'imposer que les paramètres soient égaux (dans\n",
    "l'exemple précédent imposer $\\mathbf{W_1}=\\mathbf{W_2}$) et d'arriver à\n",
    "une stratégie dite de partage des paramètres. Dans le cas des réseaux\n",
    "convolutifs utilisés en vision, cette régularisation est assez intuitive\n",
    "puisque les entrées (images) possèdent de nombreuses propriétés\n",
    "invariantes par transformations affines (une image de voiture reste une\n",
    "image de voiture, même si l'image est translatée ou mise à l'échelle ({numref}`partage`)).\n",
    "Le réseau exploite alors ce partage de\n",
    "paramètres, en calculant une même caractéristique (un neurone et son\n",
    "poids) à différentes positions dans l'image. De ce fait, le nombre de\n",
    "paramètres est drastiquement réduit, ainsi que l'empreinte mémoire du\n",
    "réseau appris.\n",
    "\n",
    "\n",
    "```{figure} ./images/partage.png\n",
    ":name: partage\n",
    ":width: 500px\n",
    ":align: center\n",
    "Partage de paramètres : les neurones voient des champs réceptifs distincts, mais partagent les mêmes paramètres (poids). Leur capacité de détection d’un triangle restera la même, quelle que soit la position de l'objet dans l’image.\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "## Initialisation\n",
    "\n",
    "Une initialisation convenable des poids est essentielle pour assurer une\n",
    "convergence de la phase d'entraînement. Un choix arbitraire des poids (à\n",
    "zéro, à de petites ou grandes valeurs aléatoires) peut ralentir, voire\n",
    "causer de la redondance dans le réseau (problème de la symétrie). Ces aspects ont déjà été abordés dans la section {ref}`content:references:initW`), où l'initialisation de Xavier\n",
    "\n",
    "$$\\begin{aligned}\n",
    "    - \\frac{\\sqrt{6}}{\\sqrt{m^{(l-1)} + m^{(l)}}} < w_{i,j}^{(l)} < \\frac{\\sqrt{6}}{\\sqrt{m^{(l-1)} + m^{(l)}}}.\n",
    "\\end{aligned}$$\n",
    "\n",
    "a été notamment détaillée. \n",
    "\n",
    "## Apprentissage\n",
    "\n",
    "La présence de nombreuses couches cachées va permettre de calculer des\n",
    "caractéristiques beaucoup plus complexes et informatives des entrées.\n",
    "Chaque couche calculant une transformation non linéaire de la couche\n",
    "précédente, le pouvoir de représentation de ces réseaux s'en trouve\n",
    "amélioré. On peut par exemple montrer qu'il existe des fonctions qu'un\n",
    "réseau à $k$ couches peut représenter de manière compacte (avec un\n",
    "nombre de neurones cachés qui est polynomial en le nombre des entrées),\n",
    "alors qu'un réseau à $k-1$ couches ne peut pas le faire, à moins d'avoir\n",
    "une combinatoire exponentielle sur le nombre de neurones cachés.\n",
    "\n",
    "### Le problème de l'entraînement\n",
    "\n",
    "Si l'intérêt de ces réseaux est manifeste, la complexité de leur\n",
    "utilisation vient de l'étape d'apprentissage. Jusqu'à récemment,\n",
    "l'algorithme utilisé était classique et consistait en une initialisation\n",
    "aléatoire des poids du réseau, suivie d'un entraînement sur un ensemble\n",
    "d'apprentissage, en minimisant une fonction objectif. Cependant, dans le\n",
    "cas des réseaux profonds, cette approche peut ne pas être adaptée :\n",
    "\n",
    "-   Les données étiquetées doivent être en nombre suffisant pour\n",
    "    permettre un entraînement efficace, d'autant plus que le réseau est\n",
    "    complexe. Dans le cas contraire, un surapprentissage peut notamment\n",
    "    être induit.\n",
    "\n",
    "-   Sur un tel réseau, l'apprentissage se résume à l'optimisation d'une\n",
    "    fonction fortement non convexe, qui amène presque sûrement à des\n",
    "    minima locaux lorsque des algorithmes classiques sont utilisés.\n",
    "\n",
    "-   Dans l'étape de rétropropagation, les gradients diminuent rapidement\n",
    "    à mesure que le nombre de couches cachées augmente. La dérivée de la\n",
    "    fonction objectif par rapport à $\\mathbf{W}$ devient alors très\n",
    "    faible à mesure que le calcul se rétropropage vers la couche\n",
    "    d'entrée. Les poids des premières couches changent donc très\n",
    "    lentement et le réseau n'est plus en capacité d'apprendre. Ce\n",
    "    problème est connu sous le nom de problème de gradient évanescent\n",
    "    (vanishing gradient).\n",
    "\n",
    "L'algorithme principalement utilisé pour l'apprentissage des réseaux\n",
    "convolutifs reste la rétropropagation du gradient. Le choix de la\n",
    "fonction objectif, de sa régularisation, de la méthode d'optimisation\n",
    "(descente de gradient, méthodes à taux d'apprentissage adaptatifs telles\n",
    "qu'AdaGrad, RMSProp ou Adam) et des paramètres associés, ou des\n",
    "techniques de présentation des exemples (batchs, minibatchs) sont autant\n",
    "de facteurs importants permettant aux modèles non seulement de converger\n",
    "vers un optimum local satisfaisant, mais également de proposer un modèle\n",
    "final ayant une bonne capacité de généralisation.\n",
    "\n",
    "Aujourd'hui, de nombreux réseaux, déjà entraînés, sont mis à\n",
    "disposition. En effet, ces entraînements nécessitent de grandes bases\n",
    "d'apprentissage (type ImageNet) et une puissance de calcul assez élevée\n",
    "(GPUs obligatoires). Pour le traitement de problèmes précis, des\n",
    "méthodes existent, qui partent de ces réseaux préentraînés et les\n",
    "modifient localement pour, par exemple, apprendre de nouvelles classes\n",
    "d'images non encore vues par le réseau. L'idée sous-jacente est que les\n",
    "premières couches capturent des caractéristiques bas niveau et que la\n",
    "sémantique vient avec les couches profondes. Ainsi, dans un problème de\n",
    "classification, où les classes n'ont pas été apprises, on peut supposer\n",
    "qu'en conservant les premières couches on extraira des caractéristiques\n",
    "communes des images (bords, colorimétrie,\\...) et qu'en changeant les\n",
    "dernières couches (information sémantique et haut niveau et étage de\n",
    "classification), c'est-à-dire en réapprenant les connexions, on\n",
    "spécifiera le nouveau réseau pour la nouvelle tâche de classification.\n",
    "Cette approche rentre dans le cadre des méthodes de *transfer learning*\n",
    "et de *fine tuning*, cas particulier d'adaptation de domaine :\n",
    "\n",
    "-   Les méthodes de transfert prennent un réseau déjà entraîné, enlèvent\n",
    "    la dernière couche complètement connectée et traitent le réseau\n",
    "    restant comme un extracteur de caractéristiques. Un nouveau\n",
    "    classifieur, la dernière couche, est alors entraîné sur le nouveau\n",
    "    problème ({numref}`transfer`).\n",
    "\n",
    "```{figure} ./images/transferLearning.png\n",
    ":name: transfer\n",
    ":align: center\n",
    "Illustration de la technique de transfer Learning. Le réseau appris à classer des images de chat est amputé de sa partie classification. Un nouveau classifieur est mis au bout du réseau, dont les poids sont entraînés sur la nouvelle tâche de classification.\n",
    "```\n",
    "\n",
    "-   Les méthodes de fine tuning ré-entraînent tout ou partie d'un réseau (suivant les données disponibles) et conservent les poids non ré-entrainés.\n",
    "\n",
    "\n",
    "Plusieurs facteurs influent sur le choix de la méthode à utiliser : la\n",
    "taille des données d'apprentissage du nouveau problème et la\n",
    "ressemblance du nouveau jeu de données avec celui qui a servi à\n",
    "entraîner le réseau initial :\n",
    "\n",
    "-   Pour un jeu de données similaire de petite taille, on utilise du\n",
    "    transfer learning, avec un classifieur utilisé sur les\n",
    "    caractéristiques calculées sur les dernières couches du réseau\n",
    "    initial.\n",
    "\n",
    "-   Pour un jeu de données de petite taille et un problème différent, on\n",
    "    utilise du transfer learning, avec un classifieur utilisé sur les\n",
    "    caractéristiques calculées sur les premières couches du réseau\n",
    "    initial.\n",
    "\n",
    "-   Pour un jeu de données, similaire ou non, de grande taille, on\n",
    "    utilise le fine tuning.\n",
    "\n",
    "Notons qu'il est toujours possible d'augmenter la taille du jeu de\n",
    "données par des technique d'augmentation de données.\n",
    "\n",
    "Dans le cas où un réseau ad hoc doit être construit et où une base\n",
    "d'apprentissage suffisante est disponible, l'entraînement par\n",
    "optimisation reste possible mais peut nécessiter des ressources de calcul importantes.  De plus, il a été montré que : \n",
    "- le transfer Learning ou le fine tuning permettaient souvent d'aboutir à de meilleures performances que l'entraînement depuis un réseau initial aléatoire (on se sert des poids du réseau pré entraîné comme initialisation, plutôt qu'une initialisation type Xavier).\n",
    "- le fine tuning améliorait la capacité de généralisation du réseau. \n",
    "\n",
    "\n",
    "### Visualisation du mécanisme des réseaux convolutifs\n",
    "\n",
    "Le mécanisme interne des réseaux convolutifs est mal compris et\n",
    "l'analyse des raisons qui font que leur puissance de prédiction est\n",
    "importante n'est pas aisée. S'il est toujours possible de rétroprojeter\n",
    "les activations depuis la première couche de convolution, les couches\n",
    "d'agrégation et de rectification empêchent de comprendre le\n",
    "fonctionnement des couches suivantes, ce qui peut être gênant dans la\n",
    "construction et l'amélioration de ces réseaux.\n",
    "\n",
    "Les méthodes de visualisation du fonctionnement des réseaux convolutifs\n",
    "peuvent être rangées en trois catégories, décrites brièvement dans les paragraphes\n",
    "suivants.\n",
    "\n",
    "### Méthodes de visualisation de base\n",
    "\n",
    "Les méthodes les plus simples consistent à visualiser les activations\n",
    "lors du passage d'une image dans le réseau. Pour des activations type\n",
    "ReLU, ces activations sont ininterprétables au début de l'entraînement,\n",
    "mais à mesure que ce dernier progresse, les cartes d'activation\n",
    "$\\mathbf{Y_i^{(l)}}$ deviennent localisées et éparses.\n",
    "\n",
    "Il est également possible de visualiser les filtres des différentes\n",
    "couches de convolution ({numref}`visufiltres`).\n",
    "Les filtres des premières couches agissent comme des détecteurs de bords\n",
    "et coins et, à mesure que l'on s'enfonce dans le réseau, les filtres\n",
    "capturent des concepts haut niveau comme des objets ou encore des\n",
    "visages.\n",
    "\n",
    "Citons encore d'autres méthodes qui proposent de visualiser les\n",
    "dernières couches (les couches complètement connectées) de grande\n",
    "dimension (par exemple 4096 pour AlexNet) via une méthode de réduction\n",
    "de dimension.\n",
    "\n",
    "```{figure} ./images/visufiltres.png\n",
    ":name: visufiltres\n",
    ":width: 500px\n",
    ":align: center\n",
    "Visualisation des filtres de la première couche d’AlexNet (à\n",
    "gauche, 64 filtres 11$\\times$11) et de ResNet-18 (à droite, 64 filtres 7$\\times 7$).\n",
    "```\n",
    "\n",
    "\n",
    "### Méthodes fondées sur les activations\n",
    "\n",
    "Plusieurs stratégies peuvent être adoptées pour sonder le fonctionnement\n",
    "d'un réseau convolutif, en utilisant les informations portées par les\n",
    "cartes $\\mathbf{Y_i^{(l)}}$, parmi lesquelles :\n",
    "\n",
    "-   Utiliser des couches de convolution transposée (improprement\n",
    "    appelées parfois couches de déconvolution), ajoutées à chaque couche\n",
    "    de convolution du réseau. Étant données les cartes d'entrée de la\n",
    "    couche $l$, les cartes de sortie $\\mathbf{Y_i^{(l)}}$ sont envoyées\n",
    "    dans la couche de convolution transposée correspondante au niveau\n",
    "    $l$. Cette dernière reconstruit les $\\mathbf{Y_i^{(l-1)}}$ qui ont\n",
    "    permis le calcul des activations de la couche $l$. Le processus est\n",
    "    alors itéré jusqu'à atteindre la couche d'entrée $l = 1$, les\n",
    "    activations de la couche $l$ étant alors rétroprojetées dans le plan\n",
    "    image. La présence de couches d'agrégation et de\n",
    "    rectification rend ce processus non inversible (par exemple, une\n",
    "    couche d'agrégation maximum nécessite de connaître à quelles\n",
    "    positions de l'image $\\mathbf{Y_i^{(l)}}$ sont situés les maxima\n",
    "    retenus).\n",
    "\n",
    "-   Faire passer un grand nombre d'images dans le réseau et, pour un\n",
    "    neurone particulier, conserver celle qui a le plus activé ce\n",
    "    neurone. Il est alors possible de visualiser les images pour\n",
    "    comprendre ce à quoi le neurone s'intéresse dans son champ réceptif\n",
    "    ({numref}`champr`).\n",
    "\n",
    "\n",
    "```{figure} ./images/champr.png\n",
    ":name: champr\n",
    ":align: center\n",
    "Champ récéptif de quelques neurones de la dernière couche\n",
    "    d’agrégation du réseau AlexNet, superposées aux images ayant le plus\n",
    "    fortement activé ces neurones. Le champ est encadré en blanc, et la\n",
    "    valeur d’activation correspondante est reportée en haut. On voit par\n",
    "    exemple que certains neurones sont très sensibles aux textes, d’autres\n",
    "    aux réflexions spéculaires, ou encore aux hauts du corps (source :{cite:p}`Girshick14`)\n",
    "```\n",
    "\n",
    "\n",
    "-   Cacher (par un rectangle noir par exemple) différentes parties de\n",
    "    l'image d'entrée qui est d'une certaine classe (disons un chien) et\n",
    "    observer la sortie du réseau (la probabilité de la classe de l'image\n",
    "    d'entrée). En représentant les valeurs de probabilité de la classe\n",
    "    d'intérêt comme une fonction de la position du rectangle occultant,\n",
    "    il est possible de voir si le réseau s'intéresse effectivement aux\n",
    "    parties de l'image spécifiques de la classe, ou à des autres zones\n",
    "    (le fond par exemple) ({numref}`occlusion`).\n",
    "\n",
    "```{figure} ./images/occlusion.png\n",
    ":name: occlusion\n",
    ":width: 500px\n",
    ":align: center\n",
    "Occlusion d’une image (à gauche). Le rectangle noir est\n",
    "    déplacé dans l’image et pour chaque position la probabilité de la classe\n",
    "    de l’image (ici un loulou de Poméranie) est enregistrée. Ces\n",
    "    probabilités sont ensuite représentées sous forme d’une carte 2D (à\n",
    "    droite). La probabilité de la classe s’effondre lorsque le rectangle\n",
    "    couvre une partie de la face du chien. Cela suggère que cette face est\n",
    "    grandement responsable de la forte probabilité de classement de l’image\n",
    "    comme un loulou. A l’inverse, l’occlusion du fond n’altère pas la forte\n",
    "    valeur de probabilité de la classe  (source :{cite:p}`ZE13`)\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "### Méthodes fondées sur le gradient\n",
    "\n",
    "Pour comprendre quelle(s) partie(s) de l'image est (sont) utilisée(s)\n",
    "par le réseau pour effectuer une prédiction, il est possible de calculer\n",
    "des cartes de saillance (saliency maps). L'idée est relativement simple\n",
    ": calculer le gradient de la classe de sortie par rapport à l'image\n",
    "d'entrée. Cela indique à quel point une petite variation dans l'image\n",
    "induit un changement de prédiction. En visualisant les gradients, on\n",
    "observe alors par exemple leurs fortes valeurs, indiquant qu'une petite\n",
    "variation du pixel correspondant augmente la valeur de sortie.\\\n",
    "Il est également possible d'utiliser le gradient par rapport à la\n",
    "dernière couche de convolution (approche Grad-CAM), ce qui permet de\n",
    "récupérer des informations de localisation spatiale des régions\n",
    "importantes pour la prédiction ({numref}`gradcam` droite).\n",
    "\n",
    "Plus généralement, en choisissant un neurone intermédiaire du réseau\n",
    "(d'une couche de convolution), la méthode de rétropropagation guidée\n",
    "calcule le gradient de sa valeur par rapport aux pixels de l'image\n",
    "d'entrée, ce qui permet de souligner les parties de l'image auxquelles\n",
    "ce neurone répond ({numref}`gradcam` milieu).\n",
    "\n",
    "\n",
    "```{figure} ./images/gradcam.png\n",
    ":name: gradcam\n",
    ":width: 500px\n",
    ":align: center\n",
    "Approches par gradient de visualisation du fonctionnement\n",
    "d’un réseau convolutif. Comparaison de la méthode de rétropropagation\n",
    "guidée et de Grad-CAM (source :{cite:p}`Selvaraju17`)\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Ces gradients peuvent également être utilisés dans la méthode de montée\n",
    "de gradient (gradient Ascent), dont l'objectif est de générer une image\n",
    "qui active de manière maximale un neurone donné du réseau. Le principe\n",
    "est d'itérativement passer l'image d'entrée $\\mathbf{I}$ dans le réseau\n",
    "pour obtenir les valeurs des cartes $\\mathbf{Y_i^{(l)}}$, de\n",
    "rétropropager pour obtenir le gradient d'un neurone par rapport aux\n",
    "pixels de $\\mathbf{I}$ et d'opérer une petite modification de ces\n",
    "pixels. Outre son aspect informatif sur la structure interne du réseau\n",
    "étudié (visualisation des cartes $\\mathbf{Y_i^{(l)}}$ intermédiaires),\n",
    "cette méthode produit des images parfois très artistiques ({numref}`monteeg`).\n",
    "\n",
    "\n",
    "```{figure} ./images/monteeg.png\n",
    ":name: monteeg\n",
    ":align: center\n",
    "Visualisation des 4 premières couches de convolution d’un\n",
    "réseau convolutif par montée de gradient  (source :{cite:p}`Yosinski15`)\n",
    "```\n",
    "\n",
    "## Implémentation\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torchsummary import summary\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "```\n",
    "\n",
    "On travaille sur les données MNIST\n",
    "\n",
    "\n",
    "```python\n",
    "# taille des batchs\n",
    "train_batch_size=128\n",
    "test_batch_size = 128\n",
    "\n",
    "# Learning rate\n",
    "lr = 0.001\n",
    "\n",
    "# Nombre d'epochs \n",
    "num_epochs = 10\n",
    "\n",
    "# Régularisation Dropout\n",
    "dropout = 0.25\n",
    "\n",
    "transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "        ])\n",
    "dataset1 = datasets.MNIST('../data', train=True, download=True,\n",
    "                       transform=transform)\n",
    "dataset2 = datasets.MNIST('../data', train=False,\n",
    "                       transform=transform)\n",
    "train_kwargs = {'batch_size': train_batch_size}\n",
    "test_kwargs = {'batch_size': test_batch_size}\n",
    "train_loader = torch.utils.data.DataLoader(dataset1,**train_kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(dataset2, **test_kwargs)\n",
    "```\n",
    "\n",
    "et on implémente un réseau convolutif ayant l'architecture suivante : \n",
    "\n",
    "CONV1 - RELU - MAX POOLING - CONV2 - RELU - MAX POOLING - FCL - DROPOUT - Prediction\n",
    "\n",
    "avec :\n",
    "- des noyaux de convolution de taille 5$\\times$5\n",
    "- un max pooling sur une région 2$\\times$2\n",
    "\n",
    "\n",
    "\n",
    "```python\n",
    "class CNN(nn.Module):\n",
    "  def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 5, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 5, 1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc1 = nn.Linear(1024, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 10)\n",
    "\n",
    "  def forward(self,x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        output = F.softmax(x, dim=1)\n",
    "        return output\n",
    "```\n",
    "\n",
    "On créé ensuite les fonction pour entraîner et tester le réseau\n",
    "\n",
    "```python\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Epoch: {} [{}/{} ({:.0f}%)]\\tPerte : {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "    return loss\n",
    "\n",
    "\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_acc = 100. * correct / len(test_loader.dataset)\n",
    "\n",
    "    print('\\nPerte moyenne en test : {:.4f}, Précision : {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),test_acc\n",
    "        ))\n",
    "    return test_loss,test_acc\n",
    "```\n",
    "\n",
    "et on instantie le modèle\n",
    "\n",
    "```python\n",
    "model = CNN().to(device)\n",
    "summary(model, (1,28,28))\n",
    "```\n",
    "\n",
    "puis on l'entraîne\n",
    "\n",
    "```python\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "scheduler = StepLR(optimizer, step_size=1)\n",
    "trainLoss = []\n",
    "testLoss = []\n",
    "testAcc = []\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    trainLoss.append(train(model, device, train_loader, optimizer, epoch).detach().numpy())\n",
    "    t,a = test(model, device, test_loader)\n",
    "    testLoss.append(t)\n",
    "    testAcc.append(a)\n",
    "    scheduler.step()\n",
    "    \n",
    "plt.plot(trainLoss, '-b',label='train')\n",
    "plt.plot(testLoss, 'r',label='test')\n",
    "plt.legend(loc='best')\n",
    "plt.title(\"Fonction de perte\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"Perte\")\n",
    "plt.figure()\n",
    "plt.plot(testAcc)\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"Précision\")\n",
    "plt.title(\"Précision en test\")\n",
    "plt.tight_layout()\n",
    "```\n",
    "\n",
    "```{figure} ./images/precisionCNN.png\n",
    ":name: perte\n",
    ":align: center\n",
    "Précision en apprentissage.\n",
    "```\n",
    "\n",
    "```{figure} ./images/perteCNN.png\n",
    ":name: perte\n",
    ":align: center\n",
    "Fonction de perte.\n",
    "```\n",
    "\n",
    "\n",
    "```{bibliography}\n",
    ":style: unsrt\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "md:myst",
   "text_representation": {
    "extension": ".md",
    "format_name": "myst"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "source_map": [
   11
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}