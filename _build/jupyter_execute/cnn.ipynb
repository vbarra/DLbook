{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06f42706",
   "metadata": {},
   "source": [
    "# Réseaux convolutifs\n",
    "\n",
    "## Introduction\n",
    "\n",
    "### Inspiration biologique\n",
    "\n",
    "Un réseau de neurones convolutif (CNN, *Convolutional Neural Network* ou\n",
    "ConvNet) est un type de réseau de neurones artificiels acyclique à\n",
    "propagation avant, dans lequel le motif de connexion entre les neurones\n",
    "est inspiré par le cortex visuel des animaux. Les neurones de cette\n",
    "région du cerveau sont arrangés de sorte à ce qu'ils correspondent à des\n",
    "régions (appelés champs réceptifs) qui se chevauchent lors du pavage du\n",
    "champ visuel. Ils sont de plus organisés de manière hiérarchique, en\n",
    "couches (aire visuelle primaire V1, secondaire V2, puis aires V3, V4, V5\n",
    "et V6, gyrus temporal inférieur), chacune des couches étant spécialisée\n",
    "dans une tâche, de plus en plus abstraite en allant de l'entrée vers la\n",
    "sortie. En simplifiant à l'extrême, une fois que les signaux lumineux\n",
    "sont reçus par la rétine et convertis en potentiels d'action :\n",
    "\n",
    "-   L'aire primaire V1 s'intéresse principalement à la détection de\n",
    "    contours, ces contours étant définis comme des zones de fort\n",
    "    contraste de signaux visuels reçus.\n",
    "\n",
    "-   L'aire V2 reçoit les informations de V1 et extrait des informations\n",
    "    telles que la fréquence spatiale, l'orientation, ou encore la\n",
    "    couleur.\n",
    "\n",
    "-   L'aire V4, qui reçoit des informations de V2, mais aussi de V1\n",
    "    directement, détecte des caractéristiques plus complexes et\n",
    "    abstraites liées par exemple à la forme.\n",
    "\n",
    "-   Le gyrus temporal inférieur est chargé de la partie sémantique\n",
    "    (reconnaissance des objets), à partir des informations reçues des\n",
    "    aires précédentes et d'une mémoire des informations stockées sur des\n",
    "    objets.\n",
    "\n",
    "L'architecture et le fonctionnement des réseaux convolutifs sont\n",
    "inspirés par ces processus biologiques. Ces réseaux consistent en un\n",
    "empilage multicouche de perceptrons, dont le but est de prétraiter de\n",
    "petites quantités d'informations. Les réseaux convolutifs ont de larges\n",
    "applications dans la reconnaissance d'image et vidéo, les systèmes de\n",
    "recommandation et le traitement du langage naturel (voir section\n",
    "[1.6](#subsubsec:applis){reference-type=\"ref\"\n",
    "reference=\"subsubsec:applis\"} pour quelques exemples)\n",
    "\n",
    "Un réseau convolutif se compose de deux types de neurones, agencés en\n",
    "couches traitant successivement l'information. Dans le cas du traitement\n",
    "de données de type images, on a ainsi :\n",
    "\n",
    "-   des *neurones de traitement*, qui traitent une portion limitée de\n",
    "    l'image (le champ réceptif) au travers d'une fonction de\n",
    "    convolution;\n",
    "\n",
    "-   des *neurones* de mise en commun des sorties dits *d'agrégation\n",
    "    totale ou partielle* (*pooling*).\n",
    "\n",
    "Un traitement correctif non linéaire est appliqué entre chaque couche\n",
    "pour améliorer la pertinence du résultat. L'ensemble des sorties d'une\n",
    "couche de traitement permet de reconstituer une image intermédiaire,\n",
    "dite carte de caractéristiques (feature map), qui sert de base à la\n",
    "couche suivante. Les couches et leurs connexions apprennent des niveaux\n",
    "d'abstraction croissants et extraient des caractéristiques de plus en\n",
    "plus haut niveau des données d'entrée.\n",
    "\n",
    "Dans la suite, le propos sera illustré sur des images 2D en niveaux de\n",
    "gris, de taille $n_1 \\times n_2$ : $$\\begin{aligned}\n",
    "    \\mathbf{I} : [\\![1\\cdots n_1]\\!]\\times [\\![1\\cdots n_2]\\!] &\\rightarrow&  \\mathbb{R}\\\\\n",
    "     (i,j) &\\mapsto& I_{i,j}\n",
    "\\end{aligned}$$ $\\mathbf{I}$ sera indifféremment vue comme une fonction\n",
    "ou une matrice.\n",
    "\n",
    "### Convolution discrète {#subsec:convolution}\n",
    "\n",
    "Pour reproduire la notion de champ réceptif, et ainsi permettre aux\n",
    "neurones de détecter des caractéristiques de petite taille mais porteurs\n",
    "d'information, l'idée est de laisser un neurone caché voir et traiter\n",
    "seulement une petite portion de l'image qu'il prend en entrée. L'outil\n",
    "retenu dans les réseaux convolutifs est la convolution discrète.\n",
    "\n",
    "::: definition\n",
    "Soient\n",
    "$h_1,h_2\\in\\mathbb{N}, \\mathbf{K} \\in \\mathbb{R}^{(2h_1+1) \\times (2h_2+1)}$.\n",
    "La convolution discrète de $\\mathbf{I}$ par le filtre $\\mathbf{K}$ est\n",
    "donnée par : $$\\begin{aligned}\n",
    "    \\label{eq:convolution}\n",
    "    \\left(\\mathbf{K} \\ast \\mathbf{I}\\right)_{r,s} = \\dsum _{u = -h_1} ^{h_1} \\dsum _{v = -h_2}^{h_2} K_{u,v} I_{r+u,s+v}\n",
    "\\end{aligned}$$ où $\\mathbf{K}$ est donné par : $$\\begin{aligned}\n",
    "    \\mathbf{K} =\n",
    "    \\begin{pmatrix}\n",
    "        K_{-h_1,-h_2} & \\ldots & K_{-h_1,h_2}\\\\\n",
    "        \\vdots & K_{0,0} & \\vdots\\\\\n",
    "        K_{h_1,-h_2} & \\ldots & K_{h_1,h_2}\\\\\n",
    "    \\end{pmatrix}.\n",
    "\\end{aligned}$$\n",
    ":::\n",
    "\n",
    "La taille du filtre $(2h_1+1) \\times (2h_2+1)$ précise le champ visuel\n",
    "capturé et traité par $\\mathbf{K}$.\\\n",
    "Lorsque $\\mathbf{K}$ parcourt $\\mathbf{I}$, le déplacement du filtre est\n",
    "réglé par deux paramètres de *stride* (horizontal et vertical). Un\n",
    "stride de 1 horizontal (respectivement vertical) signifie que\n",
    "$\\mathbf{K}$ se déplace d'une position horizontale (resp. verticale) à\n",
    "chaque application de\n",
    "[\\[eq:convolution\\]](#eq:convolution){reference-type=\"ref\"\n",
    "reference=\"eq:convolution\"}. Les valeurs de stride peuvent également\n",
    "être supérieures et ainsi sous-échantillonner $\\mathbf{I}$.\n",
    "\n",
    "Le comportement du filtre sur les bords de $\\mathbf{I}$ doit également\n",
    "être précisé, par l'intermédiaire d'un paramètre de *padding*. Si\n",
    "l'image convoluée $\\left(\\mathbf{K} \\ast \\mathbf{I}\\right)$ doit\n",
    "posséder la même taille que $\\mathbf{I}$, alors $2h_1$ lignes de 0\n",
    "($h_1$ en haut et $h_1$ en bas) et $2h_2$ colonnes de 0 ($h_2$ à gauche\n",
    "et $h_2$ à droite) doivent être ajoutées. Dans le cas où la convolution\n",
    "est réalisée sans padding, l'image convoluée est de taille\n",
    "$(n_1-2h_1)\\times (n_2-2h_2)$.\n",
    "\n",
    "## Définition des couches\n",
    "\n",
    "Nous introduisons ici les différents types de couches utilisées dans les\n",
    "réseaux convolutifs. L'assemblage de ces couches permet de construire\n",
    "des architectures complexes pour la classification ou la régression,\n",
    "dont certaines seront précisées dans un prochain cours.\n",
    "\n",
    "### Couche de convolution\n",
    "\n",
    "<figure id=\"fig-convolution-3D\">\n",
    "<div class=\"center\">\n",
    "\n",
    "</div>\n",
    "<figcaption><span id=\"fig-convolution-3D\"\n",
    "label=\"fig-convolution-3D\"></span> <em>Illustration des calculs\n",
    "effectués dans une opération de convolution discrète. Le pixel (2,2) de\n",
    "l’image <span\n",
    "class=\"math inline\"><strong>Y</strong><sub><strong>i</strong></sub><sup><strong>(</strong><strong>l</strong><strong>)</strong></sup></span>\n",
    "est une combinaison linéaire des pixels <span\n",
    "class=\"math inline\">(<em>i</em>,<em>j</em>), <em>i</em>, <em>j</em> ∈ [ [1,3] ]</span>\n",
    "de <span\n",
    "class=\"math inline\"><strong>Y</strong><sub><strong>i</strong></sub><sup><strong>(</strong><strong>l</strong><strong>−</strong><strong>1</strong><strong>)</strong></sup></span>,\n",
    "les coefficients de la combinaison étant portés par le filtre <span\n",
    "class=\"math inline\"><strong>K</strong></span> (équation <a\n",
    "href=\"#eq:convlayer\" data-reference-type=\"ref\"\n",
    "data-reference=\"eq:convlayer\">[eq:convlayer]</a>).</em></figcaption>\n",
    "</figure>\n",
    "\n",
    "Soit $l\\in\\mathbb{N}$ une couche de convolution. L'entrée de la couche\n",
    "$l$ est composée de $n^{(l-1)}$ cartes provenant de la couche\n",
    "précédente, de taille $n_1^{(l-1)} \\times n_2^{(l-1)}$. Dans le cas de\n",
    "la couche d'entrée du réseau ($l = 1$), l'entrée est l'image\n",
    "$\\mathbf{I}$. La sortie de la couche $l$ est formée de $n^{(l)}$ cartes\n",
    "de taille $n_1^{(l)} \\times n_2^{(l)}$. La $i^{\\text{e}}$ carte de la\n",
    "couche $l$, notée $\\mathbf{Y_i^{(l)}}$, se calcule comme :\n",
    "$$\\begin{aligned}\n",
    "    \\label{eq:convlayer}\n",
    "    \\mathbf{Y_i^{(l)}} = \\mathbf{B^{(l)}_{i}} + \\dsum _{j = 1}^{n^{(l-1)}} \\mathbf{K^{(l)}_{i,j}} \\ast \\mathbf{Y_j^{(l-1)}}\n",
    "\\end{aligned}$$ où $\\mathbf{B_i^{(l)}}$ est une matrice de biais et\n",
    "$\\mathbf{K^{(l)}_{i,j}}$ est le filtre de taille\n",
    "$(2h_1^{(l)} + 1) \\times (2h_2^{(l)} + 1)$ connectant la $j^{\\text{e}}$\n",
    "carte de la couche $(l-1)$ à la $i^{\\text{e}}$ carte de la couche $l$\n",
    "(voir la figure [\\[F:coucheconv\\]](#F:coucheconv){reference-type=\"ref\"\n",
    "reference=\"F:coucheconv\"}).\n",
    "\n",
    "$n_1^{(l)}$ et $n_2^{(l)}$ doivent prendre en compte les effets de bords\n",
    ": lors du calcul de la convolution, seuls les pixels dont la somme est\n",
    "définie avec des indices positifs doivent être traités. Dans le cas où\n",
    "le padding n'est pas utilisé, les cartes de sortie ont donc une taille\n",
    "de $n_1^{(l)} = n_1^{(l-1)} - 2h_1^{(l)}$ et\n",
    "$n_2^{(l)} = n_2^{(l-1)} - 2h_2^{(l)}$.\n",
    "\n",
    "Souvent, les filtres utilisés pour calculer $\\mathbf{Y_i^{(l)}}$ sont\n",
    "les mêmes, i.e. $\\mathbf{K_{i,j}^{(l)}} = \\mathbf{K _{i,k}^{(l)}}$ pour\n",
    "$j \\neq k$. De plus, la somme dans l'équation\n",
    "[\\[eq:convlayer\\]](#eq:convlayer){reference-type=\"eqref\"\n",
    "reference=\"eq:convlayer\"} peut être conduite sur un sous ensemble des\n",
    "cartes d'entrée.\n",
    "\n",
    "Il est possible de mettre en correspondance la couche de convolution et\n",
    "l'opération [\\[eq:convlayer\\]](#eq:convlayer){reference-type=\"eqref\"\n",
    "reference=\"eq:convlayer\"} qu'elle effectue, avec un perceptron\n",
    "multicouche. Pour cela, il suffit de réécrire l'équation\n",
    "[\\[eq:convlayer\\]](#eq:convlayer){reference-type=\"eqref\"\n",
    "reference=\"eq:convlayer\"} : chaque carte $\\mathbf{Y_i^{(l)}}$ de la\n",
    "couche $l$ est formée de $n_1^{(l)} \\cdot n_2^{(l)}$ neurones organisés\n",
    "dans un tableau à deux dimensions. Le neurone en position $(r,s)$\n",
    "calcule : $$\\begin{aligned}\n",
    "    \\left(\\mathbf{Y_i^{(l)}}\\right)_{r,s} &= \\left(\\mathbf{B_i^{(l)}}\\right)_{r,s} + \\dsum _{j = 1}^{n^{(l-1)}} \\left(\\mathbf{K^{(l)}_{i,j} }\\ast \\mathbf{Y_j^{(l-1)}}\\right)_{r,s}\\\\\n",
    "    &= \\left(\\mathbf{B_i^{(l)}}\\right)_{r,s} + \\dsum _{j = 1}^{n^{(l-1)}} \\dsum _{u = - h_1^{(l)}} ^{h_1^{(l)}} \\dsum _{v = - h_2^{(l)}} ^{h_2^{(l)}} \\left(\\mathbf{K^{(l)}_{i,j}}\\right)_{u,v} \\left(\\mathbf{Y_j^{(l-1)}}\\right)_{r+u,s+v}\n",
    "\\end{aligned}$$\n",
    "\n",
    "Les paramètres du réseau à entraîner (poids) peuvent alors être trouvés\n",
    "dans les filtres $\\mathbf{K^{(l)}_{i,j}}$ et les matrices de biais\n",
    "$\\mathbf{B_i^{(l)}}$.\n",
    "\n",
    "Comme nous le verrons dans la section\n",
    "[\\[subsubsec:coucheagreg\\]](#subsubsec:coucheagreg){reference-type=\"ref\"\n",
    "reference=\"subsubsec:coucheagreg\"}, un sous-échantillonnage est utilisé\n",
    "pour diminuer l'influence du bruit et des distorsions dans les images.\n",
    "Le sous-échantillonnage peut être également réalisé simplement avec des\n",
    "paramètres de stride, en sautant un nombre fixe de pixels dans les\n",
    "dimensions horizontale (saut $s_1^{(l)}$) et verticale (saut\n",
    "$s_2^{(l)}$) avant d'appliquer de nouveau le filtre. La taille des\n",
    "images de sortie est alors : $$\\begin{aligned}\n",
    "    n_1^{(l)} = \\frac{n_1^{(l-1)} - 2h_1^{(l)}}{s_1^{(l)} + 1}\\quad \\text{ et }\\quad n_2^{(l)} = \\frac{n_2^{(l-1)} - 2h_2^{(l)}}{s_2^{(l)} + 1}.\n",
    "\\end{aligned}$$\n",
    "\n",
    "::: SCfigure\n",
    "![image](images/Fig10-2-new)\n",
    ":::\n",
    "\n",
    "Un point clé des réseaux convolutifs est d'exploiter la corrélation\n",
    "spatiale des données. L'utilisation des noyaux permet d'alléger le\n",
    "modèle, plutôt que d'utiliser des couches complètement connectées.\n",
    "\n",
    "### Couche non linéaire\n",
    "\n",
    "[]{#S:nonlinl label=\"S:nonlinl\"} Pour augmenter le pouvoir d'expression\n",
    "des réseaux profonds, on utilise des couches non linéaires. Les entrées\n",
    "d'une couche non linéaire sont $n^{(l-1)}$ cartes et ses sorties\n",
    "$n^{(l)} = n^{(l-1)}$ cartes $\\mathbf{Y_i^{(l)}}$, de taille\n",
    "$n_1^{(l-1)} \\times n_2^{(l-1)}$ telles que $n_1^{(l)} = n_1^{(l-1)}$ et\n",
    "$n_2^{(l)} = n_2^{(l-1)}$, données par\n",
    "$\\mathbf{Y_i^{(l)}}$ = $f \\left(\\mathbf{Y_i^{(l-1)}}\\right)$, où $f$ est\n",
    "la fonction d'activation utilisée dans la couche $l$. Le tableau\n",
    "[\\[T:activation\\]](#T:activation){reference-type=\"ref\"\n",
    "reference=\"T:activation\"} propose quelques fonctions d'activation\n",
    "usuelles.\n",
    "\n",
    "::: tabularx\n",
    "\\|\\>Y\\| \\>Z\\| \\>Z\\| \\>Z\\| **Nom** & **Graphe** & & **$f'$**\\\n",
    "\n",
    "Rampe & & $$f(x) = x$$ & $f'(x) = 1$\\\n",
    "\n",
    "Heaviside & & $$f(x) =\n",
    "\\begin{cases}\n",
    "\\;\\; 0 & \\text{si $x \\; < \\; 0$}  \\\\\n",
    "\\;\\; 1 & \\text{si $x \\; \\geq \\; 0$}\n",
    "\\end{cases}$$ & $$f'(x) =\n",
    "\\begin{cases}\n",
    "\\;\\; 0 & \\text{si $x \\; \\neq \\; 0$}  \\\\\n",
    "\\;\\; ? & \\text{si $x \\; = \\; 0$}\n",
    "\\end{cases}$$\\\n",
    "\n",
    "Logistique ou sigmoı̈de & & $$f(x) \\; = \\; \\frac{1}{1 + e^{-x}}$$ &\n",
    "$f'(x) \\; = \\; f(x) \\, \\bigl(1 \\, - \\, f(x)\\bigr)$\\\n",
    "\n",
    "Tangente hyperbolique& & $$\\begin{aligned}\n",
    "f(x) \\; &= \\; \\tanh(x) \\\\\n",
    "& = \\;  \\frac{2}{1 + e^{-2x}} - 1\n",
    "\\end{aligned}$$ & $$f'(x) \\; = \\; 1 \\, - \\, f^2(x)$$\\\n",
    "\n",
    "Arc Tangente& & $$f(x) \\; = \\; \\tan^{-1}(x)$$ &\n",
    "$$f'(x) \\; = \\; \\frac{1}{x^2 + 1}$$\\\n",
    "\n",
    "ReLU& & $$f(x) =\n",
    "\\begin{cases}\n",
    "\\;\\; 0 & \\text{si $x \\; < \\; 0$}  \\\\\n",
    "\\;\\; x & \\text{si $x \\; \\geq \\; 0$}\n",
    "\\end{cases}$$ & $$f'(x) =\n",
    "\\begin{cases}\n",
    "\\;\\; 0 & \\text{si $x \\; \\neq \\; 0$}  \\\\\n",
    "\\;\\; 1 & \\text{si $x \\; = \\; 0$}\n",
    "\\end{cases}$$\\\n",
    "\n",
    "Exponentielle Linéaire& & $$f(x) =\n",
    "\\begin{cases}\n",
    "\\;\\; \\alpha \\, (e^x \\, - \\, 1) & \\text{si $x \\; < \\; 0$}  \\\\\n",
    "\\;\\; x & \\text{si $x \\; \\geq \\; 0$}\n",
    "\\end{cases}$$ & $$f'(x) =\n",
    "\\begin{cases}\n",
    "\\;\\; f(x) \\, + \\, \\alpha & \\text{si $x \\; < \\; 0$}  \\\\\n",
    "\\;\\; 1 & \\text{si $x \\; \\geq \\; 0$}\n",
    "\\end{cases}$$\\\n",
    ":::\n",
    "\n",
    "En apprentissage profond, il a été reporté que la sigmoïde et la\n",
    "tangente hyperbolique avaient des performances moindres que la fonction\n",
    "d'activation *softsign* : $$\\begin{aligned}\n",
    "    \\mathbf{Y_i^{(l)}}  = \\frac{1}{1+ \\left|\\mathbf{Y_i^{(l-1)}} \\right |}.\n",
    "\\end{aligned}$$ En effet, les valeurs des pixels des cartes\n",
    "$\\mathbf{Y_i^{(l-1)}}$ arrivant près des paliers de saturation de ces\n",
    "fonctions donnent des gradients faibles, qui ont tendance à s'annuler\n",
    "(problème du *gradient évanescent* ou *vanishing gradient*) lors de la\n",
    "phase d'apprentissage par rétropropagation du gradient. Une autre\n",
    "fonction, non saturante elle, est très largement utilisée. Il s'agit de\n",
    "la fonction ReLU (Rectified Linear Unit) [@Nair10] : $$\\begin{aligned}\n",
    "    \\label{eq:relu}\n",
    "\\mathbf{Y_i^{(l)}} = max\\left (0,\\mathbf{Y_i^{(l-1)}}\\right )\n",
    "\\end{aligned}$$ Les neurones utilisant la fonction décrite dans\n",
    "l'équation [\\[eq:relu\\]](#eq:relu){reference-type=\"eqref\"\n",
    "reference=\"eq:relu\"} sont appelés neurones linéaires rectifiés. Glorot\n",
    "et Bengio [@Glorot11] ont montré que l'utilisation d'une couche ReLU en\n",
    "tant que couche non linéaire permettait un entraînement efficace de\n",
    "réseaux profonds sans pré-entraînement non supervisé. Plusieurs\n",
    "variantes de cette fonction existent, par exemple pour assurer une\n",
    "différentiabilité en 0 ou pour proposer des valeurs non nulles pour des\n",
    "valeurs négatives de l'argument. La figure\n",
    "[1.2](#F:plotactiv){reference-type=\"ref\" reference=\"F:plotactiv\"}.\n",
    "illustre quelques unes de ces fonctions d'activation.\n",
    "\n",
    "<figure id=\"F:plotactiv\">\n",
    "<table>\n",
    "<tbody>\n",
    "<tr class=\"odd\">\n",
    "<td style=\"text-align: center;\"></td>\n",
    "<td style=\"text-align: center;\"></td>\n",
    "</tr>\n",
    "<tr class=\"even\">\n",
    "<td style=\"text-align: center;\"><span\n",
    "class=\"math inline\"><em>R</em><em>e</em><em>L</em><em>U</em>(<em>x</em>) = <em>m</em><em>a</em><em>x</em>(0,<em>x</em>)</span></td>\n",
    "<td style=\"text-align: center;\"><span\n",
    "class=\"math inline\">$LeakyReLU(x,\\alpha) = \\left \\lbrace\n",
    "\\begin{array}{cc}\n",
    "   x &amp; si\\ x&gt;0  \\\\\n",
    "   \\alpha x &amp; sinon\n",
    "\\end{array}\n",
    "\\right .$</span></td>\n",
    "</tr>\n",
    "<tr class=\"odd\">\n",
    "<td style=\"text-align: center;\"></td>\n",
    "<td style=\"text-align: center;\"></td>\n",
    "</tr>\n",
    "<tr class=\"even\">\n",
    "<td style=\"text-align: center;\"><span class=\"math inline\">$elu(x,\\alpha)\n",
    "=\n",
    "\\left \\lbrace\n",
    "\\begin{array}{cc}\n",
    "   x &amp; si\\ x&gt;0  \\\\\n",
    "   \\alpha(e^x-1) &amp; sinon\n",
    "\\end{array}\n",
    "\\right .$</span></td>\n",
    "<td style=\"text-align: center;\"><span\n",
    "class=\"math inline\">$SeLU(x,\\alpha,\\lambda) =\n",
    "\\left \\lbrace\n",
    "\\begin{array}{cc}\n",
    "   \\lambda x &amp; si\\ x&gt;0  \\\\\n",
    "   \\lambda\\alpha(e^x-1) &amp; sinon\n",
    "\\end{array}\n",
    "\\right .$</span></td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>\n",
    "<figcaption>Quelques fonctions d’activation</figcaption>\n",
    "</figure>\n",
    "\n",
    "### Couches de normalisation\n",
    "\n",
    "La normalisation prend aujourd'hui une place de plus en plus importante,\n",
    "notamment depuis les travaux de Ioffe et Szegedy [@Ioffe15]. Les auteurs\n",
    "suggèrent qu'un changement dans la distribution des activations d'un\n",
    "réseau profond, résultant de la présentation d'un nouveau mini batch\n",
    "d'exemples, ralentit le processus d'apprentissage. Pour pallier ce\n",
    "problème, chaque activation du mini batch est centrée et normée\n",
    "(variance unité), la moyenne et la variance étant calculées sur le mini\n",
    "batch entier, indépendamment pour chaque activation. Des paramètres\n",
    "d'offset $\\beta$ et multiplicatif $\\gamma$ sont alors appliqués pour\n",
    "normaliser les données d'entrée\n",
    "(algorithme [\\[A:bn1\\]](#A:bn1){reference-type=\"ref\"\n",
    "reference=\"A:bn1\"}).\n",
    "\n",
    "::: algorithm\n",
    ":::\n",
    "\n",
    "Lorsque la descente de gradient est achevée, un post apprentissage est\n",
    "appliqué dans lequel la moyenne et la variance sont calculées sur\n",
    "l'ensemble d'entraînement et remplacent $\\mu_\\mathcal{B}$ et\n",
    "$\\sigma^2_\\mathcal{B}$ (algorithme\n",
    "[\\[A:bn2\\]](#A:bn2){reference-type=\"ref\" reference=\"A:bn2\"}).\n",
    "\n",
    "::: algorithm\n",
    ":::\n",
    "\n",
    "### Couche d'agrégation et de sous-échantillonnage\n",
    "\n",
    "[]{#subsubsec:coucheagreg label=\"subsubsec:coucheagreg\"}\n",
    "\n",
    "::: SCfigure\n",
    "![image](images/fig-agregation2.pdf)\n",
    ":::\n",
    "\n",
    "Le sous-échantillonnage (pooling) des cartes obtenues par les couches\n",
    "précédentes a pour objectif d'assurer une robustesse au bruit et aux\n",
    "distorsions.\n",
    "\n",
    "La sortie d'une couche d'agrégation $l$\n",
    "(figure [\\[F:coucheagreg\\]](#F:coucheagreg){reference-type=\"ref\"\n",
    "reference=\"F:coucheagreg\"}) est composée de $n^{(l)} = n^{(l-1)}$ cartes\n",
    "de taille réduite. En général, l'agrégation est effectuée en déplaçant\n",
    "dans les cartes d'entrée une fenêtre de taille $2p \\times 2p$ toutes les\n",
    "$q$ positions (il y a recouvrement si $q < p$ et non recouvrement\n",
    "sinon), et en calculant, pour chaque position de la fenêtre, une seule\n",
    "valeur, affectée à la position centrale dans la carte de sortie. On\n",
    "distingue généralement deux types d'agrégation :\n",
    "\n",
    "La moyenne\n",
    "\n",
    ":   : on utilise un filtre $\\mathbf{K_B}$ de taille\n",
    "    $(2h_1 + 1)\\times (2h_2 + 1)$ défini par :\n",
    "    $$\\left(\\mathbf{K_B}\\right)_{r,s} = \\frac{1}{(2h_1 + 1)(2h_2 + 1)}$$\n",
    "\n",
    "Le maximum\n",
    "\n",
    ":   : la valeur maximum dans la fenêtre est retenue.\n",
    "\n",
    "Le maximum est souvent utilisé pour assurer une convergence rapide\n",
    "durant la phase d'entraînement. L'agrégation avec recouvrement, elle,\n",
    "semble assurer une réduction du phénomène de surapprentissage\n",
    "\n",
    "### Couche complètement connectée\n",
    "\n",
    "Si $l$ et $(l-1)$ sont des couches complètement connectées, l'équation :\n",
    "$$\\begin{aligned}\n",
    "    z_i^{(l)} = \\sum _{k = 0} ^{m^{(l-1)}} w_{i,k}^{(l)} y_k^{(l-1)}\\quad \\text{ ou }\\quad \\mathbf{Z^{(l)}} = \\mathbf{W^{(l)}} \\mathbf{Y^{(l-1)}}\n",
    "\\end{aligned}$$ avec $\\mathbf{Z^{(l)}}$, $\\mathbf{W^{(l)}}$ et\n",
    "$\\mathbf{Y^{(l-1)}}$ les représentations vectorielle et matricielle des\n",
    "entrées $z_i^{(l)}$, des poids $w_{i,k}^{(l)}$ et des sorties\n",
    "$y_k^{(l-1)}$, permet de relier ces deux couches.\n",
    "\n",
    "Dans le cas contraire, la couche $l$ attend $n^{(l-1)}$ entrées de\n",
    "taille $n_1^{(l-1)} \\times n_2^{(l-1)}$ et le $i^{\\text{e}}$ neurone de\n",
    "la couche $l$ calcule : $$\\begin{aligned}\n",
    "    y_i^{(l)} = f\\left(z_i^{(l)}\\right)\\quad\\text{ avec }\\quad z_i^{(l)} = \\dsum _{j = 1}^{n^{(l-1)}} \\dsum _{r = 1} ^{n_1^{(l-1)}} \\dsum _{s = 1}^{n_2^{(l-1)}} w_{i,j,r,s}^{(l)} \\left(\\mathbf{ Y_j^{(l-1)}} \\right)_{r,s}\n",
    "\\end{aligned}$$ où $w_{i,j,r,s}^{(l)}$ est le poids connectant le\n",
    "neurone en position $(r,s)$ de la $j^{\\text{e}}$ carte de la couche\n",
    "$(l - 1)$ au $i^{\\text{e}}$ neurone de la couche $l$.\n",
    "\n",
    "En pratique, les réseaux convolutifs sont utilisés pour apprendre une\n",
    "hiérarchie dans les données et la (ou les) couche(s) complètement\n",
    "connectée(s) est(sont) utilisée(s) en bout de réseau pour des tâches de\n",
    "classification ou de régression.\n",
    "\n",
    "Une couche de classification classiquement mise en œuvre utilise le\n",
    "classifieur *softmax*, qui généralise la régression logistique au cas\n",
    "multiclasse ($k$ classes). L'ensemble d'apprentissage\n",
    "${\\mathcal E}_a = \\left \\{(\\mathbf{x^{(i)}}, y^{(i)}),i \\in[\\![1\\cdots m]\\!]\\right \\}$\n",
    "est donc tel que $y^{(i)}\\in[\\![1\\cdots k]\\!]$ et le classifieur estime\n",
    "la probabilité $P(y^{(i)}=j |\\mathbf{x^{(i)}})$ pour chaque classe\n",
    "$1\\leq j\\leq k$. Le classifieur softmax calcule cette probabilité selon\n",
    ":\n",
    "$$\\forall j\\in[\\![1\\cdots k]\\!]\\quad P(y^{(i)}=j | \\mathbf{x^{(i)}},\\mathbf{W}) = \\frac{e^{\\mathbf{W_j^\\top x^{(i)}}}}{\\dsum_{l=1}^k e^{\\mathbf{W_l^\\top}\\mathbf{x^{(i)}}}}$$\n",
    "où $\\mathbf{W}$ est la matrice des paramètres du modèle (les poids). Ces\n",
    "paramètres sont obtenus en minimisant une fonction de coût, qui peut par\n",
    "exemple s'écrire :\n",
    "$$J(\\mathbf{W}) =- \\frac{1}{m}\\dsum_{i=1}^m \\dsum_{j=1}^k \\mathbb{I}_{y^{(i)}=j}log\\left ( \\frac{e^{\\mathbf{W_j^\\top x^{(i)}}}}{\\dsum_{l=1}^k e^{\\mathbf{W_l^\\top x^{(i)}}}}\\right ) + \\frac{\\lambda}{2}\\dsum_{i=1}^n \\dsum_{j=1}^k W_{ji}^2\n",
    "\\label{E:softmaxCout}$$ où $\\lambda$ est un paramètre de régularisation\n",
    "contrôlant le second terme du coût qui pénalise les grandes valeurs des\n",
    "poids (régularisation $\\ell_2$).\n",
    "\n",
    "## Régularisation\n",
    "\n",
    "Un des enjeux principaux en apprentissage automatique est de construire\n",
    "des algorithmes ayant une bonne capacité de généralisation. Les\n",
    "stratégies mises en œuvre pour arriver à cette fin rentrent dans la\n",
    "catégorie générale de la régularisation et de nombreuses méthodes sont\n",
    "aujourd'hui proposées en ce sens. Nous faisons ici un focus sur trois\n",
    "stratégies largement utilisées en apprentissage profond.\n",
    "\n",
    "### Régularisation de la fonction de coût\n",
    "\n",
    "L'équation [\\[E:softmaxCout\\]](#E:softmaxCout){reference-type=\"ref\"\n",
    "reference=\"E:softmaxCout\"} est un exemple de régularisation de la\n",
    "fonction de coût, utilisée lors de la phase d'entraînement. À la\n",
    "fonction d'erreur est ajoutée une fonction des poids du réseau, qui peut\n",
    "prendre de multiples formes. Les deux principales stratégies sont :\n",
    "\n",
    "-   La régularisation $\\ell_2$ (ou ridge regression), qui force les\n",
    "    poids à avoir une faible valeur absolue : un terme de régularisation\n",
    "    fonction de la norme $\\ell_2$ de la matrice des poids est ajouté (à\n",
    "    la manière de l'équation\n",
    "    [\\[E:softmaxCout\\]](#E:softmaxCout){reference-type=\"ref\"\n",
    "    reference=\"E:softmaxCout\"}). On parle souvent de *weight decay*.\n",
    "\n",
    "-   La régularisation $\\ell_1$, qui tend à rendre épars le réseau\n",
    "    profond, *i.e.* à imposer à un maximum de poids de s'annuler. Un\n",
    "    terme de régularisation, somme pondérée des valeurs absolues des\n",
    "    poids, est ajouté à la fonction objectif.\n",
    "\n",
    "### Dropout\n",
    "\n",
    "Les techniques de dropout se rapprochent des stratégies classiques de\n",
    "bagging en apprentissage automatique. L'objectif est d'entraîner un\n",
    "ensemble constitué de tous les sous-réseaux qui peuvent être construits\n",
    "en supprimant des neurones (hors neurones d'entrée et de sortie) du\n",
    "réseau initial. Si le réseau comporte $|\\mathbf{W}|$ neurones cachés, il\n",
    "existe ainsi $2^{|\\mathbf{W}|}$ modèles possibles. En pratique, les\n",
    "neurones cachés se voient perturbés par un bruit binomial, qui a pour\n",
    "effet de les empêcher de fonctionner en groupe et de les rendre, au\n",
    "contraire, plus indépendants. Le phénomène de surapprentissage est ainsi\n",
    "fortement réduit sur le réseau, qui doit décomposer les entrées en\n",
    "caractéristiques pertinentes, indépendamment les unes des autres. Les\n",
    "réseaux construits par dropout partagent partiellement leurs paramètres,\n",
    "ce qui diminue l'empreinte mémoire de la méthode.\n",
    "\n",
    "Lors de la phase de prédiction, le réseau complet est utilisé, mais les\n",
    "neurones cachés sont pondérés par la fraction de bruit utilisé pendant\n",
    "l'apprentissage (*i.e.* pour chaque neurone le nombre de fois où il a\n",
    "été supprimé d'un sous-réseau, rapporté au nombre total de réseaux),\n",
    "afin de conserver la valeur moyenne des activations des neurones\n",
    "identiques à celles durant l'apprentissage.\n",
    "\n",
    "Notons qu'il est également possible d'éteindre non pas un neurone, mais\n",
    "un poids. La stratégie correspondante est appelée DropConnect.\n",
    "\n",
    "### Partage de paramètres\n",
    "\n",
    "La régularisation de la fonction de coût permet d'imposer aux poids\n",
    "certaines contraintes (par exemple de rester faibles en amplitude pour\n",
    "la régularisation $\\ell_2$, ou de s'annuler pour la régularisation\n",
    "$\\ell_1$). Il peut également être intéressant d'imposer certains a\n",
    "priori sur les poids, par exemple une dépendance entre les valeurs des\n",
    "paramètres.\n",
    "\n",
    "Une dépendance classique consiste à imposer que les valeurs de certains\n",
    "poids soient proches les unes des autres (dans le cas par exemple où\n",
    "deux modèles de classification $M_1$ et $M_2$, de paramètres\n",
    "$\\mathbf{W_1}$ et $\\mathbf{W_2}$, opèrent sur des données similaires et\n",
    "sur des classes identiques) et, là encore, une stratégie de pénalisation\n",
    "de la fonction objectif peut être utilisée. Cependant, il est plus\n",
    "courant dans ce cas d'imposer que les paramètres soient égaux (dans\n",
    "l'exemple précédent imposer $\\mathbf{W_1}=\\mathbf{W_2}$) et d'arriver à\n",
    "une stratégie dite de partage des paramètres. Dans le cas des réseaux\n",
    "convolutifs utilisés en vision, cette régularisation est assez intuitive\n",
    "puisque les entrées (images) possèdent de nombreuses propriétés\n",
    "invariantes par transformations affines (une image de voiture reste une\n",
    "image de voiture, même si l'image est translatée ou mise à l'échelle,\n",
    "cf. figure [\\[F:partage\\]](#F:partage){reference-type=\"ref\"\n",
    "reference=\"F:partage\"}). Le réseau exploite alors ce partage de\n",
    "paramètres, en calculant une même caractéristique (un neurone et son\n",
    "poids) à différentes positions dans l'image. De ce fait, le nombre de\n",
    "paramètres est drastiquement réduit, ainsi que l'empreinte mémoire du\n",
    "réseau appris.\n",
    "\n",
    "::: SCfigure\n",
    "![image](images/fig-partage-param.pdf){width=\"0.40\\\\linewidth\"}\n",
    ":::\n",
    "\n",
    "## Initialisation\n",
    "\n",
    "Une initialisation convenable des poids est essentielle pour assurer une\n",
    "convergence de la phase d'entraînement. Un choix arbitraire des poids (à\n",
    "zéro, à de petites ou grandes valeurs aléatoires) peut ralentir, voire\n",
    "causer de la redondance dans le réseau (problème de la symétrie).\\\n",
    "Plusieurs schémas d'initialisation ont été proposés et nous donnons dans\n",
    "les deux paragraphes qui suivent d'eux d'entre eux.\n",
    "\n",
    "### Initialisation prenant en compte les variations des neurones\n",
    "\n",
    "Pour illustrer le propos, on suppose que l'entrée du réseau de neurones\n",
    "est composée de $n^{(1)}$ entrées\n",
    "$\\mathbf{x}=(x_1\\ldots x_{n^{(1)}})^\\top$, les $x_i$ étant i.i.d,\n",
    "normalisés selon une loi $\\mathcal{N}(0,\\sigma_x)$. Pour simplifier la\n",
    "démonstration, on suppose que la couche suivante calcule un simple\n",
    "potentiel post synaptique : pour un neurone de cette couche, ce\n",
    "potentiel est $\\mathbf{w}^\\top\\mathbf{x}$, avec\n",
    "$\\mathbf{w}\\in\\mathbb{R}^{n^{(1)}}$ i.i.d $\\mathcal{N}(0,\\sigma_w)$. La\n",
    "variance de ce potentiel est alors\n",
    "$$Var(\\mathbf{w}^\\top\\mathbf{x}) = \\displaystyle\\sum_{i=1}^{n^{(1)}} Var (w_ix_i) =  \\displaystyle\\sum_{i=1}^{n^{(1)}}  \\left (\\mathbb{E}(w_i)^2Var(x_i) + \\mathbb{E}(x_i)^2Var (w_i) + Var (w_i) Var(x_i) \\right )$$\n",
    "Les entrées et les poids sont de moyenne nulle, donc\n",
    "$$Var(\\mathbf{w}^\\top\\mathbf{x}) =  \\displaystyle\\sum_{i=1}^{n^{(1)}}\\sigma_x\\sigma_w$$\n",
    "et puisque les $w_ix_i$ sont i.i.d.\n",
    "$$Var(\\mathbf{w}^\\top\\mathbf{x}) =  {n^{(1)}}\\sigma_x\\sigma_w$$ On\n",
    "montre plus généralement que sur la $l^e$ couche cachée, la variance de\n",
    "$\\mathbf{Y^{(l)}}$ est\n",
    "$$Var(\\mathbf{Y^{(l)}}) = \\left ( n^{(l)} Var(w_i)\\right )^l Var (x_i)$$\n",
    "Chaque neurone peut donc varier dans un rapport de $n^{(l)}$ fois la\n",
    "variation de son entrée (qui est elle même $n^{(l-1)}$ fois la variance\n",
    "de son entrée\\...)\n",
    "\n",
    "On a alors les cas de figure suivants :\n",
    "\n",
    "-   si $n^{(l)} Var(w_i)>1$ le gradient va tendre vers de grandes\n",
    "    valeurs à mesure que l'on s'enfonce dans le réseau (que $l$ croît)\n",
    "\n",
    "-   si $n^{(l)} Var(w_i)<1$ le gradient disparaît à mesure que l'on\n",
    "    s'enfonce dans le réseau\n",
    "\n",
    "Pour éviter ces deux problèmes, la solution est de forcer\n",
    "$n^{(l)} Var(w_i)=1$, soit $Var(w_i)=1/n^{(l)}$. On initialise donc les\n",
    "poids se la couche $l$ selon une loi\n",
    "$$\\frac{1}{\\sqrt{n^{(l-1)}}}\\mathcal{N}(0,1)$$ Cette procédure est la\n",
    "méthode d'initialisation de Xavier, ou de Glorot [@Glorot10].\n",
    "\n",
    "### Initialisation de He\n",
    "\n",
    "He a montré dans [@He15] que la méthode de Xavier pouvait ne pas\n",
    "fonctionner correctement lorsque les traitements non linéaires étaient\n",
    "effectués par la fonction ReLU. Les auteurs proposent alors de plutôt\n",
    "multiplier par $\\frac{\\sqrt{2}}{\\sqrt{n^{(l-1)}}}$ pour prendre en\n",
    "compte la partie négative qui ne participe pas au calcul de la variance.\n",
    "\n",
    "## Apprentissage\n",
    "\n",
    "La présence de nombreuses couches cachées va permettre de calculer des\n",
    "caractéristiques beaucoup plus complexes et informatives des entrées.\n",
    "Chaque couche calculant une transformation non linéaire de la couche\n",
    "précédente, le pouvoir de représentation de ces réseaux s'en trouve\n",
    "amélioré. On peut par exemple montrer qu'il existe des fonctions qu'un\n",
    "réseau à $k$ couches peut représenter de manière compacte (avec un\n",
    "nombre de neurones cachés qui est polynomial en le nombre des entrées),\n",
    "alors qu'un réseau à $k-1$ couches ne peut pas le faire, à moins d'avoir\n",
    "une combinatoire exponentielle sur le nombre de neurones cachés.\\\n",
    "\n",
    "### Le problème de l'entraînement\n",
    "\n",
    "Si l'intérêt de ces réseaux est manifeste, la complexité de leur\n",
    "utilisation vient de l'étape d'apprentissage. Jusqu'à récemment,\n",
    "l'algorithme utilisé était classique et consistait en une initialisation\n",
    "aléatoire des poids du réseau, suivie d'un entraînement sur un ensemble\n",
    "d'apprentissage, en minimisant une fonction objectif. Cependant, dans le\n",
    "cas des réseaux profonds, cette approche peut ne pas être adaptée :\n",
    "\n",
    "-   Les données étiquetées doivent être en nombre suffisant pour\n",
    "    permettre un entraînement efficace, d'autant plus que le réseau est\n",
    "    complexe. Dans le cas contraire, un surapprentissage peut notamment\n",
    "    être induit.\n",
    "\n",
    "-   Sur un tel réseau, l'apprentissage se résume à l'optimisation d'une\n",
    "    fonction fortement non convexe, qui amène presque sûrement à des\n",
    "    minima locaux lorsque des algorithmes classiques sont utilisés.\n",
    "\n",
    "-   Dans l'étape de rétropropagation, les gradients diminuent rapidement\n",
    "    à mesure que le nombre de couches cachées augmente. La dérivée de la\n",
    "    fonction objectif par rapport à $\\mathbf{W}$ devient alors très\n",
    "    faible à mesure que le calcul se rétropropage vers la couche\n",
    "    d'entrée. Les poids des premières couches changent donc très\n",
    "    lentement et le réseau n'est plus en capacité d'apprendre. Ce\n",
    "    problème est connu sous le nom de problème de gradient évanescent\n",
    "    (vanishing gradient).\n",
    "\n",
    "L'algorithme principalement utilisé pour l'apprentissage des réseaux\n",
    "convolutifs reste la rétropropagation du gradient. Le choix de la\n",
    "fonction objectif, de sa régularisation, de la méthode d'optimisation\n",
    "(descente de gradient, méthodes à taux d'apprentissage adaptatifs telles\n",
    "qu'AdaGrad, RMSProp ou Adam) et des paramètres associés, ou des\n",
    "techniques de présentation des exemples (batchs, minibatchs) sont autant\n",
    "de facteurs importants permettant aux modèles non seulement de converger\n",
    "vers un optimum local satisfaisant, mais également de proposer un modèle\n",
    "final ayant une bonne capacité de généralisation.\n",
    "\n",
    "Aujourd'hui, de nombreux réseaux, déjà entraînés, sont mis à\n",
    "disposition. En effet, ces entraînements nécessitent de grandes bases\n",
    "d'apprentissage (type ImageNet) et une puissance de calcul assez élevée\n",
    "(GPU obligatoire(s)). Pour le traitement de problèmes précis, des\n",
    "méthodes existent, qui partent de ces réseaux préentraînés et les\n",
    "modifient localement pour, par exemple, apprendre de nouvelles classes\n",
    "d'images non encore vues par le réseau. L'idée sous-jacente est que les\n",
    "premières couches capturent des caractéristiques bas niveau et que la\n",
    "sémantique vient avec les couches profondes. Ainsi, dans un problème de\n",
    "classification, où les classes n'ont pas été apprises, on peut supposer\n",
    "qu'en conservant les premières couches on extraira des caractéristiques\n",
    "communes des images (bords, colorimétrie,\\...) et qu'en changeant les\n",
    "dernières couches (information sémantique et haut niveau et étage de\n",
    "classification), c'est-à-dire en réapprenant les connexions, on\n",
    "spécifiera le nouveau réseau pour la nouvelle tâche de classification.\n",
    "Cette approche rentre dans le cadre des méthodes de *transfer learning*\n",
    "[@Pan10] et de *fine tuning*, cas particulier d'adaptation de domaine :\n",
    "\n",
    "-   Les méthodes de transfert prennent un réseau déjà entraîné, enlèvent\n",
    "    la dernière couche complètement connectée et traitent le réseau\n",
    "    restant comme un extracteur de caractéristiques. Un nouveau\n",
    "    classifieur, la dernière couche, est alors entraîné sur le nouveau\n",
    "    problème.\n",
    "\n",
    "-   Les méthodes de fine tuning ré-entraînent le classifieur du réseau\n",
    "    et remettent à jour les poids du réseau pré-entraîné par\n",
    "    rétropropagation.\n",
    "\n",
    "Plusieurs facteurs influent sur le choix de la méthode à utiliser : la\n",
    "taille des données d'apprentissage du nouveau problème et la\n",
    "ressemblance du nouveau jeu de données avec celui qui a servi à\n",
    "entraîner le réseau initial :\n",
    "\n",
    "-   Pour un jeu de données similaire de petite taille, on utilise du\n",
    "    transfer learning, avec un classifieur utilisé sur les\n",
    "    caractéristiques calculées sur les dernières couches du réseau\n",
    "    initial.\n",
    "\n",
    "-   Pour un jeu de données de petite taille et un problème différent, on\n",
    "    utilise du transfer learning, avec un classifieur utilisé sur les\n",
    "    caractéristiques calculées sur les premières couches du réseau\n",
    "    initial.\n",
    "\n",
    "-   Pour un jeu de données, similaire ou non, de grande taille, on\n",
    "    utilise le fine tuning.\n",
    "\n",
    "Notons qu'il est toujours possible d'augmenter la taille du jeu de\n",
    "données par des technique d'augmentation de données.\n",
    "\n",
    "Dans le cas où un réseau ad hoc doit être construit et où une base\n",
    "d'apprentissage suffisante est disponible, l'entraînement par\n",
    "optimisation reste possible. Il existe en particulier des techniques\n",
    "d'apprentissage couche à couche, lorsque les couches successives\n",
    "calculent des fonctions d'activation des couches précédentes (empilement\n",
    "d'autoencodeurs par exemple).\n",
    "\n",
    "### Apprentissage glouton par couche\n",
    "\n",
    "L'idée est d'entraîner les couches une à une, d'abord dans un réseau à\n",
    "une couche cachée, puis à deux couches cachées\\... À chaque étape $k$,\n",
    "la couche $k$ est ajoutée et a pour entrée la couche $k-1$ précédemment\n",
    "entraînée. L'entraînement peut être supervisé, mais le plus souvent il\n",
    "est non supervisé. Les poids issus de cet entraînement servent\n",
    "d'initialisation pour le réseau final.\\\n",
    "En comparaison des points précédents, cette approche est bien plus\n",
    "pertinente :\n",
    "\n",
    "-   Les données non étiquetées sont très faciles à obtenir.\n",
    "\n",
    "-   L'initialisation des poids sur des données non étiquetées est plus\n",
    "    performante qu'une initialisation aléatoire. Empiriquement, une\n",
    "    méthode type descente de gradient permet d'aboutir à un meilleur\n",
    "    minimum local (les données non étiquetées fournissent en effet des\n",
    "    informations a priori déjà importantes sur les données).\n",
    "\n",
    "### Visualisation du mécanisme des réseaux convolutifs\n",
    "\n",
    "Le mécanisme interne des réseaux convolutifs est mal compris et\n",
    "l'analyse des raisons qui font que leur puissance de prédiction est\n",
    "importante n'est pas aisée. S'il est toujours possible de rétroprojeter\n",
    "les activations depuis la première couche de convolution, les couches\n",
    "d'agrégation et de rectification empêchent de comprendre le\n",
    "fonctionnement des couches suivantes, ce qui peut être gênant dans la\n",
    "construction et l'amélioration de ces réseaux.\n",
    "\n",
    "Les méthodes de visualisation du fonctionnement des réseaux convolutifs\n",
    "peuvent être rangées en trois catégories, décrites dans les paragraphes\n",
    "suivants.\n",
    "\n",
    "### Méthodes de visualisation de base\n",
    "\n",
    "Les méthodes les plus simples consistent à visualiser les activations\n",
    "lors du passage d'une image dans le réseau. Pour des activations type\n",
    "ReLU, ces activations sont ininterprétables au début de l'entraînement,\n",
    "mais à mesure que ce dernier progresse, les cartes d'activation\n",
    "$\\mathbf{Y_i^{(l)}}$ deviennent localisées et éparses.\n",
    "\n",
    "Il est également possible de visualiser les filtres des différentes\n",
    "couches de convolution (figure\n",
    "[1.3](#F:viewfilters){reference-type=\"ref\" reference=\"F:viewfilters\"}).\n",
    "Les filtres des premières couches agissent comme des détecteurs de bords\n",
    "et coins et, à mesure que l'on s'enfonce dans le réseau, les filtres\n",
    "capturent des concepts haut niveau comme des objets ou encore des\n",
    "visages.\n",
    "\n",
    "Citons encore d'autres méthodes qui proposent de visualiser les\n",
    "dernières couches (les couches complètement connectées) de grande\n",
    "dimension (par exemple 4096 pour AlexNet) via une méthode de réduction\n",
    "de dimension.\n",
    "\n",
    "<figure id=\"F:viewfilters\">\n",
    "<img src=\"images/viewfilters\" />\n",
    "<figcaption>Visualisation des filtres de la première couche d’AlexNet (à\n",
    "gauche, 64 filtres 11<span class=\"math inline\">×</span>11) et de\n",
    "ResNet-18 (à droite, 64 filtres 7<span\n",
    "class=\"math inline\">×</span>7).</figcaption>\n",
    "</figure>\n",
    "\n",
    "### Méthodes fondées sur les activations\n",
    "\n",
    "Plusieurs stratégies peuvent être adoptées pour sonder le fonctionnement\n",
    "d'un réseau convolutif, en utilisant les informations portées par les\n",
    "cartes $\\mathbf{Y_i^{(l)}}$, parmi lesquelles :\n",
    "\n",
    "-   Utiliser des couches de convolution transposée (improprement\n",
    "    appelées parfois couches de déconvolution), ajoutées à chaque couche\n",
    "    de convolution du réseau. Étant données les cartes d'entrée de la\n",
    "    couche $l$, les cartes de sortie $\\mathbf{Y_i^{(l)}}$ sont envoyées\n",
    "    dans la couche de convolution transposée correspondante au niveau\n",
    "    $l$. Cette dernière reconstruit les $\\mathbf{Y_i^{(l-1)}}$ qui ont\n",
    "    permis le calcul des activations de la couche $l$. Le processus est\n",
    "    alors itéré jusqu'à atteindre la couche d'entrée $l = 1$, les\n",
    "    activations de la couche $l$ étant alors rétroprojetées dans le plan\n",
    "    image [@ZE13]. La présence de couches d'agrégation et de\n",
    "    rectification rend ce processus non inversible (par exemple, une\n",
    "    couche d'agrégation maximum nécessite de connaître à quelles\n",
    "    positions de l'image $\\mathbf{Y_i^{(l)}}$ sont situés les maxima\n",
    "    retenus).\n",
    "\n",
    "-   Faire passer un grand nombre d'images dans le réseau et, pour un\n",
    "    neurone particulier, conserver celle qui a le plus activé ce\n",
    "    neurone. Il est alors possible de visualiser les images pour\n",
    "    comprendre ce à quoi le neurone s'intéresse dans son champ réceptif\n",
    "    (figure [1.4](#F:champreceptif){reference-type=\"ref\"\n",
    "    reference=\"F:champreceptif\"}).\n",
    "\n",
    "    <figure id=\"F:champreceptif\">\n",
    "    <img src=\"images/champreceptif\" />\n",
    "    <figcaption>Champ récéptif de quelques neurones de la dernière couche\n",
    "    d’agrégation du réseau AlexNet, superposées aux images ayant le plus\n",
    "    fortement activé ces neurones. Le champ est encadré en blanc, et la\n",
    "    valeur d’activation correspondante est reportée en haut. On voit par\n",
    "    exemple que certains neurones sont très sensibles aux textes, d’autres\n",
    "    aux réflexions spéculaires, ou encore aux hauts du corps (source : <span\n",
    "    class=\"citation\" data-cites=\"Girshick14\"></span>).</figcaption>\n",
    "    </figure>\n",
    "\n",
    "-   Cacher (par un rectangle noir par exemple) différentes parties de\n",
    "    l'image d'entrée qui est d'une certaine classe (disons un chien) et\n",
    "    observer la sortie du réseau (la probabilité de la classe de l'image\n",
    "    d'entrée). En représentant les valeurs de probabilité de la classe\n",
    "    d'intérêt comme une fonction de la position du rectangle occultant,\n",
    "    il est possible de voir si le réseau s'intéresse effectivement aux\n",
    "    parties de l'image spécifiques de la classe, ou à des autres zones\n",
    "    (le fond par exemple) (figure\n",
    "    [1.5](#F:occlusion){reference-type=\"ref\" reference=\"F:occlusion\"}).\n",
    "\n",
    "    <figure id=\"F:occlusion\">\n",
    "    <img src=\"images/occlusion\" />\n",
    "    <figcaption>Occlusion d’une image (à gauche). Le rectangle noir est\n",
    "    déplacé dans l’image et pour chaque position la probabilité de la classe\n",
    "    de l’image (ici un loulou de Poméranie) est enregistrée. Ces\n",
    "    probabilités sont ensuite représentées sous forme d’une carte 2D (à\n",
    "    droite). La probabilité de la classe s’effondre lorsque le rectangle\n",
    "    couvre une partie de la face du chien. Cela suggère que cette face est\n",
    "    grandement responsable de la forte probabilité de classement de l’image\n",
    "    comme un loulou. A l’inverse, l’occlusion du fond n’altère pas la forte\n",
    "    valeur de probabilité de la classe (source : <span class=\"citation\"\n",
    "    data-cites=\"ZE13\"></span>).</figcaption>\n",
    "    </figure>\n",
    "\n",
    "### Méthodes fondées sur le gradient\n",
    "\n",
    "Pour comprendre quelle(s) partie(s) de l'image est (sont) utilisée(s)\n",
    "par le réseau pour effectuer une prédiction, il est possible de calculer\n",
    "des cartes de saillance (saliency maps). L'idée est relativement simple\n",
    ": calculer le gradient de la classe de sortie par rapport à l'image\n",
    "d'entrée. Cela indique à quel point une petite variation dans l'image\n",
    "induit un changement de prédiction. En visualisant les gradients, on\n",
    "observe alors par exemple leurs fortes valeurs, indiquant qu'une petite\n",
    "variation du pixel correspondant augmente la valeur de sortie.\\\n",
    "Il est également possible d'utiliser le gradient par rapport à la\n",
    "dernière couche de convolution (approche Grad-CAM), ce qui permet de\n",
    "récupérer des informations de localisation spatiale des régions\n",
    "importantes pour la prédiction (figure\n",
    "[1.6](#F:grad){reference-type=\"ref\" reference=\"F:grad\"}, droite).\\\n",
    "Plus généralement, en choisissant un neurone intermédiaire du réseau\n",
    "(d'une couche de convolution), la méthode de rétropropagation guidée\n",
    "calcule le gradient de sa valeur par rapport aux pixels de l'image\n",
    "d'entrée, ce qui permet de souligner les parties de l'image auxquelles\n",
    "ce neurone répond (figure [1.6](#F:grad){reference-type=\"ref\"\n",
    "reference=\"F:grad\"}, milieu).\n",
    "\n",
    "<figure id=\"F:grad\">\n",
    "<img src=\"images/gradCam\" />\n",
    "<figcaption>Approches par gradient de visualisation du fonctionnement\n",
    "d’un réseau convolutif. Comparaison de la méthode de rétropropagation\n",
    "guidée et de Grad-CAM.(source : <span class=\"citation\"\n",
    "data-cites=\"Selvaraju17\"></span>).</figcaption>\n",
    "</figure>\n",
    "\n",
    "Ces gradients peuvent également être utilisés dans la méthode de montée\n",
    "de gradient (gradient Ascent), dont l'objectif est de générer une image\n",
    "qui active de manière maximale un neurone donné du réseau. Le principe\n",
    "est d'itérativement passer l'image d'entrée $\\mathbf{I}$ dans le réseau\n",
    "pour obtenir les valeurs des cartes $\\mathbf{Y_i^{(l)}}$, de\n",
    "rétropropager pour obtenir le gradient d'un neurone par rapport aux\n",
    "pixels de $\\mathbf{I}$ et d'opérer une petite modification de ces\n",
    "pixels. Outre son aspect informatif sur la structure interne du réseau\n",
    "étudié (visualisation des cartes $\\mathbf{Y_i^{(l)}}$ intermédiaires),\n",
    "cette méthode produit des images parfois très artistiques (figure\n",
    "[1.7](#F:gradAscent){reference-type=\"ref\" reference=\"F:gradAscent\"}).\n",
    "\n",
    "<figure id=\"F:gradAscent\">\n",
    "<img src=\"images/gradAscent\" />\n",
    "<figcaption>Visualisation des 4 premières couches de convolution d’un\n",
    "réseau convolutif par montée de gradient (source : <span\n",
    "class=\"citation\" data-cites=\"Yosinski15\"></span>).</figcaption>\n",
    "</figure>\n",
    "\n",
    "## Quelques applications {#subsubsec:applis}\n",
    "\n",
    "Depuis leur introduction en reconnaissance de caractères manuscrits\n",
    "[@LeCun90], les réseaux convolutifs n'ont cessé de trouver des champs\n",
    "applicatifs, commerciaux et parfois ludiques. Parmi ces applications,\n",
    "nous en citons ici quelques-unes.\n",
    "\n",
    "##### La classification d'images\n",
    "\n",
    "  \\\n",
    "Premier domaine d'application des réseaux convolutifs, la classification\n",
    "d'images consiste à affecter une image à une classe, apprise par le\n",
    "réseau sur un grand nombre d'exemples. Depuis l'avènement d'ImageNet, et\n",
    "la mise en place de la compétition ILSVRC, les résultats obtenus ne\n",
    "cessent de s'améliorer et sont depuis quelques années la référence dans\n",
    "ce domaine (dépassant même les performances humaines en 2015).\n",
    "\n",
    "##### L'annotation de scènes\n",
    "\n",
    "  \\\n",
    "Des réseaux convolutifs ont été utilisés pour annoter des scènes 2D ou\n",
    "2D+t, *i.e.* assigner à chaque pixel un label identifiant l'objet auquel\n",
    "il appartient. De nombreux réseaux ont été développés à cet effet\n",
    "(R-CNN, Fast R-CNN, Mast R-CNN par exemple)\n",
    "(figure [\\[F:annotation\\]](#F:annotation){reference-type=\"ref\"\n",
    "reference=\"F:annotation\"}).\n",
    "\n",
    "##### La reconnaissance d'actions\n",
    "\n",
    "  \\\n",
    "Le développement de champs réceptifs 3D dans les réseaux convolutifs a\n",
    "permis d'extraire dans ces réseaux des caractéristiques invariantes à la\n",
    "translation. L'intégration de techniques de régularisation adaptée (type\n",
    "partage de paramètres) a rendu possible l'optimisation de ces réseaux\n",
    "pour la reconnaissance d'actions dans des scènes dynamiques.\n",
    "\n",
    "##### L'analyse de documents\n",
    "\n",
    "  \\\n",
    "L'analyse de documents à des fins de reconnaissance de caractères, de\n",
    "classification de documents ou encore d'annotation sémantique a\n",
    "largement bénéficié de l'apport des réseaux convolutifs.\n",
    "\n",
    "#####  L'augmentation de données\n",
    "\n",
    "  \\\n",
    "De nombreux réseaux ont été proposés pour ajouter à des données nD des\n",
    "informations manquantes (colorisation d'images,\n",
    "figure [\\[F:color\\]](#F:color){reference-type=\"ref\"\n",
    "reference=\"F:color\"}, inpainting, restauration d'images,\n",
    "superrésolution), ou pour proposer des versions différentes des données\n",
    "initiales en fonction d'une contrainte de style extérieure (transfert de\n",
    "style, figure [\\[F:transfer\\]](#F:transfer){reference-type=\"ref\"\n",
    "reference=\"F:transfer\"}).\n",
    "\n",
    "<figure>\n",
    "\n",
    "<figcaption>Quelques applications des réseaux convolutifs.</figcaption>\n",
    "</figure>\n",
    "\n",
    "Les réseaux convolutifs sont des réseaux spécialisés pour traiter des\n",
    "données dont la topologie se conforme à une structure de grille\n",
    "n-dimensionnelle. Dans le cas de données 1D séquentielles, d'autres\n",
    "réseaux performants ont été développés : les réseaux récurrents."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "md:myst",
   "text_representation": {
    "extension": ".md",
    "format_name": "myst"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "source_map": [
   11
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}