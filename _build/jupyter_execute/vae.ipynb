{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f487c044",
   "metadata": {},
   "source": [
    "# Autoencodeurs variationnels\n",
    "Les autoencodeurs variationnels sont, comme les GAN (abordés dans le cours suivant), des modèles génératifs, c'est-à-dire des modèles probabilistes $p$ pouvant être utilisés pour \n",
    "simuler (ou générer) des données réalistes $\\boldsymbol x\\sim p(\\boldsymbol x,\\boldsymbol\\theta)$, aussi proches que possible de la vraie (mais inconnue) distribution des données $p(\\boldsymbol x)$, pour laquelle seul un échantillon de données est disponible.\n",
    "\n",
    "Le paysage de ces modèles génératifs s'est beaucoup peuplé depuis 2014 ({numref}`landscape`).\n",
    "\n",
    "\n",
    "\n",
    "```{figure} ./images/landscapegen.png\n",
    ":name: landscape\n",
    "Paysage des modèles génératifs (Source: [Song et al., CVPR 2023](https://cvpr2023.thecvf.com/virtual/2023/tutorial/18546))\n",
    "```\n",
    "\n",
    "Dans ce cours, nous aborderons uniquement les autoencodeurs variationnels (VAE) et les réseaux antagonistes générateurs (GAN).\n",
    "\n",
    "\n",
    "\n",
    "## Inférence variationnelle\n",
    "\n",
    "### Modèles à variables latentes\n",
    "\n",
    "Un modèle à variables latentes met en relation un ensemble de variables observables $\\boldsymbol x\\in \\mathcal X$ avec un ensemble de variables latentes  $\\boldsymbol h\\in \\mathcal H$\n",
    "\n",
    "$$p(\\boldsymbol x,\\boldsymbol h) = p(\\boldsymbol x|\\boldsymbol h)p(\\boldsymbol h)$$\n",
    "\n",
    "Si $\\boldsymbol h$ sont des facteurs causaux pour  $\\boldsymbol x$, alors échantillonner selon $p(\\boldsymbol x|\\boldsymbol h)$ permet de créer un modèle génératif de  $\\mathcal H$ vers $\\mathcal X$.\n",
    "\n",
    "Pour l'inférence, étant donnée $p(\\boldsymbol x,\\boldsymbol h)$, il \"suffit\" de calculer \n",
    "\n",
    "$$ p(\\boldsymbol h|\\boldsymbol x) = \\frac{p(\\boldsymbol x|\\boldsymbol h)p(\\boldsymbol h)}{p(\\boldsymbol x)}$$\n",
    "\n",
    "Malheureusement, $p(\\boldsymbol x)$ est inaccessible.\n",
    "\n",
    "### Inférence variationnelle\n",
    "\n",
    "L'inférence variationnelle transforme l'estimation de $p(\\boldsymbol h|\\boldsymbol x)$ en un problème d'optimisation.\n",
    "\n",
    "On considère une famille de distributions $q_{\\boldsymbol\\phi} (\\boldsymbol{h}|\\boldsymbol x)$ approchant $p(\\boldsymbol{h}|\\boldsymbol x)$, où $\\boldsymbol \\phi$ sont les paramètres variationnels. Ces paramètres sont optimisés pour minimiser une distance entre $q_{\\boldsymbol\\phi} (\\boldsymbol{h}|\\boldsymbol x)$ et $p(\\boldsymbol x,\\boldsymbol h)$. Parmi toutes les distances possibles, on retient la divergence de Kullback-Leibler\n",
    "\n",
    " $$KL(p\\|q) = \\sum_{x \\in X} p(x) \\log \\frac{p(x)}{q(x)}$$\n",
    "\n",
    "et ainsi : \n",
    "\n",
    "$$\\begin{align}\n",
    "KL( q_{\\boldsymbol\\phi} (\\boldsymbol{h}|\\boldsymbol x)|| p(\\boldsymbol h| \\boldsymbol  x)) &=& \\mathbb{E}_{ q_{\\boldsymbol\\phi} (\\boldsymbol{h}|\\boldsymbol x)} \\left [log\\frac{ q_{\\boldsymbol\\phi} (\\boldsymbol{h}|\\boldsymbol x)}{ p(\\boldsymbol{h}|\\boldsymbol x)} \\right ]\\\\\n",
    "&=& \\mathbb{E}_{ q_{\\boldsymbol\\phi} (\\boldsymbol{h}|\\boldsymbol x)}  \\left [ log(q_{\\boldsymbol\\phi} (\\boldsymbol{h}|\\boldsymbol x))-log(p(\\boldsymbol{x},\\boldsymbol h))\\right ] +log(p(\\boldsymbol x))\n",
    "\\end{align}$$\n",
    "\n",
    "Le dernier terme $log(p(\\boldsymbol x)$ reste cependant toujours inaccessible.\n",
    "\n",
    "Cependant, \n",
    "\n",
    "$$\\begin{align}\n",
    "\\displaystyle\\min_\\boldsymbol\\phi KL( q_{\\boldsymbol\\phi} (\\boldsymbol{h}|\\boldsymbol x)|| p(\\boldsymbol h| \\boldsymbol  x)) \n",
    "&=&\\displaystyle\\min_\\phi log(p(\\boldsymbol x))- \\mathbb{E}_{ q_{\\boldsymbol\\phi} (\\boldsymbol{h}|\\boldsymbol x)}  \\left [log(p(\\boldsymbol{x},\\boldsymbol h)) - log (q_{\\boldsymbol\\phi} (\\boldsymbol{h}|\\boldsymbol x))\\right ]\\\\\n",
    "&=&\\displaystyle\\max_\\phi \\underbrace{\\mathbb{E}_{ q_{\\boldsymbol\\phi} (\\boldsymbol{h}|\\boldsymbol x)}  \\left [log(p(\\boldsymbol{x},\\boldsymbol h)) - log (q_{\\boldsymbol\\phi} (\\boldsymbol{h}|\\boldsymbol x))\\right ]}_{ELBO(\\boldsymbol x,\\phi)}\n",
    "\\end{align}$$\n",
    "\n",
    "avec ELBO(Evidence Lower Bound Objective) définie par \n",
    "\n",
    "$$\\begin{align}\n",
    "ELBO(\\boldsymbol x,\\boldsymbol\\phi)&=&\\mathbb{E}_{ q_{\\boldsymbol\\phi} (\\boldsymbol{h}|\\boldsymbol x)}  \\left [log(p(\\boldsymbol{x},\\boldsymbol h)) - log (q_{\\boldsymbol\\phi} (\\boldsymbol{h}|\\boldsymbol x))\\right ] \\\\\n",
    "&=& \\mathbb{E}_{ q_{\\boldsymbol\\phi} (\\boldsymbol{h}|\\boldsymbol x)}  \\left [log(p(\\boldsymbol{x}|\\boldsymbol h))p(\\boldsymbol h) - log (q_{\\boldsymbol\\phi} (\\boldsymbol{h}|\\boldsymbol x))\\right ]\\\\\n",
    "&=& \\mathbb{E}_{ q_{\\boldsymbol\\phi} (\\boldsymbol{h}|\\boldsymbol x)}  \\left [\\color{blue}{log(p(\\boldsymbol{x}|\\boldsymbol h))}\\right ]-\\color{red}{KL( q_{\\boldsymbol\\phi} (\\boldsymbol{h}|\\boldsymbol x)|| p(\\boldsymbol h))}\n",
    "\\end{align}$$\n",
    "\n",
    "Dans l'équation prédécente, on a éliminé $log(p(\\boldsymbol x))$ qui ne dépend pas de $\\boldsymbol\\phi$.\n",
    "\n",
    "En maximisant la fonction $ELBO(\\boldsymbol x,\\boldsymbol\\phi)$ :\n",
    "- <span style=\"color:blue\"> le premier terme encourage les distributions à converger dans les configurations des variables latentes $\\boldsymbol h$ expliquant les données observées </span>\n",
    "- <span style=\"color:red\"> le second terme force les distribution à être proches du prior.</span>\n",
    "\n",
    "Finalement, étant donné un échantillon  $E_a = \\{\\boldsymbol x_i,i\\in[\\![1,N]\\!]\\}$, la fonction objectif finale est \n",
    "\n",
    "$$\\displaystyle\\sum_{\\boldsymbol x_i\\in E_a}ELBO(\\boldsymbol x_i,\\boldsymbol \\phi)$$\n",
    "\n",
    "Pour la maximiser, on peut utiliser une montée de gradient, maix $\\nabla_{\\boldsymbol\\phi}ELBO(\\boldsymbol x_i,\\boldsymbol \\phi)$ est en général difficile à calculer.\n",
    "\n",
    "\n",
    "## Autoencodeurs variationnels\n",
    "\n",
    "Un autoencodeur variationnel (VAE) est un modèle profond à variables latentes tel que :\n",
    "- $p(\\boldsymbol h)$ est prescrit à l'avance\n",
    "- la vraisemblance $p_{\\boldsymbol\\theta}(\\boldsymbol x|\\boldsymbol h)$ est un décodeur (réseau génératif) $D_{\\boldsymbol\\theta}$ tel que $\\boldsymbol\\Phi = D_{\\boldsymbol\\theta}(\\boldsymbol h)$ où $\\\\boldsymbol Phi$  sont les paramètres de la distribution des données. Par exemple \n",
    "\n",
    "$$\\mu,\\sigma  = D_{\\boldsymbol\\theta}(\\boldsymbol h), \\quad p_{\\boldsymbol\\theta}(\\boldsymbol x|\\boldsymbol h) = \\mathcal N(\\boldsymbol x,\\mu,\\sigma^2 \\boldsymbol I)$$\n",
    "- la distribution approchée $q_{\\boldsymbol\\phi}(\\boldsymbol h|\\boldsymbol x)$ est paramétrée par un encodeur (réseau d'inférence) $E_{\\boldsymbol\\phi}$  tel que $\\boldsymbol\\nu = E_{\\boldsymbol\\phi}(\\boldsymbol x)$ sont les paramètres de la distribution approchée. Par exemple : \n",
    "\n",
    "$$ \\mu,\\sigma  = \\boldsymbol\\nu = E_{\\boldsymbol\\phi}(\\boldsymbol x)\\quad q_{\\boldsymbol\\phi}(\\boldsymbol h|\\boldsymbol x) = \\mathcal N(\\boldsymbol h,\\mu,\\sigma^2 \\boldsymbol I)$$"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "md:myst",
   "text_representation": {
    "extension": ".md",
    "format_name": "myst"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "source_map": [
   11
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}