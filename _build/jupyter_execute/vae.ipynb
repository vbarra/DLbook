{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f0bf043",
   "metadata": {},
   "source": [
    "::: frame\n",
    "::: center\n",
    "![image](../images/VAE2.png){height=\"80pt\"}\n",
    ":::\n",
    ":::\n",
    "\n",
    "# Generative models\n",
    "\n",
    "::: frame\n",
    "Generative models\n",
    "\n",
    "::: Lblock\n",
    "Learning a model that represents the distribution of data, with input\n",
    "training samples\n",
    "\n",
    "$$\\mathcal{P}_{model}(x)\\sim  \\mathcal{P}_{data}(x)$$\n",
    ":::\n",
    "\n",
    "![image](../images/mnist.png)$\\quad \\Longrightarrow\\quad$\n",
    "![image](../images/mnist2.png)\n",
    ":::\n",
    "\n",
    "::: frame\n",
    "Generative models\n",
    "\n",
    "![image](../images/generativemodels.png)\n",
    "\n",
    "Source: G Louppe\n",
    ":::\n",
    "\n",
    "::: frame\n",
    "Generative models\n",
    "\n",
    "### The landscape of generative models\n",
    "\n",
    "![image](../images/landscapegen)\n",
    "\n",
    "Source: Song et al., CVPR 2023.\n",
    "\n",
    "::: Lblock\n",
    "-   Variational Autoencoders\n",
    "\n",
    "-   Generative Adversarial Networks\n",
    ":::\n",
    ":::\n",
    "\n",
    "# Latent models\n",
    "\n",
    "::: frame\n",
    "Latent models A latent variable model relates a set of observable\n",
    "variables $\\boldsymbol x\\in X$ to a set of latent variables $\\boldsymbol h\\in H$\n",
    "\n",
    "$$\\textbf{\\textsf{\\textup{p}}}(\\boldsymbol x,\\boldsymbol h) = \\textbf{\\textsf{\\textup{p}}}(\\boldsymbol x|\\boldsymbol h)\\textbf{\\textsf{\\textup{p}}}(\\boldsymbol h)$$\n",
    "\n",
    "if $\\boldsymbol h$ are causal factors for $\\boldsymbol x$ $\\Rightarrow$\n",
    "sampling from\n",
    "$\\textbf{\\textsf{\\textup{p}}}(\\boldsymbol x|\\boldsymbol h)$ = generative\n",
    "process from $H$ to $X$.\n",
    "\n",
    "::: Lblock\n",
    "Inference: given\n",
    "$\\textbf{\\textsf{\\textup{p}}}(\\boldsymbol x,\\boldsymbol h)$, compute\n",
    "$$\\textbf{\\textsf{\\textup{p}}}(\\boldsymbol h|\\boldsymbol x) = \\frac{\\textbf{\\textsf{\\textup{p}}}(\\boldsymbol x|\\boldsymbol h)\\textbf{\\textsf{\\textup{p}}}(\\boldsymbol h)}{\\textcolor{red}{\\textbf{\\textsf{\\textup{p}}}(\\boldsymbol x)}}$$\n",
    "[Intractable]{style=\"color: red\"}\n",
    ":::\n",
    ":::\n",
    "\n",
    "::: frame\n",
    "Latent models\n",
    "\n",
    "::: Lblock\n",
    "-   $\\textbf{\\textsf{\\textup{p}}}_{\\phi} (\\boldsymbol{h}|\\boldsymbol x)$:\n",
    "    family of distributions approximating\n",
    "    $\\textbf{\\textsf{\\textup{p}}}(\\boldsymbol{h}|\\boldsymbol x)$\n",
    "\n",
    "-   $\\phi$ is optimized to minimize the \\\"distance\\\" between both\n",
    "    distributions.\n",
    "\n",
    "-   Among all similarity measures: Kullback Leibler divergence\n",
    "\n",
    "$$\\begin{aligned}\n",
    "KL( \\textbf{\\textsf{\\textup{p}}}_{\\phi} (\\boldsymbol{h}|\\boldsymbol x)|| \\textbf{\\textsf{\\textup{p}}}(\\boldsymbol h| \\boldsymbol  x)) &=& \\mathbb{E}_{ \\textbf{\\textsf{\\textup{p}}}_{\\phi} (\\boldsymbol{h}|\\boldsymbol x)} \\left [log\\frac{ \\textbf{\\textsf{\\textup{p}}}_{\\phi} (\\boldsymbol{h}|\\boldsymbol x)}{ \\textbf{\\textsf{\\textup{p}}}(\\boldsymbol{h}|\\boldsymbol x)} \\right ]\\\\\n",
    "&=& \\mathbb{E}_{ \\textbf{\\textsf{\\textup{p}}}_{\\phi} (\\boldsymbol{h}|\\boldsymbol x)}  \\left [ log(\\textbf{\\textsf{\\textup{p}}}_{\\phi} (\\boldsymbol{h}|\\boldsymbol x))-log(\\textbf{\\textsf{\\textup{p}}}(\\boldsymbol{x},\\boldsymbol h))\\right ] +\\textcolor{red}{log(\\textbf{\\textsf{\\textup{p}}}(\\boldsymbol x))}\n",
    "\\end{aligned}$$ \n",
    "\n",
    "[Still intractable]{style=\"color: red\"}\n",
    ":::\n",
    ":::\n",
    "\n",
    "::: frame\n",
    "Latent models But\\... \n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\displaystyle\\min_\\phi KL( \\textbf{\\textsf{\\textup{p}}}_{\\phi} (\\boldsymbol{h}|\\boldsymbol x)|| \\textbf{\\textsf{\\textup{p}}}(\\boldsymbol h| \\boldsymbol  x)) \n",
    "&=&\\displaystyle\\min_\\phi log(\\textbf{\\textsf{\\textup{p}}}(\\boldsymbol x))- \\mathbb{E}_{ \\textbf{\\textsf{\\textup{p}}}_{\\phi} (\\boldsymbol{h}|\\boldsymbol x)}  \\left [log(\\textbf{\\textsf{\\textup{p}}}(\\boldsymbol{x},\\boldsymbol h)) - log (\\textbf{\\textsf{\\textup{p}}}_{\\phi} (\\boldsymbol{h}|\\boldsymbol x))\\right ]\\\\\n",
    "&=&\\displaystyle\\max_\\phi \\underbrace{\\mathbb{E}_{ \\textbf{\\textsf{\\textup{p}}}_{\\phi} (\\boldsymbol{h}|\\boldsymbol x)}  \\left [log(\\textbf{\\textsf{\\textup{p}}}(\\boldsymbol{x},\\boldsymbol h)) - log (\\textbf{\\textsf{\\textup{p}}}_{\\phi} (\\boldsymbol{h}|\\boldsymbol x))\\right ]}_{ELBO(\\boldsymbol x,\\phi)}\n",
    "\\end{aligned}$$\n",
    "\n",
    ":::\n",
    "\n",
    "# VAE\n",
    "\n",
    "::: frame\n",
    "Autoencoders\n",
    "\n",
    "myTrapezium/.pic = (0,0) -- (0,)̱ -- (,)̧ -- (,-)̧ -- (0,-)̱ -- cycle ;\n",
    "(-center) at (/2,0); (-out) at (,0);\n",
    "\n",
    "= \\[thick, decoration=markings,mark=at position 1 with , double\n",
    "distance=1.4pt, shorten \\>= 5.5pt, preaction = decorate, postaction =\n",
    "draw,line width=1.4pt, white,shorten \\>= 4.5pt\\]\n",
    "\n",
    "::: center\n",
    ":::\n",
    "\n",
    "::: Lblock\n",
    "-   A neural network trained using unsupervised learning\n",
    "\n",
    "-   Trained to copy its input to its output\n",
    "\n",
    "-   Learns an embedding $h$\n",
    "\n",
    "$$\\hat{\\boldsymbol x} = g[f(\\boldsymbol x)]\\quad h = f(\\boldsymbol x)$$\n",
    ":::\n",
    ":::\n",
    "\n",
    "::: frame\n",
    "Variational Autoencoders\n",
    "\n",
    "::: center\n",
    "![image](../images/xhx){width=\".8\\\\textwidth\"}\n",
    ":::\n",
    ":::\n",
    "\n",
    "::: frame\n",
    "ELBO\n",
    "\n",
    "::: Lblock\n",
    "\n",
    "$$\\displaystyle\\min_{\\phi,\\theta} (-ELBO(\\boldsymbol x,\\phi)) = -\\mathbb{E}_{ \\textbf{\\textsf{\\textup{p}}}_{\\phi} (\\boldsymbol{h}|\\boldsymbol x)}  \\left [\\textcolor{bluelimos}{log(\\textbf{\\textsf{\\textup{p}}}_\\theta(\\boldsymbol{x}|\\boldsymbol h))}\\right ]+\\textcolor{orange}{KL( \\textbf{\\textsf{\\textup{p}}}_{\\phi} (\\boldsymbol{h}|\\boldsymbol x)|| \\textbf{\\textsf{\\textup{p}}}(\\boldsymbol h))}$$\n",
    "\n",
    "-   Given $\\theta$, optimize $\\phi$ so that latent variable distribution\n",
    "    explains the observed data, while remaining close to the prior\n",
    "\n",
    "-   Given $\\phi$, optimize $\\theta$ so that observed data is well\n",
    "    explained by the latent variables.\n",
    ":::\n",
    ":::\n",
    "\n",
    "::: frame\n",
    "Variational Autoencoders\n",
    ":::\n",
    "\n",
    "::: frame\n",
    "Variational Autoencoders\n",
    "\n",
    "Let $\\boldsymbol x_1\\cdots \\boldsymbol x_n$ be the training inputs.\n",
    "$\\textcolor{orange}{KL( \\textbf{\\textsf{\\textup{p}}}_{\\phi} (\\boldsymbol{h}|\\boldsymbol x)|| \\textbf{\\textsf{\\textup{p}}}(\\boldsymbol h))} =\\frac 1n\\left [ -\\frac 12 \\displaystyle\\sum_{i=1}^d \\left ( 1+log\\sigma^f_i(\\boldsymbol x)-(\\mu^f_i(\\boldsymbol x))^2-\\sigma^f_i(x)\\right )\\right ]$\n",
    ":::\n",
    "\n",
    "::: frame\n",
    "Reparameterization trick\n",
    "\n",
    "::: columns\n",
    "::: Lblock\n",
    "Expressing $\\boldsymbol h$ as some differentiable and invertible\n",
    "transformation of another random variable $\\epsilon$ given\n",
    "$\\boldsymbol x$ and $\\phi$.\n",
    "\n",
    "$$\\boldsymbol h = \\mu(\\boldsymbol x,\\phi) + \\sigma(\\boldsymbol x,\\phi)\\odot \\epsilon,\\quad \\epsilon\\sim \\mathcal{N}(0,I)$$\n",
    ":::\n",
    ":::\n",
    "\n",
    "Gradient can flow out of any random variable $\\Rightarrow$\n",
    "backpropagation is possible.\n",
    ":::\n",
    "\n",
    "::: frame\n",
    "Summary ![image](../images/VAE2){height=\"80pt\"}\n",
    "\n",
    "::: Lblock\n",
    "1.  Define Encoder and Decoder\n",
    "\n",
    "2.  Define the latent space distribution using the reparametrization\n",
    "    trick\n",
    "\n",
    "3.  Define the ELBO loss\n",
    "\n",
    "4.  Train and play with VAE !\n",
    ":::\n",
    ":::\n",
    "\n",
    "# Latent space\n",
    "\n",
    "::: frame\n",
    "Exploration\n",
    "\n",
    "-   Changing one single variable $h_i$ and keep all other $h_j$ fixed.\n",
    "\n",
    "-   Dimensions of $\\boldsymbol h$ encode different interpretable latent\n",
    "    features.\n",
    "\n",
    "::: center\n",
    "![image](../images/mnistlatent){width=\".45\\\\textwidth\"}\n",
    "![image](../images/fashionmnistlatent){width=\".45\\\\textwidth\"}\n",
    ":::\n",
    ":::\n",
    "\n",
    "::: frame\n",
    "Disentanglement\n",
    "![image](../images/disentangle){width=\".7\\\\textwidth\"}\\\n",
    "Kingma et al, 2014\n",
    ":::\n",
    "\n",
    "::: frame\n",
    "Disentanglement\n",
    "![image](../images/disentangle2){width=\"\\\\textwidth\"} Amini and\n",
    "Soleimany, 2019\n",
    ":::\n",
    "\n",
    "::: frame\n",
    "Disentanglement $\\beta-VAE$: enforces disentanglement\n",
    "$$\\displaystyle\\min_{\\phi,\\theta} (-ELBO(\\boldsymbol x,\\phi)) = -\\mathbb{E}_{ \\textbf{\\textsf{\\textup{p}}}_{\\phi} (\\boldsymbol{h}|\\boldsymbol x)}  \\left [\\textcolor{bluelimos}{log(\\textbf{\\textsf{\\textup{p}}}_\\theta(\\boldsymbol{x}|\\boldsymbol h))}\\right ]+\\beta \\textcolor{orange}{KL( \\textbf{\\textsf{\\textup{p}}}_{\\phi} (\\boldsymbol{h}|\\boldsymbol x)|| \\textbf{\\textsf{\\textup{p}}}(\\boldsymbol h))}$$\n",
    "![image](../images/betavae){width=\".6\\\\textwidth\"} Higgins et al.,\n",
    "2017\n",
    ":::\n",
    "\n",
    "::: frame\n",
    "Exploration\n",
    "\n",
    "::: center\n",
    "![image](../images/interpol){width=\".7\\\\textwidth\"}\n",
    ":::\n",
    "\n",
    "::: columns\n",
    "\\\n",
    "Interpolation in the image space\n",
    "\n",
    "\\\n",
    "Interpolation in the latent space\n",
    ":::\n",
    ":::\n",
    "\n",
    "::: frame\n",
    "Sampling ![image](../images/vaesampling){width=\".8\\\\textwidth\"}\n",
    ":::\n",
    "\n",
    "# Implementation\n",
    "\n",
    "::: frame\n",
    "Encoder-Decoder myTrapezium/.pic = (0,0) -- (0,)̱ -- (,)̧ -- (,-)̧ -- (0,-)̱\n",
    "-- cycle ; (-center) at (/2,0); (-out) at (,0);\n",
    "\n",
    "= \\[thick, decoration=markings,mark=at position 1 with , double\n",
    "distance=1.4pt, shorten \\>= 5.5pt, preaction = decorate, postaction =\n",
    "draw,line width=1.4pt, white,shorten \\>= 4.5pt\\]\n",
    "\n",
    "::: center\n",
    ":::\n",
    "\n",
    "::: Lblock\n",
    "-   MLP\n",
    "\n",
    "-   CNN\n",
    "\n",
    "-   RNN\n",
    ":::\n",
    ":::\n",
    "\n",
    "::: frame\n",
    "Reparameterization trick myTrapezium/.pic = (0,0) -- (0,)̱ -- (,)̧ -- (,-)̧\n",
    "-- (0,-)̱ -- cycle ; (-center) at (/2,0); (-out) at (,0);\n",
    "\n",
    "= \\[thick, decoration=markings,mark=at position 1 with , double\n",
    "distance=1.4pt, shorten \\>= 5.5pt, preaction = decorate, postaction =\n",
    "draw,line width=1.4pt, white,shorten \\>= 4.5pt\\]\n",
    "\n",
    "::: center\n",
    ":::\n",
    "\n",
    "$$\\boldsymbol h = \\mu(\\boldsymbol x,\\phi) + \\sigma(\\boldsymbol x,\\phi)\\odot \\epsilon,\\quad \\epsilon\\sim \\mathcal{N}(0,I)$$\n",
    "\n",
    "::: center\n",
    "![image](../images/trick){width=\"\\\\textwidth\"}\n",
    ":::\n",
    ":::\n",
    "\n",
    "::: frame\n",
    "KL divergence\n",
    "$$\\textcolor{orange}{KL( \\textbf{\\textsf{\\textup{p}}}_{\\phi} (\\boldsymbol{h}|\\boldsymbol x)|| \\textbf{\\textsf{\\textup{p}}}(\\boldsymbol h))} =\\frac 1n\\left [ -\\frac 12 \\displaystyle\\sum_{i=1}^d \\left ( 1+log\\sigma^f_i(\\boldsymbol x)-(\\mu^f_i(\\boldsymbol x))^2-\\sigma^f_i(x)\\right )\\right ]$$\n",
    "\n",
    "::: center\n",
    "![image](../images/KL){width=\"\\\\textwidth\"}\n",
    ":::\n",
    ":::\n",
    "\n",
    "::: frame\n",
    "And now gather all the stuff\n",
    "\n",
    "::: center\n",
    "![image](../images/allvae){width=\"\\\\textwidth\"}\n",
    ":::\n",
    ":::"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "md:myst",
   "text_representation": {
    "extension": ".md",
    "format_name": "myst"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "source_map": [
   11
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}