{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93a83e49",
   "metadata": {},
   "source": [
    "# Graph Neural Networks\n",
    "\n",
    "Comme leur nom l'indique, les Graph Neural Networks sont des réseaux de\n",
    "neurones qui traitent les graphes. Ce traitement pose trois\n",
    "problématiques :\n",
    "\n",
    "-   leur topologie est variable, et il est difficile de concevoir des\n",
    "    réseaux qui soient à la fois suffisamment expressifs et capables de\n",
    "    gérer cette variation.\n",
    "\n",
    "-   ils peuvent être de taille conséquente : un graphe représentant les\n",
    "    connexions entre les utilisateurs d'un réseau social peut avoir\n",
    "    plusieurs millions de sommets.\n",
    "\n",
    "-   il se peut qu'il n'y ait à disposition pour le problème à traiter\n",
    "    qu'un seul graphe, de sorte que le protocole habituel d'entraînement\n",
    "    avec de nombreux exemples de données et de test avec de nouvelles\n",
    "    données n'est pas toujours possible.\n",
    "\n",
    "## Représentation d'un graphe\n",
    "\n",
    "### Définition matricielle d'un graphe\n",
    "\n",
    "Soit $G=(V,E)$ un graphe à $N=|V|$ sommets et $M=|E|$ arcs (ou arêtes).\n",
    "Chaque sommet et chaque arc (ou arête) peut porter une information\n",
    "vectorielle (graphe pondéré). On choisit alors de représenter $G$ par\n",
    "trois matrices $\\mathbf A,\\mathbf X$ et $\\mathbf E$ représentant\n",
    "respectivement la struture de $G$, les sommets et arcs (ou arêtes):\n",
    "\n",
    "1.  $\\mathbf A$ est la matrice d'adjacence (pour les arêtes) ou\n",
    "    d'incidence (pour les arcs).\n",
    "\n",
    "2.  $\\mathbf X$ est une matrice de taille $d\\times N$, la $i$-eme\n",
    "    colonne de $\\mathbf X$ donnant les $d$ informations portées par le\n",
    "    sommet $i\\in[\\![1,N]\\!]$.\n",
    "\n",
    "3.  $\\mathbf E$ est une matrice de taille $d_e\\times M$, la $i$-eme\n",
    "    colonne de $\\mathbf E$ donnant les $d_e$ informations portées par\n",
    "    l'arc (ou l'arête) $i\\in[\\![1,N]\\!]$.\n",
    "\n",
    "Dans un premier temps, on considérera uniquement le cas où $\\mathbf X$\n",
    "existe (seuls les sommets sont pondérés). On reviendra sur le cas de\n",
    "$\\mathbf E$ plus tard.\n",
    "\n",
    "### Propriétés\n",
    "\n",
    "La matrice d'adjacence peut être utilisée pour trouver les voisins d'un\n",
    "sommet. Supposons que le $i$-eme sommet soit encodé sous la forme d'un\n",
    "vecteur colonne $\\mathbf X_{\\bullet,i}$ avec une seule entrée non nulle\n",
    "à la position $i$, fixée à un. En prémultipliant $\\mathbf X_{\\bullet,i}$\n",
    "par la matrice $\\mathbf A$, on calcule un vecteur avec des uns aux\n",
    "positions des voisins. En répétant cette procédure $n$ fois, on accède\n",
    "aux voisins du sommet $i$ accessibles en $n$ étapes. Ainsi, le\n",
    "coefficient (l,c) de $\\mathbf A^n$ contient le nombre de chaînes uniques\n",
    "de longueur $n$ du sommet $l$ au sommet $c$. A noter que ce n'est pas le\n",
    "nombre de chemins uniques puisqu'il inclut les chemins qui visitent le\n",
    "même sommet plus d'une fois. Les sommets étant numérotés arbitrairement\n",
    "dans $G$, il est de plus possible de changer l'indexation de ces\n",
    "sommets, sans changer la struture de $G$, à l'aide d'une matrice de\n",
    "permutation $\\mathbf P$. Les informations portées par les sommets sont\n",
    "alors résumées dans la matrice $\\mathbf X\\mathbf P$, et la nouvelle\n",
    "matrice d'adjacence est donnée par $\\mathbf P^T\\mathbf A \\mathbf P$.\n",
    "\n",
    "## Introduction aux GNN\n",
    "\n",
    "Un GNN est un modèle qui prend les représentations des sommets\n",
    "$\\mathbf X$ et la matrice d'adjacence $\\mathbf A$ comme entrées et les\n",
    "fait passer par une série de $K$ couches. Les représentations des\n",
    "sommets sont mises à jour à chaque couche pour créer des représentations\n",
    "intermédiaires cachées $\\mathbf H_k$ avant de calculer les\n",
    "représentations de sortie $\\mathbf H_K$, dont les colonnes comprennent\n",
    "des informations sur les sommets correspondants et leur contexte dans le\n",
    "$G$.\n",
    "\n",
    "Les GNN peuvent être utilisés en exploitant la structure de $G$, les\n",
    "sommets et/ou les arcs ou arêtes.\n",
    "\n",
    "### Utilisations\n",
    "\n",
    "#### En relation avec la structure de graphe\n",
    "\n",
    "Le réseau attribue une étiquette ou estime une ou plusieurs valeurs à\n",
    "partir de l'ensemble du graphe, en exploitant à la fois la structure et\n",
    "la représentation des sommets. Par exemple, on peut vouloir prédire la\n",
    "température à laquelle un gaz (molécules représentées sous la forme d'un\n",
    "graphe) devient liquide (régression) ou si une molécule est toxique ou\n",
    "non (classification). Les représentations des sommets de sortie sont\n",
    "combinées (par exemple, en calculant la moyenne) et le vecteur résultant\n",
    "est mis en correspondance avec un vecteur de taille fixe par le biais\n",
    "d'une transformation linéaire ou d'un réseau de neurones. Pour la\n",
    "régression, le décalage entre le résultat et les valeurs de vérité\n",
    "terrain est calculé à l'aide de la fonction de perte moindres carrés.\n",
    "Pour la classification binaire, la sortie passe par une fonction\n",
    "sigmoïde et la perte est calculée à l'aide de l'entropie croisée binaire\n",
    "$$P(y=1\\mid \\mathbf X,\\mathbf A) = \\sigma(\\beta_K+\\mathbf w_K\\mathbf H_K\\mathbf 1/N)$$\n",
    "où $\\beta_K$ et $\\mathbf w_K^T\\in\\mathbb{R}^D$ sont des paramètres à\n",
    "apprendre. Multiplier à droite par le vecteur colonne $\\mathbf 1$ somme\n",
    "toutes les représentations, et diviser par $N$ calcule la moyenne. La\n",
    "technique résultante est dite *mean pooling*.\n",
    "\n",
    "#### En relation avec les sommets\n",
    "\n",
    "Le réseau attribue une étiquette (classification) ou une ou plusieurs\n",
    "valeurs (régression) à chaque sommet du graphe, en utilisant à la fois\n",
    "la structure du graphe et les représentations des sommets. Par exemple,\n",
    "dans un graphe construit à partir d'un nuage de points 3D d'un avion,\n",
    "l'objectif peut être de classer les sommets selon qu'ils appartiennent\n",
    "aux ailes ou au fuselage (classification). En régression, on peut par\n",
    "exemple vouloir prédire le nombre de messages qu'un abonné d'un réseau\n",
    "social recevra. Les fonctions de perte sont définies de la même manière\n",
    "que pour les tâches au niveau du graphe, sauf que cette opération est\n",
    "effectuée indépendamment pour chaque sommet i :\n",
    "\n",
    "$$P(y^{(i)}=1\\mid \\mathbf X,\\mathbf A) = \\sigma(\\beta_K+\\mathbf w_K\\mathbf h^{(i)}_K)$$\n",
    "\n",
    "#### En relation avec les arcs ou arêtes\n",
    "\n",
    "Le réseau prédit s'il doit y avoir ou non un arc (ou arête) entre les\n",
    "sommets $i$ et $j$. Par exemple, dans le cadre d'un réseau social, le\n",
    "réseau peut prédire si deux personnes se connaissent et s'apprécient et\n",
    "suggérer qu'elles se connectent si c'est le cas. C'est une tâche de\n",
    "classification binaire pour laquelle la représentation des deux sommets\n",
    "doit être convertie en un nombre unique représentant la probabilité de\n",
    "présence de l'arc (ou arête). L'une des possibilités consiste à prendre\n",
    "le produit scalaire des représentations des sommets et à faire passer le\n",
    "résultat par une fonction sigmoïde pour calculer la probabilité.\n",
    "\n",
    "### GCN\n",
    "\n",
    "Il existe de nombreux types de GNN, on se restreint ici aux réseaux\n",
    "convolutifs de graphes, ou GCN. Ces modèles sont convolutifs en ce sens\n",
    "qu'ils mettent à jour chaque sommet en agrégeant les informations\n",
    "provenant des sommets voisins. En tant que tels, ils induisent un biais\n",
    "inductif relationnel (une tendance à donner la priorité aux informations\n",
    "provenant des voisins). On suppose de plus que les convolutions\n",
    "s'opèrent dans le domaine spatial (utilisant la structure de $G$),\n",
    "plutôt que dans l'espace de Fourier (méthodes basées spectre). Chaque\n",
    "couche du GCN est une fonction $\\mathbf F_{\\boldsymbol \\phi}$, de paramètres\n",
    "${\\boldsymbol \\phi}$, qui prend en entrée les représentations des sommets\n",
    "$\\mathbf X$ et la matrice d'adjacence $\\mathbf A$ de $G$ et produit de\n",
    "nouvelles représentations des sommets : $$\\begin{aligned}\n",
    "\\mathbf H_1 &=& \\mathbf F_{\\boldsymbol \\phi_O}(\\mathbf X,\\mathbf A)\\\\ \n",
    "\\mathbf H_2 &=& \\mathbf F_{\\boldsymbol \\phi_1}(\\mathbf H_1,\\mathbf A)\\\\ \n",
    "\\cdots &&\\\\ \n",
    "\\mathbf H_K &=& \\mathbf F_{\\boldsymbol \\phi_{K-1}}(\\mathbf H_{K-1},\\mathbf A)\n",
    "\\end{aligned}$$ les $\\boldsymbol \\phi_{k}$ étant les paramètres du réseau entre\n",
    "la couche $k$ et la couche $k+1$\n",
    "\n",
    "#### Equivariance et invariance\n",
    "\n",
    "L'indexation des sommets dans le graphe étant arbitraire, il est\n",
    "indispensable que tout modèle respecte cette propriété. Chaque couche\n",
    "doit donc être équivariante[^1] par rapport aux permutations des indices\n",
    "des sommets, soit pour toute permutation $\\mathbf P$ et tout $k$:\n",
    "$$\\mathbf{H_{k+1}}\\mathbf P = F_{\\boldsymbol \\phi_{k}}(\\mathbf H_k\\mathbf P, \\mathbf P^T\\mathbf A\\mathbf P)$$\n",
    "\n",
    "Pour les tâches de classification des sommets et de prédiction des arcs\n",
    "ou arêtes, les résultats doivent également être équivariants pour les\n",
    "permutations des indices des sommets. Toutefois, pour les tâches en\n",
    "relation avec le graphe, la couche finale agrège les informations\n",
    "provenant de l'ensemble du graphe, de sorte que le résultat est\n",
    "invariant par rapport à l'ordre des sommets.\n",
    "\n",
    "#### Partage des paramètres\n",
    "\n",
    "Dans les réseaux convolutifs, des couches convolutives sont utilisées,\n",
    "qui traitent chaque position de l'image de manière identique. Cela\n",
    "permet de réduire le nombre de paramètres et d'introduire un biais\n",
    "inductif qui force le modèle à traiter chaque partie de l'image de la\n",
    "même manière. Le même argument peut être avancé pour les sommets d'un\n",
    "graphe. On pourrait apprendre un modèle avec des paramètres distincts\n",
    "associés à chaque sommet. Cependant, le réseau doit maintenant apprendre\n",
    "indépendamment la signification des connexions dans le graphe à chaque\n",
    "position, et l'apprentissage nécessiterait de nombreux graphes ayant la\n",
    "même topologie. Il est plus judicieux de construire un modèle qui\n",
    "utilise les mêmes paramètres à chaque sommet, réduisant ainsi le nombre\n",
    "de paramètres et partageant ce que le réseau apprend à chaque sommet sur\n",
    "l'ensemble du graphe.\n",
    "\n",
    "On peut modéliser une convolution (qui met à jour une variable en\n",
    "prenant une somme pondérée des informations provenant de ses voisins)\n",
    "comme le fait que chaque voisin envoie un message à la variable\n",
    "d'intérêt, qui agrège ces messages pour former la mise à jour. Dans le\n",
    "cas des images, les voisins sont les pixels d'une région carrée de\n",
    "taille fixe autour de la position actuelle, de sorte que les relations\n",
    "spatiales à chaque position sont les mêmes. Dans un graphe, chaque\n",
    "sommet peut avoir un nombre différent de voisins et il n'existe a priori\n",
    "pas de relation privilégiée entre sommets : il n'y a aucune raison de\n",
    "pondérer favorablement (ou pas) les informations provenant d'un sommet\n",
    "particulier.\n",
    "\n",
    "#### Exemple\n",
    "\n",
    "La ({numref}`gcn`) présente un exemple simple de GCN.\n",
    "\n",
    "\n",
    "\n",
    "```{figure} ./images/gcn.png\n",
    ":name: gcn\n",
    "Exemple de GCN\n",
    "```\n",
    "\n",
    "-   à gauche le graphe $G$ initial, les colonnes de $\\mathbf X$ étant\n",
    "    reportées à côté des sommets correspondants.\n",
    "\n",
    "-   au milieu, chaque sommet de la première couche cachée est mis à jour\n",
    "    en :\n",
    "\n",
    "    1.  agrégeant les sommets voisins d'un sommet $i$ en un unique\n",
    "        vecteur :\n",
    "        $\\mathbf f_1(i) = \\displaystyle\\sum_{j\\textrm{ voisin de }i} \\mathbf h_1(j)$\n",
    "\n",
    "    2.  appliquant pour tout $i$ une transformation linéaire $\\mathbf L$\n",
    "        au sommet initial $\\mathbf x^{i}$ et aux sommets agrégés et en\n",
    "        ajoutant un biais $\\boldsymbol\\beta_0$ :\n",
    "        $\\boldsymbol \\beta_0 + \\mathbf L\\mathbf x^{i} +  \\mathbf L \\mathbf f_1(i)$\n",
    "\n",
    "    3.  appliquant une fonction non linéaire $g$ au résultat précédent :\n",
    "        $\\mathbf h_1^{i} =g(\\boldsymbol \\beta_0 + \\mathbf L \\mathbf x^{i} + \\mathbf L f_1(i))$\n",
    "\n",
    "-   à droite, le processus répété pour toute couche $k$ :\n",
    "\n",
    "    $$(\\forall i)\\; \\; \\mathbf h_{k+1}^{i} = g \\left (\\boldsymbol \\beta_k+\\mathbf L_k \\mathbf h_k^{i} + \\mathbf L_k\\left (\\displaystyle\\sum_{j\\textrm{ voisin de }i} \\mathbf h_k(j)\\right ) \\right )$$\n",
    "\n",
    "On peut écrire ce processus de manière matricielle : Si\n",
    "$\\mathbf H_k\\in\\mathcal{M}_{D,N}(\\mathbb{R})$ est la matrice dont les\n",
    "colonnes sont les représentations des sommets, alors\n",
    "\n",
    "$$\\mathbf H_{k+1} = g\\left (\\boldsymbol\\beta_k \\mathbf 1^T + \\mathbf L_k\\mathbf H_k+\\mathbf L_k\\mathbf H_k \\mathbf A\\right ) = g\\left (\\boldsymbol\\beta_k \\mathbf 1^T + \\mathbf L_k\\mathbf H_k(\\mathbf A+\\mathbf I)\\right )$$\n",
    "\n",
    "où $g$ est appliquée point à point sur les éléments de la matrice\n",
    "argument. On remarque que la couche $k+1$ est bien équivariante à la\n",
    "permutation de la numérotation des sommets, utilise la structure du\n",
    "graphe ($\\mathbf A$) pour produire un biais inductif et partage les\n",
    "paramètres sur tout le graphe.\n",
    "\n",
    "### Application en classification \n",
    "\n",
    "Pour l'exemple, on s'intéresse à un problème de classification binaire.\n",
    "On modélise une molécule comme un graphe, sont les sommets sont les\n",
    "atomes. La matrice $\\mathbf A$ donne les liaisons entre les atomes, et\n",
    "la matrice $\\mathbf X$ donne le nom de l'atome : si la table périodique\n",
    "des éléments comporte $D$ atomes, le sommet (l'atome) $i$ est un vecteur\n",
    "de $\\{0,1\\}^D$, où la seule composante qui vaille 1 est celle qui\n",
    "identifie le type de l'atome. On s'intéresse alors de savoir si une\n",
    "molécule donnée est toxique ($y=1$) ou pas ($y=0$).\n",
    "\n",
    "Les équations du réseau sont alors :\n",
    "\n",
    "$$\\forall k\\in[\\![0,K-1]\\!]\\; \\mathbf H_{k+1} = g\\left (\\boldsymbol\\beta_k \\mathbf 1^T + \\mathbf L_k\\mathbf H_k(\\mathbf A+\\mathbf I)\\right )$$\n",
    "\n",
    "et\n",
    "\n",
    "$$f(\\mathbf X,\\mathbf A,\\boldsymbol\\phi) = P(y=1\\mid \\mathbf X,\\mathbf A) = \\sigma(\\boldsymbol \\beta_K+\\mathbf w_K\\mathbf H_K\\mathbf 1/N)$$\n",
    "\n",
    "où $\\boldsymbol \\phi=(\\boldsymbol \\beta_k,\\mathbf L_k)_{k\\in[\\![0,K]\\!]}$ sont les\n",
    "paramètres du réseau à apprendre et $\\sigma$ la fonction sigmoïde.\n",
    "\n",
    "Étant donnés $n$ exemples d'entraînement\n",
    "$(\\mathbf X_i,\\mathbf A_i,y_i)_{i\\in[\\![1,n]\\!]}$, $\\boldsymbol \\phi$ peut être\n",
    "classiquement appris par minimisation de l'entropie croisée binaire sur\n",
    "des batchs d'exemples. Si dans les MLP et les CNN, les entrées sont de\n",
    "taille identique (et donc les exemples sont concaténés en un tenseur de\n",
    "dimension supérieure pour un entraînement efficace par GPU ou TPU), les\n",
    "graphes de la base d'entraînement ont très probablement un nombre de\n",
    "sommets $N$ et une dimension de l'espace de représentation $D$\n",
    "différents, ce qui rend cette concaténation impossible. Une astuce\n",
    "simple permet cependant de traiter l'ensemble du batch en parallèle. Les\n",
    "graphes du batch sont traités comme des composantes disjointes d'un seul\n",
    "grand graphe. Le réseau peut alors être exécuté comme une instance\n",
    "unique des équations de réseau. La mise en commun des moyennes est\n",
    "effectuée uniquement sur les graphes individuels afin d'obtenir une\n",
    "représentation unique par graphe qui peut être introduite dans la\n",
    "fonction de perte.\n",
    "\n",
    "### Modèles inductifs et transductifs\n",
    "\n",
    "Jusqu'à présent, tous les modèles présentés dans ce cours ont été\n",
    "inductifs : on exploite un ensemble de données étiquetées pour apprendre\n",
    "la relation entre les entrées et les sorties. On l'applique ensuite à de\n",
    "nouvelles données de test. En d'autres termes, on apprend la règle qui\n",
    "associe les entrées aux sorties, puis on l'applique ailleurs.\n",
    "\n",
    "En revanche, un modèle transductif (ou apprentissage semi-supervisé)\n",
    "prend en compte les données étiquetées et non étiquetées en même temps.\n",
    "Il ne produit pas de règle, mais simplement une étiquette pour les\n",
    "sorties inconnues. Il présente l'avantage de pouvoir utiliser des\n",
    "modèles sur des données non étiquetées pour prendre ses décisions, mais\n",
    "nécessite un nouvel entraînement du modèle lorsque des données non\n",
    "étiquetées supplémentaires sont ajoutées.\n",
    "\n",
    "Les deux types de problèmes sont couramment rencontrés pour les graphes.\n",
    "Parfois, on dispose de nombreux graphes étiquetés et on apprend une\n",
    "correspondance entre le graphe et les étiquettes. D'autres fois, il\n",
    "arrive qu'il n'y ait à disposition qu'un seul graphe de très grande\n",
    "dimension et dans ce cas, les données d'apprentissage et de test sont\n",
    "nécessairement connectées.\n",
    "\n",
    "Les utilisations des GNN en relation avec la structure des graphes ne se\n",
    "produisent que dans le cadre inductif où il existe des graphes\n",
    "d'apprentissage et de test. Toutefois, les utilisations en relation avec\n",
    "les sommets et les tâches de prédiction des arcs ou arêtes peuvent se\n",
    "produire dans l'un ou l'autre cadre. Dans le cas transductif, la\n",
    "fonction de perte minimise le décalage entre la sortie du modèle et la\n",
    "vérité lorsqu'elle est connue. Les nouvelles prédictions sont calculées\n",
    "en exécutant la passe avant et en récupérant les résultats lorsque la\n",
    "vérité est inconnue.\n",
    "\n",
    "### Exemple en classification de sommets\n",
    "\n",
    "On s'intéresse ici à un problème de classification binaire des sommet\n",
    "dans un cadre transductif. Le graphe considéré comporte des millions de\n",
    "sommets, certains ayant des étiquettes binaires $y_i$. L'objectif est\n",
    "alors d'étiqueter les sommets non étiquetés restants. Le réseau est le\n",
    "même que dans l'exemple [1.2.3](#S:excl){reference-type=\"ref\"\n",
    "reference=\"S:excl\"} avec une couche finale différente qui produit un\n",
    "vecteur de sortie de taille $1\\times N$ :\n",
    "$$f(\\mathbf X,\\mathbf A,\\boldsymbol\\phi) =  \\sigma(\\boldsymbol \\beta_K\\mathbf 1^T+\\mathbf w_K\\mathbf H_K)$$\n",
    "la fonction $\\sigma$ agissant point à point. On trouve $\\boldsymbol \\phi$ par\n",
    "minimisation de l'entropie croisée binaire, mais seulement à partir des\n",
    "valeurs des sommets pour lesquels les étiquettes $y_i$ sont connues.\n",
    "\n",
    "L'entraînement de ce réseau pose deux problèmes. Tout d'abord, il est\n",
    "difficile d'entraîner un réseau de cette taille, ne serait-ce que parce\n",
    "qu'il faut stocker les représentations des sommets à chaque couche du\n",
    "réseau dans la passe avant. Cela implique à la fois le stockage et le\n",
    "traitement d'une structure plusieurs fois plus grande que le graphe\n",
    "entier. De plus, n'ayant qu'un seul graphe à disposition, la descente de\n",
    "gradient (ou tout autre algorithme d'optimisation) sur batch est\n",
    "impossible, puisq'un seul objet constitue la base d'entraînement.\n",
    "\n",
    "Pour répondre à ce second problème, on choisit un sous-ensemble\n",
    "aléatoire de sommets étiquetés à chaque étape de l'entraînement. Chaque\n",
    "sommet dépend de ses voisins dans la couche précédente. Ces derniers\n",
    "dépendent à leur tour de leurs voisins de la couche précédente, de sorte\n",
    "que chaque sommet possède l'équivalent d'un champ réceptif comme dans\n",
    "les CNN. La taille du champ réceptif est appelée voisinage à $k$ sauts.\n",
    "On peut donc effectuer une étape de descente de gradient en utilisant le\n",
    "graphe qui forme l'union des voisinages de $k$-sauts des sommets du\n",
    "batch. Les entrées restantes ne contribuent pas. S'il y a de nombreuses\n",
    "couches et que le graphe est fortement connecté, chaque sommet d'entrée\n",
    "peut se trouver dans le champ réceptif de chaque sortie, ce qui ne\n",
    "réduit pas du tout la taille du graphe : c'est le problème de\n",
    "l'expansion du graphe. Deux approches s'attaquent à ce problème :\n",
    "l'échantillonnage du voisinage et le partitionnement du graphe.\n",
    "\n",
    "#### Échantillonnage du voisinage\n",
    "\n",
    "Le graphe complet est échantillonné, ce qui réduit les connexions à\n",
    "chaque couche du réseau. Par exemple, on peut commencer par les sommets\n",
    "du batch et échantillonner aléatoirement un nombre fixe de leurs voisins\n",
    "dans la couche précédente. Ensuite, on échantillonne au hasard un nombre\n",
    "fixe de leurs voisins dans la couche précédente, et ainsi de suite. La\n",
    "taille du graphe augmente toujours à chaque couche, mais de manière\n",
    "beaucoup plus contrôlée. Cette opération est renouvelée pour chaque\n",
    "batch, de sorte que les voisins contributeurs diffèrent même si le même\n",
    "batch est tiré deux fois. Cette techique rappelle celle du dropout et\n",
    "ajoute une certaine régularisation.\n",
    "\n",
    "#### Partitionnement du graphe\n",
    "\n",
    "On peut également partitionner le graphe original en sous-ensembles de\n",
    "sommet disjoints, et construire des graphes plus petits qui ne sont pas\n",
    "connectés les uns aux autres avant le traitement. Il existe des\n",
    "algorithmes standards pour choisir ces sous-ensembles afin de maximiser\n",
    "le nombre de liens internes. Ces petits graphes peuvent chacun être\n",
    "traités comme des batchs, ou un sous-ensemble aléatoire d'entre eux peut\n",
    "être combiné pour former un batch (en rétablissant toutes les arêtes\n",
    "entre eux à partir du graphe d'origine). Utilisant l'une de ces deux\n",
    "approches, il est alors possible d'entraîner les paramètres du réseau de\n",
    "la même manière que pour le cadre inductif, en divisant les sommets\n",
    "étiquetés en ensembles d'entrainement, de test et de validation comme\n",
    "souhaité. Pour effectuer l'inférence, on calcule les prédictions pour\n",
    "les sommets inconnus sur la base de leur voisinage de $k$-sauts.\n",
    "Contrairement à l'entraînement, il n'est pas nécessaire de stocker les\n",
    "représentations intermédiaires, ce qui rend l'utilisation de la mémoire\n",
    "beaucoup plus efficiente.\n",
    "\n",
    "### Couches d'un GNN\n",
    "\n",
    "les sections précédentes combinaient les sommets adjacents par\n",
    "sommation, en multipliant $\\mathbf H$ par $\\mathbf A+\\mathbf I$. Dans la\n",
    "suite de ce paragraphe, on présente des alternatives à cette approche.\n",
    "\n",
    "#### Amélioration de la diagonale\n",
    "\n",
    "La mise à jour proposée jusqu'à lors\n",
    "$$\\forall k\\in[\\![0,K-1]\\!]\\; \\mathbf H_{k+1} = g\\left (\\boldsymbol\\beta_k \\mathbf 1^T + \\mathbf L_k\\mathbf H_k(\\mathbf A+\\mathbf I)\\right )$$\n",
    "\n",
    "peut être modifiée en\n",
    "\n",
    "$$\\forall k\\in[\\![0,K-1]\\!]\\; \\mathbf H_{k+1} = g\\left (\\boldsymbol\\beta_k \\mathbf 1^T + \\mathbf L_k\\mathbf H_k(\\mathbf A+(1+\\epsilon_k)\\mathbf I)\\right )$$\n",
    "où $\\epsilon_k$ est appris, ou en\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\forall k\\in[\\![0,K-1]\\!]\\; \\mathbf H_{k+1} &=& g\\left (\\boldsymbol\\beta_k \\mathbf 1^T + \\mathbf L_k\\mathbf H_k\\mathbf A+\\boldsymbol \\psi_k \\mathbf H_k\\right )\\\\\n",
    "&=&g\\left (\\boldsymbol\\beta_k \\mathbf 1^T + \\begin{pmatrix}\\mathbf L_k &\\boldsymbol \\psi_k\\end{pmatrix} \\begin{pmatrix}\\mathbf H_k\\mathbf A\\\\ \\mathbf H_k\\end{pmatrix}\\right )\\\\ \n",
    "&=&g\\left (\\boldsymbol\\beta_k \\mathbf 1^T + \\mathbf L'_k \\begin{pmatrix}\\mathbf H_k\\mathbf A\\\\ \\mathbf H_k\\end{pmatrix}\\right )\n",
    "\\end{aligned}$$ \n",
    "\n",
    "où\n",
    "$\\mathbf L'_k=\\begin{pmatrix}\\mathbf L_k &\\boldsymbol \\psi_k\\end{pmatrix}$\n",
    "permet d'appliquer une transformation linéaire différente au sommet\n",
    "courant.\n",
    "\n",
    "#### Connexions résiduelles\n",
    "\n",
    "Avec les connexions résiduelles, la représentation agrégée des voisins\n",
    "est transformée et passe par la fonction d'activation avant d'être\n",
    "additionnée ou concaténée avec le sommet actuel :\n",
    "\n",
    "$$\\mathbf H_{k+1} = \\begin{pmatrix}g\\left (\\boldsymbol\\beta_k \\mathbf 1^T + \\mathbf L_k\\mathbf H_k\\mathbf A\\right ) \\\\ \\mathbf H_{k}   \\end{pmatrix}$$\n",
    "\n",
    "#### Agrégation moyenne\n",
    "\n",
    "Les méthodes précédentes regroupent les voisins en additionnant les\n",
    "représentation des sommets. Cependant, il est possible de combiner\n",
    "différemment ces représentations. Parfois, il est préférable de prendre\n",
    "la moyenne des voisins plutôt que la somme. Cette méthode peut s'avérer\n",
    "plus performante si les informations de représentation sont plus\n",
    "importantes et les informations structurelles moins, car la part de\n",
    "contribution du voisinage ne dépend pas du nombre de voisins :\n",
    "\n",
    "$$\\mathbf f(i) = \\frac{1}{|\\mathcal{V}_i|}\\displaystyle\\sum_{j\\in\\mathcal{V}_i} \\mathbf h(j)$$\n",
    "où $\\mathcal{V}_i$ désigne l'ensemble des voisins du sommet $i$. En\n",
    "notation matricielle, si $\\mathbf D$ est la matrice diagonale des degrés\n",
    "alors\n",
    "\n",
    "$$\\forall k\\in[\\![0,K-1]\\!]\\; \\mathbf H_{k+1} = g\\left (\\boldsymbol\\beta_k \\mathbf 1^T + \\mathbf L_k\\mathbf H_k(\\mathbf A\\mathbf D^{-1} + I) \\right )$$\n",
    "\n",
    "#### Normalisation de Kipf\n",
    "\n",
    "Ici\n",
    "\n",
    "$$\\mathbf f(i) = \\displaystyle\\sum_{j\\in\\mathcal{V}_i} \\frac{\\mathbf h(j)}{\\sqrt{|\\mathcal{V}_i||\\mathcal{V}_j|}}$$\n",
    "\n",
    "l'information provenant des sommets ayant un grand nombre de voisins\n",
    "devant être revue à la baisse (il existe un grand nombre d'arcs qui\n",
    "fournissent moins d'information unique). En notation matricielle, cette\n",
    "normalisation s'écrit\n",
    "\n",
    "$$\\forall k\\in[\\![0,K-1]\\!]\\; \\mathbf H_{k+1} = g\\left (\\boldsymbol\\beta_k \\mathbf 1^T + \\mathbf L_k\\mathbf H_k(\\mathbf D^{-\\frac12}\\mathbf A\\mathbf D^{-\\frac12} + I) \\right )$$\n",
    "\n",
    "#### Agrégation par max pooling\n",
    "\n",
    "Comme dans le cas des CNN, on peut envisager d'agréger par le max, qui\n",
    "s'effectue alors composante par composante.\n",
    "\n",
    "$$\\mathbf f(i) = \\displaystyle\\max_{j\\in\\mathcal{V}_i} \\mathbf h(j)$$\n",
    "\n",
    "#### Agrégation par attention\n",
    "\n",
    "Les méthodes d'agrégation examinées jusqu'à présent pondèrent la\n",
    "contribution des voisins de manière égale ou d'une manière qui dépend de\n",
    "la topologie du graphe. Inversement, dans les couches d'attention de\n",
    "graphe, les poids dépendent des données aux sommets. Une transformation\n",
    "linéaire est appliquée aux représentations des sommets\n",
    "\n",
    "$$\\forall k\\in[\\![0,K-1]\\!]\\; \\mathbf H'_{k} = \\boldsymbol\\beta_k \\mathbf 1^T + \\mathbf L_k\\mathbf H_k$$\n",
    "\n",
    "La similarité $s_{ij}$ entre les représentations transformées\n",
    "$\\mathbf h'_i, \\mathbf h'_j$ des sommets $i$ et $j$ est calculée en\n",
    "concaténant les paires, en effectuant un produit scalaire avec un\n",
    "vecteur colonne $\\boldsymbol \\phi_k$ de paramètres appris et en appliquant une\n",
    "fonction d'activation\n",
    "\n",
    "$$s_{ij} = g\\left (\\boldsymbol \\phi_k^T \\begin{pmatrix}\\mathbf h'_i\\\\ \\mathbf h'_j \\end{pmatrix}\\right )$$\n",
    "\n",
    "Les similarités sont stockées dans une matrice $\\mathbf S$. Comme pour\n",
    "les mécanismes d'attention, les poids doivent être positifs et de somme\n",
    "1, mais pour un sommet donné, seuls lui et ses voisins doivent\n",
    "contribuer. On effectue donc l'opération\n",
    "\n",
    "$$\\forall k\\in[\\![0,K-1]\\!]\\; \\mathbf H_{k+1} = g\\left (\\mathbf H'_{k}.\\textrm{Softmax}(\\mathbf S,\\mathbf A+\\mathbf I) \\right )$$\n",
    "\n",
    "la fonction $\\textrm{Softmax}(\\bullet,\\bullet)$ calcule les valeurs\n",
    "d'attention en appliquant l'opération softmax séparément à chaque\n",
    "colonne de son premier argument $\\mathbf S$, mais seulement après avoir\n",
    "fixé à $-\\infty$ les valeurs pour lesquelles le deuxième argument\n",
    "$\\mathbf A+\\mathbf I$ est égal à zéro, de sorte qu'elles ne contribuent\n",
    "pas. Cela garantit que l'attention accordée aux sommets non voisins est\n",
    "nulle.\n",
    "\n",
    "### Prise en compte de l'information des arcs ou arêtes {#S:E}\n",
    "\n",
    "Les paragraphes précédents ont abordé le traitement des représentrations\n",
    "des sommets. Ceux-ci évoluent au fur et à mesure qu'ils sont transmis\n",
    "dans le réseau, de sorte qu'à la fin ils représentent à la fois le\n",
    "sommet et son contexte dans le graphe. On considère maintenant le cas où\n",
    "l'information est associée aux arêtes du graphe.\n",
    "\n",
    "Il est facile d'adapter le mécanisme de représentation précédent pour\n",
    "traiter la représentation des arêtes à l'aide du graphe des arêtes (ou\n",
    "graphe adjoint). Il s'agit d'un graphe complémentaire, dans lequel\n",
    "chaque arête du graphe original devient un sommet, et chaque paire\n",
    "d'arêtes ayant un sommet commun dans le graphe original crée une arête\n",
    "dans le nouveau graphe. En général, un graphe peut être reconstruit à\n",
    "partir de son graphe d'arêtes, de sorte qu'il est possible de passer\n",
    "d'une représentation à l'autre.\n",
    "\n",
    "Une fois le graphe d'arêtes construit, on utilise les mêmes techniques,\n",
    "en agrégeant les informations de chaque nouveau sommet à partir de ses\n",
    "voisins et en les combinant avec la représentation actuelle. Lorsque les\n",
    "représentations des sommets et d'arêtes sont tous deux présents, on peut\n",
    "passer d'un graphe à l'autre. Il existe donc quatre mises à jour\n",
    "possibles (les sommets mettent à jour les sommets, les sommets mettent à\n",
    "jour les arêtes, les arêtes mettent à jour les sommets et les arêtes\n",
    "mettent à jour les arêtes), qui peuvent être alternées à volonté ou,\n",
    "moyennant des modifications mineures, les sommets peuvent être mis à\n",
    "jour simultanément à partir des sommets et des arêtes.\n",
    "\n",
    "## Partie pratique\n",
    "\n",
    "Pour bien comprendre la structure d'un GNN, on propose de ne pas\n",
    "utiliser de librairie dédiée (type\n",
    "[Spectral](https://graphneural.network/),\n",
    "[StellarGraph](https://stellargraph.readthedocs.io/en/stable/README.html)\n",
    "ou encore [GraphNets](https://github.com/deepmind/graph_nets)), mais\n",
    "plutôt de l'implémenter directement à partir de Tensorflow et Keras.\n",
    "\n",
    "\\# Classification de sommets par GNN\n",
    "\n",
    "On propose ici de construire et apprendre un GNN pour prédire le sujet\n",
    "d'un article à partir de son contenu et de ses citations. Nous\n",
    "utiliserons pour cela le \\[jeu de données\n",
    "Cora\\](https://relational.fit.cvut.cz/dataset/CORA)\n",
    "\n",
    "\\## Jeu de données\n",
    "\n",
    "Le jeu de données Cora comprend 2 708 articles scientifiques de machine\n",
    "learning étiquetés avec l'un des 7 thèmes suivants : Neural_Networks,\n",
    "Probabilistic_Methods, Genetic_Algorithms, Theory ,Case_Based,\n",
    "Reinforcement_Learning et Rule_Learning.\n",
    "\n",
    "Les articles sont reliés par une arc indiquant quel article cite quel\n",
    "autre. Il existe 5 429 citations dans la base.\n",
    "\n",
    "Chaque article possède un vecteur de mots binaire de taille 1433,\n",
    "indiquant la présence d'un mot correspondant.\n",
    "\n",
    "En pratique, le jeu de données comporte deux fichiers - 'cora.cites' qui\n",
    "gère les citations. Les deux colonnes donne l'article cité\n",
    "('cited_paper_id') et l'article qui cite ('citing_paper_id'). -\n",
    "'cora.content' qui gère le contenu des papiers, et qui contient 1435\n",
    "colonnes : 'paper_id', 'subject', et 1433 descripteurs binaires.\n",
    "\n",
    "[^1]: Une fonction $f$ est équivariante pour une transformation $t$ si\n",
    "    pour tout $x, f(t(x)) = t(f(x))$"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "md:myst",
   "text_representation": {
    "extension": ".md",
    "format_name": "myst"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "source_map": [
   11
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}