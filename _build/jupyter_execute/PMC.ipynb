{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49efe8a1",
   "metadata": {},
   "source": [
    "# Perceptrons multicouches \n",
    "\n",
    "````{prf:definition} Perceptron multicouches\n",
    "Un perceptron à $(L+1)$ couches ({numref}`mlp`) est un réseau constitué d'une\n",
    "rétine à $D$ neurones (auxquels on rajoute l'entrée $x_0$), $C$ neurones\n",
    "de sortie, et des neurones dits **cachés**, organisés dans $L$ couches\n",
    "cachées intermédiaires. De fait, un tel réseau comporte $(L+2)$ couches\n",
    "mais on compte rarement la rétine, puisque cette dernière n'effectue pas\n",
    "de calculs. Le $i^{\\text{e}}$ neurone dans la couche cachée $l$ calcule\n",
    "la sortie \n",
    "\n",
    "$$\\begin{aligned}\n",
    "    y_i^{(l)} &= f\\left(z_i^{(l)}\\right) \\quad\\text{ avec }\\quad z_i^{(l)} = \\sum _{k = 1} ^{m^{(l-1)}} w_{i,k}^{(l)} y_k^{(l-1)} + w_{i,0}^{(l)}\n",
    "\\end{aligned}$$ \n",
    "\n",
    "où $w_{i,k}^{(l)}$ est le poids de la connexion entre le\n",
    "$k^{\\text{e}}$ neurone de la couche $(l-1)$ et le $i^{\\text{e}}$ neurone\n",
    "de la couche $l$, et $w_{i,0}^{(l)}$ est le biais. De plus, $m^{(l)}$\n",
    "est le nombre de neurones de la couche $l$, de sorte que $D = m^{(0)}$\n",
    "et $C = m^{(L+1)}$. Enfin, $f$ est la fonction d'activation du neurone\n",
    "(supposée identique pour tous les neurones).\n",
    "````\n",
    "\n",
    "En introduisant dans chaque couche un neurone supplémentaire\n",
    "$y_0^{(l)} = 1$ pour gérer le biais, on a : \n",
    "\n",
    "$$\\begin{aligned}\n",
    "    {\\mathbf z_i^{(l)}} = \\displaystyle\\sum _{k = 0} ^{m^{(l-1)}} {\\mathbf w_{i,k}^{(l)}} {\\mathbf y_k^{(l-1)}}\\quad \\text{ où }\\quad {\\mathbf z^{(l)}} = {\\mathbf w^{(l)} y^{(l-1)}}\n",
    "\\end{aligned}$$ \n",
    "\n",
    "avec ${\\mathbf z^{(l)}}$, $\\mathbf{w^{(l)}}$ et $\\mathbf{y^{(l-1)}}$ les\n",
    "représentations vectorielle et matricielle des entrées $z_i^{(l)}$, des\n",
    "poids $w_{i,k}^{(l)}$ et des sorties $y_k^{(l-1)}$.\n",
    "\n",
    "\n",
    "```{figure} ./images/mlp.png\n",
    ":name: mlp\n",
    "Perceptron multicouches à $(L + 1)$ couches, $D$ entrées et $C$ sorties.\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Un tel réseau représente une fonction \n",
    "\n",
    "$$\\begin{aligned}\n",
    "    {\\mathbf y}(\\cdot,{\\mathbf w}) &:& \\mathbb{R}^D \\rightarrow \\mathbb{R}^C\\\\\n",
    "    {\\mathbf x} &\\mapsto& {\\mathbf y(x,w)}\n",
    "\\end{aligned}$$ \n",
    "\n",
    "où ${\\mathbf y(x,w)}$ est tel que ${\\mathbf y_i}({\\mathbf x},{\\mathbf w}) = {\\mathbf y_i^{(L+1)}}$ et ${\\mathbf w}$\n",
    "est la matrice de tous les poids du réseau.\n",
    "\n",
    "On parlera de **réseau profond (Deep network)** lorsque le nombre de\n",
    "couches cachées est \"suffisamment important\" (supérieur à 3 par exemple).\n",
    "\n",
    "## Fonctions d'activation \n",
    "\n",
    "Trois grandes classes de fonction d'activation $f$ sont généralement\n",
    "utilisées : les fonctions de seuils (comme dans le perceptron linéaire à\n",
    "seuil), les fonctions linéaires par morceau et les fonctions de type\n",
    "sigmoïde. Dans les deux premiers cas, de nombreux problèmes se\n",
    "présentent, notamment en raison de la non différentiabilité de ces\n",
    "fonctions (qui est nécessaire dans les algorithmes d'apprentissage du\n",
    "type descente de gradient), ou encore en raison de la faiblesse de leur\n",
    "pouvoir d'expression. Ainsi, il est préférable d'utiliser des fonctions\n",
    "de type sigmoïde, et par exemple la sigmoïde logistique est donnée par :\n",
    "\n",
    "$$\\begin{aligned}\n",
    "    \\sigma(z) = \\frac{1}{1 + \\exp(-z)}.\n",
    "\\end{aligned}$$\n",
    "\n",
    "La tangente hyperbolique $\\tanh(z)$, également utilisée pour ses bonnes\n",
    "propriétés de dérivabilité ($(\\tanh)'=1-\\tanh^2$), peut être vue comme\n",
    "une transformation linéaire de la sigmoïde dans l'intervalle $[-1,1]$.\n",
    "\n",
    "Ces réseaux peuvent être utilisés en régression (sortie à valeurs dans\n",
    "$\\mathbb{R}^C$) ou en classification. Dans ce dernier cas, la fonction\n",
    "d'activation softmax est utilisée à la sortie du réseau pour interpréter\n",
    "les sorties comme des valeurs de probabilité a posteriori. S'il s'agit\n",
    "de classer un exemple $x$ à la classe $c$, la probabilité conditionnelle\n",
    "$p(c|x)$ peut être calculée en utilisant la règle de Bayes :\n",
    "\n",
    "$$\\begin{aligned}\n",
    "p(c|x) = \\frac{p(x|c)p(c)}{p(x)}\n",
    "\\end{aligned}$$ \n",
    "\n",
    "$p(c|x)$ est alors interprétée comme une probabilité a\n",
    "posteriori. Disposant de ces probabilités pour tout $c=1,\\ldots,C$, la\n",
    "règle de décision de Bayes donne :\n",
    "\n",
    "$$\\begin{aligned}\n",
    "c: \\mathbb{R}^D \\rightarrow \\{1,\\ldots,C\\}, x \\mapsto  argmax_{c}\\left(p(c|x)\\right).\n",
    "\\end{aligned}$$ \n",
    "\n",
    "L'utilisation de la fonction d'activation softmax en\n",
    "sortie permet d'interpréter les sorties du réseau comme de telles\n",
    "probabilités :la sortie du $i^{\\text{e}}$ neurone de la couche de sortie\n",
    "est\n",
    "\n",
    "$$\\begin{aligned}\n",
    "    \\sigma(z^{(L+1)},i) = \\frac{\\exp(z_i^{(L+1)})}{\\displaystyle\\sum_{k = 1} ^C \\exp(z_k^{(L+1)})}.\n",
    "\\end{aligned}$$\n",
    "\n",
    "En apprentissage profond, il a été reporté que la sigmoïde et la\n",
    "tangente hyperbolique avaient des performances moindres que la fonction\n",
    "d'activation softsign : \n",
    "\n",
    "$$\\begin{aligned}\n",
    "    s(z) = \\frac{1}{1+ |z|}.\n",
    "\\end{aligned}$$ \n",
    "\n",
    "En effet, les valeurs de $z$ arrivant près des paliers\n",
    "de saturation de ces fonctions donnent des gradients faibles, qui ont\n",
    "tendance à s'annuler lors de la phase d'apprentissage détaillée plus\n",
    "loin (rétropropagation du gradient). Une autre fonction, non saturante\n",
    "elle, peut être utilisée : \n",
    "\n",
    "$$\\begin{aligned}\n",
    "    r(z) = \\max (0,z).\n",
    "\\end{aligned}$$ \n",
    "\n",
    "Les neurones cachés utilisant la fonction décrite dans\n",
    "l'équation précédente sont appelés neurones linéaires rectifiés\n",
    "(**Rectified Linear Units, ReLUs**), et sont en pratique très utilisés.\n",
    "\n",
    "Quelques fonctions d'activation sont présentées dans la ({numref}`tabact`).\n",
    "\n",
    "```{figure} ./images/tabactivation.pdf\n",
    ":name: tabact\n",
    "Quelques fonctions d'activation\n",
    "```\n",
    "\n",
    "Les fonctions d'activation sous Pytorch sont résumées [ici](https://pytorch.org/docs/stable/nn.functional.html).\n",
    "\n",
    "## Entraînement des réseaux multicouches \n",
    "\n",
    "Pour pouvoir utiliser les réseaux multicouches en apprentissage, deux\n",
    "ingrédients sont indispensables :\n",
    "\n",
    "-   une méthode indiquant comment choisir une architecture de réseau\n",
    "    pour résoudre un problème donné. C'est-à-dire, pouvoir répondre aux\n",
    "    questions suivantes : combien de couches cachées ? combien de\n",
    "    neurones par couche cachée ?\n",
    "\n",
    "-   une fois l'architecture choisie, un algorithme d'apprentissage qui\n",
    "    calcule, à partir d'un l'échantillon d'apprentissage\n",
    "    ${\\cal E}_a = \\left \\{({\\mathbf x_n}, \\mathbf t_n),n\\in[\\![1,N]\\!] \\right \\}$ , les\n",
    "    valeurs des poids synaptiques pour construire un réseau adapté au\n",
    "    problème (c'est à dire approchant une fonction $g$ désirée mais\n",
    "    inconnue, telle qu'en particulier $\\mathbf t_n \\approx {\\mathbf g(x_n)}$) .\n",
    "\n",
    "Sur le premier point, quelques algorithmes d'apprentissage\n",
    "auto-constructifs ont été proposés. Leur rôle est double :\n",
    "\n",
    "-   apprentissage de l'échantillon avec un réseau courant,\n",
    "\n",
    "-   modification du réseau courant, en ajoutant de nouvelles cellules ou\n",
    "    une nouvelle couche, en cas d'échec de l'apprentissage.\n",
    "\n",
    "Il semble assez facile de concevoir des algorithmes auto-constructifs\n",
    "qui classent correctement l'échantillon, mais beaucoup plus difficile\n",
    "d'en obtenir qui aient un bon pouvoir de généralisation.\\\n",
    "Il a fallu attendre le milieu des années 1980 pour que le deuxième\n",
    "problème trouve une solution : l'algorithme de **rétropropagation du\n",
    "gradient**.\n",
    "\n",
    "L'entraînement, comme dans le cas de l'algorithme\n",
    "de descentre de gradient, consiste à trouver les poids qui minimisent une\n",
    "fonction d'erreur, mesurant l'écart entre la sortie du réseau $y({\\mathbf x_n})$\n",
    "et $\\mathbf t_n$, pour tous les exemples de ${\\cal E}_a$. Les fonctions\n",
    "couramment choisies sont les sommes des fonctions de perte sur chaque\n",
    "exemple, et incluent l'erreur quadratique \n",
    "\n",
    "$$\\begin{aligned}\n",
    "    E(\\mathbf w) = \\displaystyle\\sum_{n = 1}^N E_n(\\mathbf w) = \\displaystyle\\sum_{n = 1}^N \\sum_{i = 1}^C (y_i(\\mathbf x_n,\\mathbf w) - t^i_{n})^2\n",
    "\\end{aligned}$$ \n",
    "\n",
    "ou l'erreur d'entropie croisée \n",
    "\n",
    "$$\\begin{aligned}\n",
    "    E(\\mathbf w) = \\displaystyle\\sum_{n = 1}^N E_n(\\mathbf w) = \\displaystyle\\sum_{n = 1}^N \\sum_{i = 1}^C t^i_{n} \\log(y_i(\\mathbf x_n,\\mathbf w)),\n",
    "\\end{aligned}$$ \n",
    "\n",
    "où $t^i_{n}$ est la $i^{\\text{e}}$ composante de $\\mathbf t_n$.\n",
    "\n",
    "## Stratégies d'entraînement\n",
    "\n",
    "Parmi les stratégies d'entraînement qui peuvent être retenues, trois\n",
    "sont classiquement utilisées\n",
    "\n",
    "-   entraînement sur ${\\cal E}_a$, les poids étant mis à jour après\n",
    "    présentation, en fonction de l'erreur totale\n",
    "    $E(\\mathbf w) = \\displaystyle\\sum_{n=1}^N E_n(\\mathbf w)$.\n",
    "\n",
    "-   entraînement stochastique : un exemple est présenté et les poids\n",
    "    sont mis à jour sur l'erreur $E_n(\\mathbf w)$ calculée sur cet exemple\n",
    "    (règle Adaline)\n",
    "\n",
    "-   entraînement par batch sur un sous-ensemble\n",
    "    $M \\subseteq \\{1,\\ldots,N\\}$ de ${\\cal E}_a$, les poids étant mis à\n",
    "    jour en fonction de l'erreur cumulée\n",
    "    $E_M(\\mathbf w) = \\displaystyle\\sum_{n \\in M} E_n(\\mathbf w)$.\n",
    "\n",
    "## Optimisation des paramètres \n",
    "\n",
    "Considérons le cas de l'entraînement stochastique. La condition\n",
    "nécessaire d'optimalité d'ordre 1 donne \n",
    "\n",
    "$$\\begin{aligned}\n",
    "    \\frac{\\partial E_n}{\\partial \\mathbf w} = \\nabla E_n(\\mathbf w) = 0\n",
    "\\end{aligned}$$\n",
    "\n",
    "Une méthode itérative est utilisée pour trouver une solution approchée.\n",
    "Si $\\mathbf w[t]$ est le vecteur de poids à la $t^{\\text{e}}$ itération, une\n",
    "mise à jour des poids $\\Delta \\mathbf w[t]$ est calculée et propagée à\n",
    "l'itération suivante : $\\mathbf w[t+1] = \\mathbf w[t] + \\Delta \\mathbf w[t]$. Comme dans le cas du perceptron, on peut utiliser une méthode de type descente de gradient\n",
    "(ordre 1), ou une méthode type Newton (ordre 2, qui nécessite alors le\n",
    "calcul ou l'estimation du Hessien $H_n$ de $E_n$ à chaque itération).\n",
    "\n",
    "-   pour la méthode de descente du gradient, la mise à jour est effectuée\n",
    "    par \n",
    "\n",
    "    $$\\begin{aligned}\n",
    "                \\Delta \\mathbf w[t] = - \\gamma \\frac{\\partial E_n}{\\partial \\mathbf w[t]} = - \\gamma \\nabla E_n (\\mathbf w[t])\n",
    "            \n",
    "    \\end{aligned}$$ \n",
    "    \n",
    "    où $\\gamma$ est le taux d'apprentissage.\n",
    "\n",
    "-   pour les méthodes d'ordre 2, type Newton, la mise à jour s'effectue\n",
    "    selon le schéma \n",
    "    \n",
    "    $$\\begin{aligned}\n",
    "            \\Delta \\mathbf w[t] = - \\gamma \\left(\\frac{\\partial^2 E_n}{\\partial \\mathbf w[t]^2}\\right)^{-1} \\frac{\\partial E_n}{\\partial \\mathbf w[t]} = - \\gamma \\left(\\mathbf H_n\\mathbf (\\mathbf w[t])\\right)^{-1} \\nabla E_n(\\mathbf w[t])  \n",
    "    \\end{aligned}$$ \n",
    "    \n",
    "    où $\\gamma$ est le taux d'apprentissage. L'ordre 2 assure une convergence plus rapide, mais requiert le calcul et l'inversion du Hessien $\\mathbf H_n(\\mathbf w[t])$ de $E_n$, ce qui est coûteux.\n",
    "\n",
    "## Initialisation des poids \n",
    "\n",
    "Une méthode itérative d'optimisation étant utilisée, l'initialisation\n",
    "des poids requiert une attention toute particulière. \n",
    "\n",
    "Une première idée est d'initialiser les poids selon une loi normale : $\\forall i\\; w_{ij} \\rightsquigarrow 10^{-m}\\mathcal{N}(0,1)\\ m>0 $. Cependant, cela amène naturellement rapidement à une évolution des poids vers des valeurs nulles ({numref}`vanishing`) (phénomène de disparition du gradient).\n",
    "\n",
    "\n",
    "```{figure} ./images/vanishing.png\n",
    ":name: vanishing\n",
    "Evolution de la distribution des poids au cours des epochs d'apprentissage (10 epochs à gauche, 20 au centre et 50 epochs à droite).\n",
    "```\n",
    "\n",
    "Plus le réseau est profond, plus l'apprentissage sera sensible à ce phénomène. On parle de *disparition du gradient* (venishing gradient).\n",
    "\n",
    "En faisant l'hypothèse que les entrées de chaque cellule de la rétine sont\n",
    "distribuées selon une loi gaussienne, il est alors courant d'introduire une dépendance à la profondeur des couches considérées : on choisit les poids aléatoirement dans \n",
    "\n",
    "$$\\begin{aligned}\n",
    "    - \\frac{1}{\\sqrt{m^{(l-1)}}} < w_{i,j}^{(l)} < \\frac{1}{\\sqrt{m^{(l-1)}}}.\n",
    "\\end{aligned}$$\n",
    "\n",
    "En utilisant des fonctions d'activation sigmoïde, il a été prouvé que\n",
    "l'apprentissage était alors optimal, en le sens que l'apprentissage est\n",
    "rapide et que les poids atteignent une valeur stable quasiment tous en\n",
    "même temps.\n",
    "\n",
    "Un autre schéma d'initialisation est possible (initialisation\n",
    "normalisée, ou initialisation de Xavier) en choisissant\n",
    "\n",
    "$$\\begin{aligned}\n",
    "    - \\frac{\\sqrt{6}}{\\sqrt{m^{(l-1)} + m^{(l)}}} < w_{i,j}^{(l)} < \\frac{\\sqrt{6}}{\\sqrt{m^{(l-1)} + m^{(l)}}}.\n",
    "\\end{aligned}$$\n",
    "\n",
    "Donnons quelques éléments qui amènent à ce schéma. Supposons un perceptron multicouches avec $\\mathbf w\\in\\mathbb{R}^d, w_i i.i.d \\rightsquigarrow\\mathcal{N}(0,1)$ et d'entrées $\\mathbf x\\in\\mathbb{R}^d$, $x_i\\ \\  i.i.d \\rightsquigarrow\\mathcal{N}(0,1)$. \n",
    "\n",
    "Le potentiel post-synaptique d'un neurone de la première couche cachée est de la forme $\\mathbf w^\\top \\mathbf x$.\n",
    "\n",
    "Alors \n",
    "\n",
    "$$\\begin{aligned} Var(\\mathbf w^\\top\\mathbf x) =& \\displaystyle\\sum_{i=1}^d Var (w_ix_i) \\\\\n",
    "=&  \\displaystyle\\sum_{i=1}^d \\left (\\mathbb{E}(w_i)^2Var(x_i) + \\mathbb{E}(x_i)^2Var (w_i) + Var (w_i) Var(x_i) \\right )\n",
    "\\end{aligned}$$\n",
    "\n",
    "\n",
    "Puisque $\\mathbb{E}(w_i)=\\mathbb{E}(x_i)=0$\n",
    "\n",
    "$$Var(\\mathbf w^\\top\\mathbf x) =  \\displaystyle\\sum_{i=1}^dVar (w_i) Var(x_i) $$\n",
    "\n",
    "Or $w_i,\\ x_i i.i.d $  donc $ Var(\\mathbf w^\\top\\mathbf x) =  dVar (w_i) Var(x_i) $\n",
    "\n",
    "Et plus généralement \n",
    "\n",
    "$$Var(y^l) = \\left ( d Var(w_i)\\right )^l Var (x_i)$$ \n",
    "\n",
    "Chaque neurone peut varier d'un facteur $d$ par rapport à son entrée.\n",
    "\n",
    "Si $dVar(w_i)>1$ le gradient croît à mesure que l'on s'enfonce dans le réseau. A l'inverse, si $d Var(w_i)<1$  le gradient disparaît lorsque $l$ croît. \n",
    "\n",
    "Il est donc légitime pour imiter ces deux phénomènes d'imposer  $d Var(w_i)=1$, et donc $ Var(w_i)=1/d$\n",
    "et  \n",
    "\n",
    "$$\\mathbf  w_{ij}^l \\rightsquigarrow\\ \\frac{1}{\\sqrt{m^{l-1}}}\\mathcal{N}(0,1)$$\n",
    "\n",
    "Si la fonction d'activation du neurone est la fonction ReLU, on peut multiplier par $\\frac{\\sqrt{2}}{\\sqrt{m^{l-1}}}$ pour prendre en compte la partie négative qui ne participe pas au calcul de la variance.\n",
    "\n",
    "\n",
    "## Rétropropagation de l'erreur \n",
    "\n",
    "l'{prf:ref}`backprop`, dit algorithme de\n",
    "rétropropagation du gradient, est utilisé pour évaluer le gradient\n",
    "$\\nabla E_n (\\mathbf w[t])$ de l'erreur $E_n$ à chaque itération, ceci pour tous\n",
    "les poids\n",
    "\n",
    "```{prf:algorithm} Algorithme de rétropropagation du gradient\n",
    ":label: backprop\n",
    "1.  Propager un exemple $\\mathbf x_n$ dans le réseau.\n",
    "2.  Calculer les erreurs $\\delta_i^{(L+1)}$ des neurones de sortie :\n",
    "\n",
    "    $$\\begin{aligned}\n",
    "                (\\forall i\\in\\{1\\cdots C\\})\\quad\\delta_i^{(L+1)} = \\frac{\\partial E_n}{\\partial y_i^{(L+1)}} f'(z_i^{(L+1)}).\n",
    "            \n",
    "    \\end{aligned}$$\n",
    "3.  Déterminer $\\delta _i ^{(l)}$ pour toutes les couches cachées :\n",
    "\n",
    "    $$\\begin{aligned}\n",
    "                (\\forall l\\in\\{1\\cdots L\\})(\\forall i\\in\\{1\\cdots m^l\\})\\quad\\delta _i ^{(l)} = f' (z_i^{(l)}) \\sum _{k = 1} ^{m^{(l+1)}} w_{i,k}^{(l+1)} \\delta _k ^{(l+1)}.\n",
    "            \n",
    "    \\end{aligned}$$\n",
    "4.  Calculer les composantes du gradient : \n",
    "\n",
    "$$\\begin{aligned}\n",
    "                \\frac{\\partial E_n}{\\partial w_{j,i}^{(l)}} = \\delta _j ^{(l)} y_i^{(l-1)}.           \n",
    "    \\end{aligned}$$\n",
    "```\n",
    "\n",
    "Dans le cas d'un apprentissage stochastique, cet algorithme est appliqué\n",
    "jusqu'à convergence, pour estimer les poids du réseau de neurones.\n",
    "\n",
    "## Critères d'arrêt\n",
    "\n",
    "Plusieurs critères d'arrêt peuvent être utilisés avec l'algorithme de\n",
    "rétropropagation du gradient. Le plus commun consiste à fixer un nombre\n",
    "maximum de périodes d'entraînement (sur ${\\cal E}_a$), ce qui fixe\n",
    "effectivement une limite supérieure sur la durée de l'apprentissage. Ce\n",
    "critère est important car la rétropropagation n'offre aucune garantie\n",
    "quant à la convergence de l'algorithme. Il peut arriver, par exemple,\n",
    "que le processus d'optimisation reste pris dans un minimum local. Sans\n",
    "un tel critère, l'algorithme pourrait ne jamais se terminer. Un deuxième\n",
    "critère commun consiste à fixer une borne inférieure sur l'erreur\n",
    "quadratique moyenne. Dépendant de l'application, il est parfois possible\n",
    "de fixer *a priori* un objectif à atteindre. Lorsque l'indice de\n",
    "performance choisi diminue en dessous de cet objectif, on considère\n",
    "simplement que le réseau a suffisamment bien appris ses données et on\n",
    "arrête l'apprentissage.\\\n",
    "Les deux critères précédents sont utiles mais ils comportent aussi des\n",
    "limitations. Le critère relatif au nombre maximum de périodes\n",
    "d'entraînement n'est aucunement lié à la performance du réseau. Le\n",
    "critère relatif à l'erreur minimale obtenue mesure quant à lui un indice\n",
    "de performance mais ce dernier peut engendrer du sur-apprentissage qui n'est pas désirable dans la pratique, surtout si l'on ne possède pas une grande quantité de données d'apprentissage, ou\n",
    "si ces dernières ne sont pas de bonne qualité.\\\n",
    "Un processus d'apprentissage comme celui de la rétropropagation, vise à\n",
    "réduire autant que possible l'erreur que commet le réseau. Mais cette\n",
    "erreur est mesurée sur un ensemble de données d'apprentissage\n",
    "${\\cal E}_a$. Si les données sont bonnes, c'est-à-dire quelles\n",
    "représentent bien le processus physique sous-jacent que l'on tente\n",
    "d'apprendre ou de modéliser, et que l'algorithme a convergé sur un\n",
    "optimum global, alors il devrait bien se comporter sur d'autres données\n",
    "issues du même processus physique . Cependant, si les données\n",
    "d'apprentissage sont partiellement corrompues par du bruit ou par des\n",
    "erreurs de mesure, alors il n'est pas évident que la performance\n",
    "optimale du réseau soit atteinte en minimisant l'erreur, lorsqu'on la\n",
    "testera sur un jeu de données différent de celui qui a servi à\n",
    "l'entraînement. On parle alors de la **capacité de généralisation** du\n",
    "réseau, c'est-à-dire sa capacité à bien se comporter avec des données\n",
    "qu'il n'a jamais vu auparavant.\n",
    "\n",
    "Une solution à ce problème consiste à faire appel à un autre critère\n",
    "d'arrêt basé sur une technique de validation croisée. Cette technique\n",
    "consiste à utiliser deux ensembles indépendants de données. En pratique,\n",
    "il s'agit de partitionner ${\\cal E}_a$ pour entraîner le réseau en un\n",
    "ensemble d'apprentissage (ajustement des poids) un ensemble de\n",
    "validation (calcul d'un indice de performance). Le critère d'arrêt\n",
    "consiste alors à stopper l'apprentissage lorsque l'indice de performance\n",
    "calculé sur les données de validation cesse de s'améliorer pendant\n",
    "plusieurs périodes d'entraînement. Lors de deux périodes successives\n",
    "d'entraînement, des exemples peuvent être échangés entre ensembles\n",
    "d'apprentissage et de validation.\n",
    "\n",
    "## Propriété fondamentale\n",
    "\n",
    "Terminons par une dernière remarque sur la puissance de représentation\n",
    "des réseaux multicouches. La plupart des fonctions numériques peuvent\n",
    "être approximées avec une précision arbitraire par des réseaux à une\n",
    "seule couche cachée. Mais cette couche cachée peut être démesurément\n",
    "grande et le théorème de Hornik, qui affirme cette propriété\n",
    "d'approximateurs universels des réseaux multicouches, est\n",
    "essentiellement un résultat théorique sur l'expressivité des réseaux.\n",
    "\n",
    "Plus formellement, la propriété fondamentale des réseaux de neurones est\n",
    "l'**approximation parcimonieuse**, qui traduit deux propriétés distinctes :\n",
    "d'une part les réseaux de neurones sont des **approximateurs universels**,\n",
    "et d'autre part, une approximation à l'aide d'un réseau de neurones\n",
    "nécessite, en général, moins de paramètres ajustables que les\n",
    "approximateurs usuels.\n",
    "\n",
    "-   Approximateurs universels : Cybenko a énoncé en 1989 la propriété\n",
    "    suivante : toute fonction bornée, suffisamment régulière, peut être\n",
    "    approchée uniformément, avec une précision arbitraire, dans un\n",
    "    domaine fini de l'espace de ses variables, par un réseau de neurones\n",
    "    comportant une couche de neurones cachés en nombre fini, possédant\n",
    "    tous la même fonction d'activation, et un neurone de sortie\n",
    "    linéaire.\n",
    "\n",
    "-   Parcimonie : Hornik a montré en 1994 que si la sortie d'un réseau de\n",
    "    neurones est une fonction non linéaire des paramètres ajustables,\n",
    "    elle est plus parcimonieuse que si elle était une fonction linéaire\n",
    "    de ces paramètres. De plus, pour les réseaux dont la fonction\n",
    "    d'activation des neurones est une sigmoïde, l'erreur commise dans\n",
    "    l'approximation varie comme l'inverse du nombre de neurones cachés,\n",
    "    et elle est indépendante du nombre de variables de la fonction à\n",
    "    approcher. Ainsi, pour une précision donnée (*i.e.* étant donné un\n",
    "    nombre de neurones cachés) le nombre de paramètres du réseau est\n",
    "    proportionnel au nombre de variables de la fonction à approcher.\n",
    "\n",
    "Dans la plupart des cas d'utilisation des réseaux de neurones, il va\n",
    "s'agir d'établir un modèle d'une fonction inconnue à partir de mesures\n",
    "bruitées de l'ensemble d'apprentissage, permettant de reproduire les\n",
    "sorties à partir des entrées, et de proposer une généralisation à des\n",
    "données test. On cherche alors la **fonction de régression** du\n",
    "processus considéré, *i.e.* la fonction obtenue en calculant la moyenne\n",
    "d'une infinité de mesures effectuées en chaque point du domaine de\n",
    "validité du modèle. Le nombre de points de ce domaine étant lui-même\n",
    "infini, la connaissance de la fonction de régression nécessiterait donc\n",
    "une infinité de mesures en un nombre infini de points.\n",
    "\n",
    "Les réseaux de neurones, en raison de leur propriété fondamentale, sont\n",
    "de bons candidats pour réaliser une approximation de la fonction de\n",
    "régression à partir d'un nombre fini de mesures. Ils entrent donc dans\n",
    "le cadre des méthodes statistiques d'apprentissage, et élargissent ce\n",
    "domaine déjà bien exploré pour des fonctions de régression linéaire au\n",
    "cas non linéaire.\n",
    "\n",
    "\n",
    "Pour illustration simple, on considère le problème d'apprentissage de la fonction $f(x)=x^2$ en utilisant un  réseau à une couche cachée de trois neurones, équipés de la fonction d'activation $tanh$ ({numref}`approx2`). La ({numref}`approx`) présente le résultat de l'apprentissage : en rouge sont représentés 50 points d'apprentissage, en bleu la fonction prédite sur l'intervalle [-1,1]. Les fonctions apprises par les trois neurones sont en pointillés, et la combinaison linéaire de ces fonctions donne la courbe prédite.\n",
    "\n",
    "```{figure} ./images/exmlp.png\n",
    ":name: approx2\n",
    "Un perceptron multicouches à une couche cachée\n",
    "```\n",
    "\n",
    "```{figure} ./images/f1.png\n",
    ":name: approx\n",
    "Approximation de $f(x)=x^2$ par un réseau à une couche cachée.\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Régularisation\n",
    "\n",
    "La notion d'approximateur universel peut induire également un problème\n",
    "de surapprentissage (overfitting) de ${\\cal E}_a$. Les techniques de\n",
    "régularisation permettent d'éviter ce problème, et permettent aux\n",
    "réseaux de neurones (et à d'autres algorithmes d'ailleurs, comme les\n",
    "autoencodeurs ou les SVM par exemple) d'avoir une bonne capacité de\n",
    "généralisation.\\\n",
    "Il existe plusieurs techniques permettant d'introduire de la\n",
    "régularisation dans les réseaux. Parmi elles, on note :\n",
    "\n",
    "-   ${\\cal E}_a$ est enrichi pour introduire certaines invariances que\n",
    "    le réseau est supposé apprendre.\n",
    "\n",
    "-   à chaque exemple/batch présenté, chaque neurone caché est supprimé du\n",
    "    calcul de la sortie avec probabilité $p$ (dropout). Cette technique\n",
    "    peut être vue comme la construction d'un modèle moyen\n",
    "    d'apprentissage de plusieurs réseaux distincts.\n",
    "\n",
    "-   lorsque ${\\cal E}_a$ est séparé en un ensemble d'apprentissage\n",
    "    ${\\cal E}^1_a$ et un ensemble de validation ${\\cal E}^2_a$, il est\n",
    "    courant de voir que l'erreur baisse sur ${\\cal E}^1_a$ au fil des\n",
    "    itérations, alors que l'erreur sur ${\\cal E}^2_a$ tend à augmenter\n",
    "    lorsque le réseau commence à sur-apprendre sur ${\\cal E}^1_a$.\n",
    "    L'entraînement est alors stoppé dès que l'erreur sur ${\\cal E}^2_a$\n",
    "    atteint un minimum. Cette technique est appelée early stopping\n",
    "    (arrêt précoce).\n",
    "\n",
    "-   le partage de poids : plusieurs neurones d'une même couche partagent\n",
    "    des mêmes valeurs de poids. La complexité du réseau est réduite et\n",
    "    des informations *a priori* peuvent être introduites par ce biais\n",
    "    dans l'architecture du réseau. L'algorithme de rétropropagation du\n",
    "    gradient s'en trouve modifié et l'étape 4 devient \n",
    "    \n",
    "    $$\\begin{aligned}\n",
    "        \\frac{\\partial E_n}{\\partial w_{j,i}^{(l)}} = \\sum _{k = 1} ^{m^{(l)}} \\delta_k^{(l)} y_i^{(l-1)}\n",
    "    \\end{aligned}$$ \n",
    "    \n",
    "    en supposant que tous les neurones de la couche $l$\n",
    "    sont tels que $w_{j,i}^{(l)} = w_{k,i}^{(l)}$ pour\n",
    "    $1 \\leq j,k \\leq m^{(l)}$\n",
    "\n",
    "-   un terme de régularisation est ajouté à la fonctionnelle à minimiser\n",
    "    pour contrôler la complexité et le forme de la solution et, par\n",
    "    exemple \n",
    "    \n",
    "    $$\\begin{aligned}\n",
    "        \\hat{E}_n (\\mathbf w) = E_n (\\mathbf w) + \\eta P(\\mathbf w)\n",
    "    \\end{aligned}$$ \n",
    "    \n",
    "    où $P(\\mathbf w)$ influence la forme de la solution et\n",
    "    $\\eta$ contrôle l'influence du terme de régularisation. $P(\\mathbf w)$ peut\n",
    "    prendre la forme d'une fonction de la norme $L_p$ de $\\mathbf w$. Deux\n",
    "    exemples classiques sont :\n",
    "\n",
    "    -   la régularisation $L_2$ : $P(\\mathbf w) = \\|\\mathbf w\\|_2^2 = \\mathbf w^\\top \\mathbf w$, où le principe est de pénaliser les poids de fortes valeurs, qui tendent à amplifier le problème de  surapprentissage.\n",
    "    -   la régularisation $L_1$ : $P(\\mathbf w) = \\|\\mathbf w\\|_1 = \\displaystyle\\sum_{k = 1} ^W |w_k|$ où $W$ est la dimension de $\\mathbf w$, qui tend à rendre épars le vecteur de poids (beaucoup de valeurs de poids deviennent nulles).\n",
    "\n",
    "## Exemple\n",
    "\n",
    "On va considérer le réseau décrit sur la ({numref}`xor`) pour apprendre la fonction du OU\n",
    "exclusif (aussi appelé XOR). L'opérateur XOR est défini par sa table de\n",
    "vérité donnée par le tableau  suivant \n",
    "\n",
    "| $x_1 \\backslash x_2$ | 0 | 1 |\n",
    "|-----------------------------|---|---|\n",
    "| 0                           | 0 | 1 |\n",
    "| 1                           | 1 | 0 |\n",
    "\n",
    "### Réseau\n",
    "\n",
    "Sur le réseau de la figure\n",
    "({numref}`xor`) les différentes relations sont\n",
    "données par l'équation suivante où les paramètres en rouge correspondent aux poids à\n",
    "calculer durant la phase d'apprentissage.\n",
    "\n",
    "$$\\left\\{\n",
    "        \\begin{array}{r c l}\n",
    "            z_1 &=& {\\color{red}w_1}~x_1+{\\color{red}w_2}~x_2+{\\color{red}b_1}\\\\\n",
    "            z_2 &=& {\\color{red}w_3}~x_1+{\\color{red}w_4}~x_2+{\\color{red}b_2}\\\\\n",
    "            z_3 &=& {\\color{red}w_5}~x_1+{\\color{red}w_6}~x_2+{\\color{red}b_3}\\\\\n",
    "            z_4 &=& {\\color{red}w_7}~h_1\\left(z_1\\right)+{\\color{red}w_8}~h_2\\left(z_2\\right)+{\\color{red}w_9}~h_3\\left(z_3\\right)+{\\color{red}b_4}\\\\\n",
    "            y &=& h_4\\left(z_4\\right) \\\\\n",
    "            h_i\\left(z_i\\right) &=& \\frac{1}{1+e^{-z_i}} \\ \\ \\ \\ (sigmoide)\n",
    "        \\end{array}\n",
    "    \\right .\n",
    "    $$\n",
    "\n",
    "\n",
    "```{figure} ./images/xor.png\n",
    ":name: xor\n",
    "Exemple de réseau pour apprendre le XOR\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Phase d'apprentissage\n",
    "\n",
    "Durant la phase d'apprentissage, on minimise un risque empirique par une\n",
    "fonction de coût. Dans cet exemple, nous allons choisir la minimisation\n",
    "de l'écart quadratique avec la base d'apprentissage labelisée\n",
    "${\\cal E}_a =\\left( \\textbf{x}, {y}_{lab} \\right)$ ou une partie\n",
    "de cette base d'apprentissage $\\mathcal{E}_a^\\prime$ :\n",
    "\n",
    "$$E\\left({\\cal E}_a\\right)=E_{tot}=\\displaystyle\\frac{1}{2}\\sum_k \\left(y[k]_{lab} - y \\right)^2 = \\displaystyle\\frac{1}{2}\\sum_k\\left(y[k]_{lab} - h_4\\left(z_4\\right)  \\right)^2.$$\n",
    "\n",
    "On peut utiliser qu'une partie de la base, voire que le $k^{ieme}$\n",
    "échantillon de la base (gradient stochastique) :\n",
    "\n",
    "$$E\\left(x_k\\right)=E_{k}=\\displaystyle\\frac{1}{2}\\left(y[k]_{lab} - y \\right)^2 = \\displaystyle\\frac{1}{2}\\left(y[k]_{lab} - h_4\\left(z_4\\right)  \\right)^2 .\n",
    "    $$\n",
    "\n",
    "L'objectif de la phase d'apprentissage est de mettre à jour les poids du\n",
    "réseau par une approche de descente du gradient. Si l'on considère un\n",
    "poids quelconque du réseau que l'on note $\\theta$, sa mise à jour durant\n",
    "l'itération $n+1$ se fait pas l'équation suivante:\n",
    "\n",
    "$$\\theta^{\\left(n+1\\right)} = \\theta^{\\left(n\\right)} + \\gamma_n\\times \\Delta \\theta$$\n",
    "\n",
    "où $\\Delta \\theta = -\\nabla_\\theta E$. \n",
    "\n",
    "\n",
    "On peut choisir $E=E_{tot}$ ou\n",
    "$E=E_k$ et pour cet exemple nous choisirons le deuxième cas.\n",
    "\n",
    "#### Couche de sortie\n",
    "\n",
    "Pour la couche de sortie, prenons par exemple le paramètre $w_7$, sa\n",
    "mise à jour est donnée par la relation suivante :\n",
    "\n",
    "$$w_7^{\\left(n+1\\right)} = w_7^{\\left(n\\right)} - \\eta \\times \\frac{\\partial E_k}{\\partial w_7}.$$\n",
    "\n",
    "Le problème consiste à calculer $\\frac{\\partial E_k}{\\partial w_7}$,\n",
    "pour cela nous allons utiliser le théorème de dérivation des fonctions\n",
    "composées, d'où:\n",
    "\n",
    "$$\\frac{\\partial E_k}{\\partial w_7} = \\frac{\\partial E_k}{\\partial h_4} \\times  \\frac{\\partial h_4}{\\partial z_4} \\times  \\frac{\\partial z_4}{\\partial w_7}$$\n",
    "\n",
    "Cette relation est représentée graphiquement sur la ({numref}`xor2`). \n",
    "Utilisant l'expression de A $E_k$ on a alors :\n",
    "\n",
    "$$\\left\\{\n",
    "        \\begin{array}{r c l}\n",
    "            \\displaystyle\\frac{\\partial E_k}{\\partial h_4} &=& \\frac{\\partial }{\\partial h_4}\\left( \\frac{1}{2}\\left(y[k]_{lab} - h_4\\left(z_4\\right)  \\right)^2 \\right) = -\\left(y[k]_{lab} - h_4\\left(z_4\\right)  \\right)\\\\\n",
    "            \\displaystyle\\frac{\\partial h_4}{\\partial z_4}  &=& \\frac{\\partial }{\\partial z_4}\\left( \\frac{1}{1+e^{-z_4}}\\right) = \\frac{e^{-z_4}}{\\left(1+e^{-z_4}\\right)^2} = h_4\\left(z_4\\right)\\left( 1 - h_4\\left(z_4\\right)\\right) \\\\\n",
    "            \\displaystyle\\frac{\\partial z_4}{\\partial w_7} &=& h_1\\left(z_1\\right)\n",
    "        \\end{array}\n",
    "    \\right.\n",
    "    $$ \n",
    "\n",
    "d'où\n",
    "\n",
    "$$w_7^{(n+1)} = w_7^{(n) }+ (y[k]_{lab} - h_4(z_4)) h_4(z_4)( 1 - h_4(z_4))\n",
    "h_1(z_1)$$\n",
    "\n",
    "On peut réaliser la même démarche pour les poids $w_8$, $w_9$ et $b_4$,\n",
    "pour obtenir les relations suivantes :\n",
    "\n",
    "$$\\left\\{\n",
    "        \\begin{array}{r c l}\n",
    "            w_8^{\\left(n+1\\right)} &=& w_8^{\\left(n\\right)} + \\eta \\times \\left(y[k]_{lab} - h_4\\left(z_4\\right)\\right) h_4\\left(z_4\\right)\\left( 1 - h_4\\left(z_4\\right)\\right) h_2\\left(z_2\\right) \\\\\n",
    "            w_9^{\\left(n+1\\right)} &=& w_9^{\\left(n\\right)} + \\eta \\times \\left(y[k]_{lab} - h_4\\left(z_4\\right)\\right) h_4\\left(z_4\\right)\\left( 1 - h_4\\left(z_4\\right)\\right) h_3\\left(z_3\\right) \\\\\n",
    "            b_4^{\\left(n+1\\right)} &=& b_4^{\\left(n\\right)} + \\eta \\times \\left(y[k]_{lab} - h_4\\left(z_4\\right)\\right) h_4\\left(z_4\\right)\\left( 1 - h_4\\left(z_4\\right)\\right) \n",
    "        \\end{array}\n",
    "    \\right.\n",
    "    $$\n",
    "\n",
    "```{figure} ./images/xor2.png\n",
    ":name: xor2\n",
    "Rétropropagation du gradient sur la couche de sortie.\n",
    "```\n",
    "\n",
    "#### Couche cachée\n",
    "\n",
    "Pour la couche cachée du réseau, c'est exactement le même raisonnement.\n",
    "Prenons par exemple, le paramètre $w_1$ pour le calcul :\n",
    "\n",
    "$$w_1^{\\left(n+1\\right)} = w_1^{\\left(n\\right)} - \\eta \\times \\frac{\\partial E_k}{\\partial w_1}.$$\n",
    "\n",
    "Le problème maintenant consiste à calculer\n",
    "$\\frac{\\partial E_k}{\\partial w_1}$, pour cela nous allons utiliser le\n",
    "théorème de dérivation des fonctions composées, d'où:\n",
    "\n",
    "$$\\frac{\\partial E_k}{\\partial w_1} = \\frac{\\partial E_k}{\\partial z_4} \\times  \\frac{\\partial z_4}{\\partial h_1}  \\times  \\frac{\\partial h_1}{\\partial z_1} \\times  \\frac{\\partial z_1}{\\partial w_1}$$\n",
    "\n",
    "Cette relation est représentée graphiquement sur la ({numref}`xor3`). \n",
    "Utilisant les équations précédentes, on obtient alors  \n",
    "\n",
    "$$\\left\\{\n",
    "        \\begin{array}{r c l}\n",
    "            \\displaystyle\\frac{\\partial E_k}{\\partial z_4} &=& -\\left(y[k]_{lab} - h_4\\left(z_4\\right)\\right) h_4\\left(z_4\\right)\\left( 1 - h_4\\left(z_4\\right)\\right)\\\\\n",
    "            \\displaystyle\\frac{\\partial z_4}{\\partial h_1}  &=& w_7 \\\\\n",
    "            \\displaystyle\\frac{\\partial h_1}{\\partial z_1} &=& h_1\\left(z_1\\right)\\left( 1 - h_1\\left(z_1\\right)\\right)\\\\\n",
    "            \\displaystyle\\frac{\\partial z_1}{\\partial w_1} &=& x_1\n",
    "        \\end{array}\n",
    "    \\right.\n",
    "    $$ \n",
    "    \n",
    " d'où\n",
    "\n",
    "$$\n",
    "w_1^{(n+1)} = w_1^{(n)} + (y[k]_{ lab} - h_4(z_4)) h_4(z_4)( 1 - h_4(z_4))h_1(z_1)( 1 - h_1(z_1)) w_7 x_1\n",
    "$$\n",
    "\n",
    "On peut réaliser la même démarche pour les poids $w_2$ et $b_1$, pour\n",
    "obtenir les relations suivantes :\n",
    "\n",
    "$$\\left\\{\n",
    "        \\begin{array}{r c l}\n",
    "            w_2^{\\left(n+1\\right)} &=& w_2^{\\left(n\\right)} + \\eta \\left(y[k]_{lab} - h_4\\left(z_4\\right)\\right) h_4\\left(z_4\\right)\\left( 1 - h_4\\left(z_4\\right)\\right) h_1\\left(z_1\\right)\\left( 1 - h_1\\left(z_1\\right)\\right) w_7 x_2\\\\\n",
    "            b_1^{\\left(n+1\\right)} &=& b_1^{\\left(n\\right)} + \\eta \\left(y[k]_{lab} - h_4\\left(z_4\\right)\\right) h_4\\left(z_4\\right)\\left( 1 - h_4\\left(z_4\\right)\\right) h_1\\left(z_1\\right)\\left( 1 - h_1\\left(z_1\\right)\\right) w_7\n",
    "        \\end{array}\n",
    "    \\right.\n",
    "    $$\n",
    "\n",
    "```{figure} ./images/xor3.png\n",
    ":name: xor3\n",
    "Rétropropagation du gradient sur la couche cachée.\n",
    "```\n",
    "\n",
    "Il suffit de réaliser des calculs identiques pour les autres neurones et\n",
    "on obtient les relations suivantes :\n",
    "\n",
    " $$\\left\\{\n",
    "            \\begin{array}{r c l}\n",
    "                w_3^{\\left(n+1\\right)} &=& w_3^{\\left(n\\right)} + \\eta \\left(y[k]_{lab} - h_4\\left(z_4\\right)\\right) h_4\\left(z_4\\right)\\left( 1 - h_4\\left(z_4\\right)\\right) h_2\\left(z_2\\right)\\left( 1 - h_2\\left(z_2\\right)\\right) w_8 x_1\\\\                \n",
    "                w_4^{\\left(n+1\\right)} &=& w_4^{\\left(n\\right)} + \\eta \\left(y[k]_{lab} - h_4\\left(z_4\\right)\\right) h_4\\left(z_4\\right)\\left( 1 - h_4\\left(z_4\\right)\\right) h_2\\left(z_2\\right)\\left( 1 - h_2\\left(z_2\\right)\\right) w_8 x_2\\\\\n",
    "                w_5^{\\left(n+1\\right)} &=& w_5^{\\left(n\\right)} + \\eta \\left(y[k]_{lab} - h_4\\left(z_4\\right)\\right) h_4\\left(z_4\\right)\\left( 1 - h_4\\left(z_4\\right)\\right) h_3\\left(z_3\\right)\\left( 1 - h_3\\left(z_3\\right)\\right) w_9 x_1\\\\                \n",
    "                w_6^{\\left(n+1\\right)} &=& w_6^{\\left(n\\right)} + \\eta \\left(y[k]_{lab} - h_4\\left(z_4\\right)\\right) h_4\\left(z_4\\right)\\left( 1 - h_4\\left(z_4\\right)\\right) h_3\\left(z_3\\right)\\left( 1 - h_3\\left(z_3\\right)\\right) w_9 x_2\\\\\n",
    "                b_2^{\\left(n+1\\right)} &=& b_2^{\\left(n\\right)} + \\eta \\left(y[k]_{lab} - h_4\\left(z_4\\right)\\right) h_4\\left(z_4\\right)\\left( 1 - h_4\\left(z_4\\right)\\right) h_2\\left(z_2\\right)\\left( 1 - h_2\\left(z_2\\right)\\right) w_8 \\\\\n",
    "                b_3^{\\left(n+1\\right)} &=& b_3^{\\left(n\\right)} + \\eta \\left(y[k]_{lab} - h_4\\left(z_4\\right)\\right) h_4\\left(z_4\\right)\\left( 1 - h_4\\left(z_4\\right)\\right) h_3\\left(z_3\\right)\\left( 1 - h_3\\left(z_3\\right)\\right) w_9               \n",
    "            \\end{array}\n",
    "        \\right.$$\n",
    "\n",
    "#### Initialisation des poids \n",
    "\n",
    "-   les biais sont initialisés à zéro : \n",
    "    $( b_1, b_2, b_3, b_4 ) = \\mathbf{0}_4$ ;\n",
    "\n",
    "-   pour les poids $w_i$, ils sont initialisées de façon aléatoire\n",
    "    dépendant de la taille de la couche d'avant $m^{(l-1)}$ et d'après\n",
    "    $m^{(l)}$.\n",
    "    L'initialisation de Xavier propose un tirage uniforme dans\n",
    "    $\\left[-\\sqrt{\\frac{6}{m^{(l-1)}+m^{(l)}}} ;\\sqrt{\\frac{6}{m^{(l-1)}+m^{(l)}}} \\right]$.\n",
    "\n",
    "Pour notre exemple, on obtient l'initialisation suivante : \n",
    "\n",
    "$$\\left\\{\n",
    "        \\begin{array}{r c l}\n",
    "            w_1 \\ ...\\ w_6 &=& U\\left ( \\left[-\\sqrt{\\frac{6}{2+3}} ;\\sqrt{\\frac{6}{2+3}} \\right] \\right ) = U(\\left[-1,09;1,09 \\right])\\\\\n",
    "            w_7 \\ ... \\ w_9 &=& U\\left (\\left[-\\sqrt{\\frac{6}{3+1}} ;\\sqrt{\\frac{6}{3+1}} \\right]\\right ) = U(\\left[-1,23;1,23 \\right])\n",
    "        \\end{array}\n",
    "    \\right.$$\n",
    "\n",
    "\n",
    "## Implémentation\n",
    "\n",
    "Nous reprenons les exemples du perceptron et montrons qu'un PMC à une couche cachée permet de résoudre le problème de séparation non linéaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b2d72f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import colors\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "fichiers_train = ['./data/linear_data_train.csv','./data/twocircles_data_train.csv','./data/moon_data_train.csv']\n",
    "fichiers_test = ['./data/linear_data_eval.csv','./data/twocircles_data_eval.csv','./data/moon_data_eval.csv']\n",
    "\n",
    "# Fonction de lecture des jeux de données\n",
    "def extract_data(filename):\n",
    "\n",
    "    labels = []\n",
    "    features = []\n",
    "\n",
    "    for line in open(filename):\n",
    "        row = line.split(\",\")\n",
    "        labels.append(int(row[0]))\n",
    "        features.append([float(x) for x in row[1:]])\n",
    "\n",
    "    features_np = np.matrix(features).astype(np.float32)\n",
    "\n",
    "    labels_np = np.array(labels).astype(dtype=np.uint8)\n",
    "    labels_onehot = (np.arange(num_labels) == labels_np[:, None]).astype(np.float32)\n",
    "\n",
    "    return features_np,labels_onehot    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb3b091",
   "metadata": {},
   "source": [
    "On écrit une fonction permettant de visualiser le résultat de la classification par le perceptron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "339e7877",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotResults(ax,ay,X,Y,model,title,pltloss,name):\n",
    "    mins = np.amin(X,0); \n",
    "    mins = mins - 0.1*np.abs(mins);\n",
    "    maxs = np.amax(X,0); \n",
    "    maxs = maxs + 0.1*maxs;\n",
    "\n",
    "    xs,ys = np.meshgrid(np.linspace(mins[0,0],maxs[0,0],300),np.linspace(mins[0,1], maxs[0,1], 300));\n",
    "\n",
    "    toto = torch.FloatTensor(np.c_[xs.flatten(), ys.flatten()])\n",
    "    Z = np.argmax(model(toto).detach().numpy(), axis=-1)\n",
    "    Z=Z.reshape(xs.shape[0],xs.shape[1])\n",
    "    \n",
    "    labelY = np.matrix(Y[:, 0]+2*Y[:, 1])\n",
    "    labelY = labelY.reshape(np.array(X[:, 0]).shape)\n",
    "\n",
    "    ax.contourf(xs, ys, Z, cmap=plt.cm.magma,alpha=.5)\n",
    "    ax.scatter(np.array(X[:, 0]),np.array(X[:, 1]),c= np.array(labelY),s=20,cmap=colors.ListedColormap(['red', 'green']))\n",
    "    ax.set_title(title)\n",
    "\n",
    "    ay.plot(pltloss)\n",
    "    ay.set_title(\"perte sur \" + name)\n",
    "    ay.set_xlabel(\"epoch\")\n",
    "\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80d9730",
   "metadata": {},
   "source": [
    "On implémente une classe PMC, dérivée de la clase `nn.Module`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6826dc9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nombre de neurones de la couche cachée\n",
    "num_hidden = 5\n",
    "\n",
    "class PMC(nn.Module):\n",
    "    def __init__(self,p):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "          nn.Linear(num_features, num_hidden),\n",
    "          nn.Tanh(),\n",
    "          nn.Linear(num_hidden, num_labels),\n",
    "          nn.Softmax(1),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x): \n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f45117",
   "metadata": {},
   "source": [
    "et on définit la fonction d'entraînement. La fonction de perte est l'[entropie croisée binaire](https://en.wikipedia.org/wiki/Cross_entropy) et l'optimiseur est [Adam ](https://arxiv.org/abs/1412.6980)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "389cb3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100  \n",
    "num_epochs = 10000\n",
    "\n",
    "def train_session(X,y,classifier,criterion,optimizer,n_epochs=num_epochs):\n",
    "    loss_values = []    \n",
    "    losses = np.zeros(n_epochs)\n",
    "    correct = 0\n",
    "    for iter in range(n_epochs):\n",
    "        optimizer.zero_grad() \n",
    "        yPred = classifier(X)\n",
    "        loss = criterion(yPred,y)\n",
    "        loss_values.append(loss.detach().numpy())\n",
    "        #Gradient et rétropropagation\n",
    "        loss.backward()\n",
    "        #Mise à jour des poids\n",
    "        optimizer.step()\n",
    "        y2 = yPred>0.5\n",
    "        correct = (y2 == y).sum().item()/2\n",
    "    acc = 100 * correct / (X.shape[0])\n",
    "    return loss_values,acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c94d9a",
   "metadata": {},
   "source": [
    "On applique enfin le perceptron sur les jeux de données et on visualise les résultats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26920b60",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'num_labels' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m fig,axs \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m,figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m15\u001b[39m,\u001b[38;5;241m8\u001b[39m))\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i,name_train,name_test \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m ([\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m],fichiers_train,fichiers_test):\n\u001b[0;32m----> 3\u001b[0m     train_data,train_labels \u001b[38;5;241m=\u001b[39m \u001b[43mextract_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     test_data, test_labels \u001b[38;5;241m=\u001b[39m extract_data(name_test)\n\u001b[1;32m      6\u001b[0m     model \u001b[38;5;241m=\u001b[39m PMC(train_data\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])\n",
      "Cell \u001b[0;32mIn[1], line 25\u001b[0m, in \u001b[0;36mextract_data\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m     22\u001b[0m features_np \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmatrix(features)\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m     24\u001b[0m labels_np \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(labels)\u001b[38;5;241m.\u001b[39mastype(dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39muint8)\n\u001b[0;32m---> 25\u001b[0m labels_onehot \u001b[38;5;241m=\u001b[39m (np\u001b[38;5;241m.\u001b[39marange(\u001b[43mnum_labels\u001b[49m) \u001b[38;5;241m==\u001b[39m labels_np[:, \u001b[38;5;28;01mNone\u001b[39;00m])\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m features_np,labels_onehot\n",
      "\u001b[0;31mNameError\u001b[0m: name 'num_labels' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABMkAAAKZCAYAAACiDnxZAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABEvElEQVR4nO3dbWyd5XkH8Mtx8DGo2IRlcV5mmkFHaQskNCGeoQhRebUESpcPUzOokiziZbQZorG2khCIS2ljxgBFKqERKYx+KEtaBKhqojDqNaoonqImsURHAqKBJqtqk6zDzkIbE/vZhwrTw7EDx/E5frl/P+l8yJP78bnOLfv5S38/Pqciy7IsAAAAACBhU8Z6AAAAAAAYa0oyAAAAAJKnJAMAAAAgeUoyAAAAAJKnJAMAAAAgeUoyAAAAAJKnJAMAAAAgeUoyAAAAAJKnJAMAAAAgeUoyAAAAAJJXdEn205/+NBYvXhyzZ8+OioqKePbZZz/wnF27dsWnP/3pyOVy8bGPfSyeeOKJEYwKQArkDAClJGcAGE7RJdnx48dj3rx5sWnTpg+1/vXXX4/rrrsurrnmmujs7IyvfOUrcdNNN8Vzzz1X9LAATH5yBoBSkjMADKciy7JsxCdXVMQzzzwTS5YsGXbNHXfcEdu3b49f/OIXg8f+9m//Nt56663YuXPnSJ8agATIGQBKSc4A8MemlvoJOjo6oqmpKe9Yc3NzfOUrXxn2nBMnTsSJEycG/z0wMBC//e1v40/+5E+ioqKiVKMCJCPLsjh27FjMnj07pkyZ2G9PKWcAxh85I2cASqlUOVPykqyrqyvq6uryjtXV1UVvb2/87ne/izPPPLPgnLa2trjnnntKPRpA8g4fPhx/9md/NtZjnBY5AzB+yRkASmm0c6bkJdlIrF27NlpaWgb/3dPTE+edd14cPnw4ampqxnAygMmht7c36uvr4+yzzx7rUcaEnAEoLTkjZwBKqVQ5U/KSbObMmdHd3Z13rLu7O2pqaob8rUtERC6Xi1wuV3C8pqZGqACMosnwJx9yBmD8kjP55AzA6BrtnCn5GwQ0NjZGe3t73rHnn38+GhsbS/3UACRAzgBQSnIGIB1Fl2T/93//F52dndHZ2RkRf/hI5M7Ozjh06FBE/OHW4uXLlw+uv/XWW+PgwYPx1a9+NQ4cOBCPPPJIfP/734/Vq1ePzisAYFKRMwCUkpwBYDhFl2Q///nP47LLLovLLrssIiJaWlrisssui/Xr10dExG9+85vBgImI+PM///PYvn17PP/88zFv3rx48MEH4zvf+U40NzeP0ksAYDKRMwCUkpwBYDgVWZZlYz3EB+nt7Y3a2tro6enxN/wAo8B1NZ/9ABhdrqv57AfA6CrVdbXk70kGAAAAAOOdkgwAAACA5CnJAAAAAEiekgwAAACA5CnJAAAAAEiekgwAAACA5CnJAAAAAEiekgwAAACA5CnJAAAAAEiekgwAAACA5CnJAAAAAEiekgwAAACA5CnJAAAAAEiekgwAAACA5CnJAAAAAEiekgwAAACA5CnJAAAAAEiekgwAAACA5CnJAAAAAEiekgwAAACA5CnJAAAAAEiekgwAAACA5CnJAAAAAEiekgwAAACA5CnJAAAAAEiekgwAAACA5CnJAAAAAEiekgwAAACA5CnJAAAAAEiekgwAAACA5CnJAAAAAEiekgwAAACA5CnJAAAAAEiekgwAAACA5CnJAAAAAEiekgwAAACA5CnJAAAAAEiekgwAAACA5CnJAAAAAEiekgwAAACA5CnJAAAAAEiekgwAAACA5CnJAAAAAEiekgwAAACA5CnJAAAAAEjeiEqyTZs2xdy5c6O6ujoaGhpi9+7dp1y/cePG+PjHPx5nnnlm1NfXx+rVq+P3v//9iAYGYPKTMwCUkpwBYChFl2Tbtm2LlpaWaG1tjb1798a8efOiubk53nzzzSHXP/nkk7FmzZpobW2N/fv3x2OPPRbbtm2LO++887SHB2DykTMAlJKcAWA4RZdkDz30UNx8882xcuXK+OQnPxmbN2+Os846Kx5//PEh17/44otx5ZVXxg033BBz586Nz33uc3H99dd/4G9rAEiTnAGglOQMAMMpqiTr6+uLPXv2RFNT03tfYMqUaGpqio6OjiHPueKKK2LPnj2DIXLw4MHYsWNHXHvttcM+z4kTJ6K3tzfvAcDkJ2cAKCU5A8CpTC1m8dGjR6O/vz/q6uryjtfV1cWBAweGPOeGG26Io0ePxmc+85nIsixOnjwZt9566ylvT25ra4t77rmnmNEAmATkDAClJGcAOJWSf7rlrl27YsOGDfHII4/E3r174+mnn47t27fHvffeO+w5a9eujZ6ensHH4cOHSz0mABOUnAGglOQMQDqKupNs+vTpUVlZGd3d3XnHu7u7Y+bMmUOec/fdd8eyZcvipptuioiISy65JI4fPx633HJLrFu3LqZMKezpcrlc5HK5YkYDYBKQMwCUkpwB4FSKupOsqqoqFixYEO3t7YPHBgYGor29PRobG4c85+233y4IjsrKyoiIyLKs2HkBmMTkDAClJGcAOJWi7iSLiGhpaYkVK1bEwoULY9GiRbFx48Y4fvx4rFy5MiIili9fHnPmzIm2traIiFi8eHE89NBDcdlll0VDQ0O89tprcffdd8fixYsHwwUA3iVnACglOQPAcIouyZYuXRpHjhyJ9evXR1dXV8yfPz927tw5+OaXhw4dyvtNy1133RUVFRVx1113xa9//ev40z/901i8eHF885vfHL1XAcCkIWcAKCU5A8BwKrIJcI9wb29v1NbWRk9PT9TU1Iz1OAATnutqPvsBMLpcV/PZD4DRVarrask/3RIAAAAAxjslGQAAAADJU5IBAAAAkDwlGQAAAADJU5IBAAAAkDwlGQAAAADJU5IBAAAAkDwlGQAAAADJU5IBAAAAkDwlGQAAAADJU5IBAAAAkDwlGQAAAADJU5IBAAAAkDwlGQAAAADJU5IBAAAAkDwlGQAAAADJU5IBAAAAkDwlGQAAAADJU5IBAAAAkDwlGQAAAADJU5IBAAAAkDwlGQAAAADJU5IBAAAAkDwlGQAAAADJU5IBAAAAkDwlGQAAAADJU5IBAAAAkDwlGQAAAADJU5IBAAAAkDwlGQAAAADJU5IBAAAAkDwlGQAAAADJU5IBAAAAkDwlGQAAAADJU5IBAAAAkDwlGQAAAADJU5IBAAAAkDwlGQAAAADJU5IBAAAAkDwlGQAAAADJU5IBAAAAkDwlGQAAAADJU5IBAAAAkDwlGQAAAADJU5IBAAAAkLwRlWSbNm2KuXPnRnV1dTQ0NMTu3btPuf6tt96KVatWxaxZsyKXy8WFF14YO3bsGNHAAEx+cgaAUpIzAAxlarEnbNu2LVpaWmLz5s3R0NAQGzdujObm5njllVdixowZBev7+vrir/7qr2LGjBnx1FNPxZw5c+JXv/pVnHPOOaMxPwCTjJwBoJTkDADDqciyLCvmhIaGhrj88svj4YcfjoiIgYGBqK+vj9tuuy3WrFlTsH7z5s3xL//yL3HgwIE444wzRjRkb29v1NbWRk9PT9TU1IzoawDwnvF8XZUzABPfeL6uyhmAia9U19Wi/tyyr68v9uzZE01NTe99gSlToqmpKTo6OoY854c//GE0NjbGqlWroq6uLi6++OLYsGFD9Pf3D/s8J06ciN7e3rwHAJOfnAGglOQMAKdSVEl29OjR6O/vj7q6urzjdXV10dXVNeQ5Bw8ejKeeeir6+/tjx44dcffdd8eDDz4Y3/jGN4Z9nra2tqitrR181NfXFzMmABOUnAGglOQMAKdS8k+3HBgYiBkzZsSjjz4aCxYsiKVLl8a6deti8+bNw56zdu3a6OnpGXwcPny41GMCMEHJGQBKSc4ApKOoN+6fPn16VFZWRnd3d97x7u7umDlz5pDnzJo1K84444yorKwcPPaJT3wiurq6oq+vL6qqqgrOyeVykcvlihkNgElAzgBQSnIGgFMp6k6yqqqqWLBgQbS3tw8eGxgYiPb29mhsbBzynCuvvDJee+21GBgYGDz26quvxqxZs4YMFADSJWcAKCU5A8CpFP3nli0tLbFly5b47ne/G/v3748vfelLcfz48Vi5cmVERCxfvjzWrl07uP5LX/pS/Pa3v43bb789Xn311di+fXts2LAhVq1aNXqvAoBJQ84AUEpyBoDhFPXnlhERS5cujSNHjsT69eujq6sr5s+fHzt37hx888tDhw7FlCnvdW/19fXx3HPPxerVq+PSSy+NOXPmxO233x533HHH6L0KACYNOQNAKckZAIZTkWVZNtZDfJDe3t6ora2Nnp6eqKmpGetxACY819V89gNgdLmu5rMfAKOrVNfVkn+6JQAAAACMd0oyAAAAAJKnJAMAAAAgeUoyAAAAAJKnJAMAAAAgeUoyAAAAAJKnJAMAAAAgeUoyAAAAAJKnJAMAAAAgeUoyAAAAAJKnJAMAAAAgeUoyAAAAAJKnJAMAAAAgeUoyAAAAAJKnJAMAAAAgeUoyAAAAAJKnJAMAAAAgeUoyAAAAAJKnJAMAAAAgeUoyAAAAAJKnJAMAAAAgeUoyAAAAAJKnJAMAAAAgeUoyAAAAAJKnJAMAAAAgeUoyAAAAAJKnJAMAAAAgeUoyAAAAAJKnJAMAAAAgeUoyAAAAAJKnJAMAAAAgeUoyAAAAAJKnJAMAAAAgeUoyAAAAAJKnJAMAAAAgeUoyAAAAAJKnJAMAAAAgeUoyAAAAAJKnJAMAAAAgeUoyAAAAAJKnJAMAAAAgeUoyAAAAAJKnJAMAAAAgeUoyAAAAAJKnJAMAAAAgeSMqyTZt2hRz586N6urqaGhoiN27d3+o87Zu3RoVFRWxZMmSkTwtAImQMwCUmqwB4P2KLsm2bdsWLS0t0draGnv37o158+ZFc3NzvPnmm6c874033oh//Md/jKuuumrEwwIw+ckZAEpN1gAwlKJLsoceeihuvvnmWLlyZXzyk5+MzZs3x1lnnRWPP/74sOf09/fHF7/4xbjnnnvi/PPPP62BAZjc5AwApSZrABhKUSVZX19f7NmzJ5qamt77AlOmRFNTU3R0dAx73te//vWYMWNG3HjjjR/qeU6cOBG9vb15DwAmPzkDQKmVI2vkDMDEVFRJdvTo0ejv74+6urq843V1ddHV1TXkOS+88EI89thjsWXLlg/9PG1tbVFbWzv4qK+vL2ZMACYoOQNAqZUja+QMwMRU0k+3PHbsWCxbtiy2bNkS06dP/9DnrV27Nnp6egYfhw8fLuGUAExUcgaAUhtJ1sgZgIlpajGLp0+fHpWVldHd3Z13vLu7O2bOnFmw/pe//GW88cYbsXjx4sFjAwMDf3jiqVPjlVdeiQsuuKDgvFwuF7lcrpjRAJgE5AwApVaOrJEzABNTUXeSVVVVxYIFC6K9vX3w2MDAQLS3t0djY2PB+osuuiheeuml6OzsHHx8/vOfj2uuuSY6OzvddgxAHjkDQKnJGgCGU9SdZBERLS0tsWLFili4cGEsWrQoNm7cGMePH4+VK1dGRMTy5ctjzpw50dbWFtXV1XHxxRfnnX/OOedERBQcB4AIOQNA6ckaAIZSdEm2dOnSOHLkSKxfvz66urpi/vz5sXPnzsE3vjx06FBMmVLStzoDYBKTMwCUmqwBYCgVWZZlYz3EB+nt7Y3a2tro6emJmpqasR4HYMJzXc1nPwBGl+tqPvsBMLpKdV316xEAAAAAkqckAwAAACB5SjIAAAAAkqckAwAAACB5SjIAAAAAkqckAwAAACB5SjIAAAAAkqckAwAAACB5SjIAAAAAkqckAwAAACB5SjIAAAAAkqckAwAAACB5SjIAAAAAkqckAwAAACB5SjIAAAAAkqckAwAAACB5SjIAAAAAkqckAwAAACB5SjIAAAAAkqckAwAAACB5SjIAAAAAkqckAwAAACB5SjIAAAAAkqckAwAAACB5SjIAAAAAkqckAwAAACB5SjIAAAAAkqckAwAAACB5SjIAAAAAkqckAwAAACB5SjIAAAAAkqckAwAAACB5SjIAAAAAkqckAwAAACB5SjIAAAAAkqckAwAAACB5SjIAAAAAkqckAwAAACB5SjIAAAAAkqckAwAAACB5SjIAAAAAkqckAwAAACB5SjIAAAAAkqckAwAAACB5SjIAAAAAkjeikmzTpk0xd+7cqK6ujoaGhti9e/ewa7ds2RJXXXVVTJs2LaZNmxZNTU2nXA8AcgaAUpM1ALxf0SXZtm3boqWlJVpbW2Pv3r0xb968aG5ujjfffHPI9bt27Yrrr78+fvKTn0RHR0fU19fH5z73ufj1r3992sMDMPnIGQBKTdYAMJSKLMuyYk5oaGiIyy+/PB5++OGIiBgYGIj6+vq47bbbYs2aNR94fn9/f0ybNi0efvjhWL58+Yd6zt7e3qitrY2enp6oqakpZlwAhjCer6tyBmDiG+/X1XJnzXjfD4CJplTX1aLuJOvr64s9e/ZEU1PTe19gypRoamqKjo6OD/U13n777XjnnXfi3HPPHXbNiRMnore3N+8BwOQnZwAotXJkjZwBmJiKKsmOHj0a/f39UVdXl3e8rq4uurq6PtTXuOOOO2L27Nl5ofR+bW1tUVtbO/ior68vZkwAJig5A0CplSNr5AzAxFTWT7e87777YuvWrfHMM89EdXX1sOvWrl0bPT09g4/Dhw+XcUoAJio5A0CpfZiskTMAE9PUYhZPnz49Kisro7u7O+94d3d3zJw585TnPvDAA3HffffFj3/847j00ktPuTaXy0UulytmNAAmATkDQKmVI2vkDMDEVNSdZFVVVbFgwYJob28fPDYwMBDt7e3R2Ng47Hn3339/3HvvvbFz585YuHDhyKcFYFKTMwCUmqwBYDhF3UkWEdHS0hIrVqyIhQsXxqJFi2Ljxo1x/PjxWLlyZURELF++PObMmRNtbW0REfHP//zPsX79+njyySdj7ty5g3/n/5GPfCQ+8pGPjOJLAWAykDMAlJqsAWAoRZdkS5cujSNHjsT69eujq6sr5s+fHzt37hx848tDhw7FlCnv3aD27W9/O/r6+uJv/uZv8r5Oa2trfO1rXzu96QGYdOQMAKUmawAYSkWWZdlYD/FBent7o7a2Nnp6eqKmpmasxwGY8FxX89kPgNHluprPfgCMrlJdV8v66ZYAAAAAMB4pyQAAAABInpIMAAAAgOQpyQAAAABInpIMAAAAgOQpyQAAAABInpIMAAAAgOQpyQAAAABInpIMAAAAgOQpyQAAAABInpIMAAAAgOQpyQAAAABInpIMAAAAgOQpyQAAAABInpIMAAAAgOQpyQAAAABInpIMAAAAgOQpyQAAAABInpIMAAAAgOQpyQAAAABInpIMAAAAgOQpyQAAAABInpIMAAAAgOQpyQAAAABInpIMAAAAgOQpyQAAAABInpIMAAAAgOQpyQAAAABInpIMAAAAgOQpyQAAAABInpIMAAAAgOQpyQAAAABInpIMAAAAgOQpyQAAAABInpIMAAAAgOQpyQAAAABInpIMAAAAgOQpyQAAAABInpIMAAAAgOQpyQAAAABInpIMAAAAgOQpyQAAAABInpIMAAAAgOQpyQAAAABInpIMAAAAgOSNqCTbtGlTzJ07N6qrq6OhoSF27959yvU/+MEP4qKLLorq6uq45JJLYseOHSMaFoA0yBkASk3WAPB+RZdk27Zti5aWlmhtbY29e/fGvHnzorm5Od58880h17/44otx/fXXx4033hj79u2LJUuWxJIlS+IXv/jFaQ8PwOQjZwAoNVkDwFAqsizLijmhoaEhLr/88nj44YcjImJgYCDq6+vjtttuizVr1hSsX7p0aRw/fjx+9KMfDR77y7/8y5g/f35s3rz5Qz1nb29v1NbWRk9PT9TU1BQzLgBDGM/XVTkDMPGN9+tqubNmvO8HwERTquvq1GIW9/X1xZ49e2Lt2rWDx6ZMmRJNTU3R0dEx5DkdHR3R0tKSd6y5uTmeffbZYZ/nxIkTceLEicF/9/T0RMQfNgGA0/fu9bTI35OUnJwBmBzGa85ElCdr5AxAaZUqZ4oqyY4ePRr9/f1RV1eXd7yuri4OHDgw5DldXV1Dru/q6hr2edra2uKee+4pOF5fX1/MuAB8gP/5n/+J2trasR5jkJwBmFzGW85ElCdr5AxAeYx2zhRVkpXL2rVr835T89Zbb8VHP/rROHTo0LgL2bHQ29sb9fX1cfjwYbdrh/0Yij3JZz8K9fT0xHnnnRfnnnvuWI8yJuTMqfmZKWRP8tmPQvYkn5yRMx/Ez0w++5HPfhSyJ/lKlTNFlWTTp0+PysrK6O7uzjve3d0dM2fOHPKcmTNnFrU+IiKXy0Uulys4Xltb65vhj9TU1NiPP2I/CtmTfPaj0JQpI/qQ45KRM+OLn5lC9iSf/ShkT/KNt5yJKE/WyJkPz89MPvuRz34Usif5RjtnivpqVVVVsWDBgmhvbx88NjAwEO3t7dHY2DjkOY2NjXnrIyKef/75YdcDkC45A0CpyRoAhlP0n1u2tLTEihUrYuHChbFo0aLYuHFjHD9+PFauXBkREcuXL485c+ZEW1tbRETcfvvtcfXVV8eDDz4Y1113XWzdujV+/vOfx6OPPjq6rwSASUHOAFBqsgaAoRRdki1dujSOHDkS69evj66urpg/f37s3Llz8I0sDx06lHe72xVXXBFPPvlk3HXXXXHnnXfGX/zFX8Szzz4bF1988Yd+zlwuF62trUPespwi+5HPfhSyJ/nsR6HxvCdyZuzZj0L2JJ/9KGRP8o33/Sh31oz3/RgL9iSf/chnPwrZk3yl2o+KbDx+LjMAAAAAlNH4eydNAAAAACgzJRkAAAAAyVOSAQAAAJA8JRkAAAAAyRs3JdmmTZti7ty5UV1dHQ0NDbF79+5Trv/BD34QF110UVRXV8cll1wSO3bsKNOk5VHMfmzZsiWuuuqqmDZtWkybNi2ampo+cP8mmmK/P961devWqKioiCVLlpR2wDFQ7J689dZbsWrVqpg1a1bkcrm48MILJ9XPTbH7sXHjxvj4xz8eZ555ZtTX18fq1avj97//fZmmLa2f/vSnsXjx4pg9e3ZUVFTEs88++4Hn7Nq1Kz796U9HLpeLj33sY/HEE0+UfM5ykzP55EwhWZNPzuSTM++RM0OTM4VkTT45k0/OFJI17xmzrMnGga1bt2ZVVVXZ448/nv3Xf/1XdvPNN2fnnHNO1t3dPeT6n/3sZ1llZWV2//33Zy+//HJ21113ZWeccUb20ksvlXny0ih2P2644YZs06ZN2b59+7L9+/dnf/d3f5fV1tZm//3f/13myUuj2P141+uvv57NmTMnu+qqq7K//uu/Ls+wZVLsnpw4cSJbuHBhdu2112YvvPBC9vrrr2e7du3KOjs7yzx5aRS7H9/73veyXC6Xfe9738tef/317LnnnstmzZqVrV69usyTl8aOHTuydevWZU8//XQWEdkzzzxzyvUHDx7MzjrrrKylpSV7+eWXs29961tZZWVltnPnzvIMXAZyJp+cKSRr8smZfHImn5wpJGcKyZp8ciafnCkka/KNVdaMi5Js0aJF2apVqwb/3d/fn82ePTtra2sbcv0XvvCF7Lrrrss71tDQkP393/99Secsl2L34/1OnjyZnX322dl3v/vdUo1YViPZj5MnT2ZXXHFF9p3vfCdbsWLFpAqULCt+T7797W9n559/ftbX11euEcuq2P1YtWpV9tnPfjbvWEtLS3bllVeWdM6x8GEC5atf/Wr2qU99Ku/Y0qVLs+bm5hJOVl5yJp+cKSRr8smZfHJmeHLmD+RMIVmTT87kkzOFZM3wypk1Y/7nln19fbFnz55oamoaPDZlypRoamqKjo6OIc/p6OjIWx8R0dzcPOz6iWQk+/F+b7/9drzzzjtx7rnnlmrMshnpfnz961+PGTNmxI033liOMctqJHvywx/+MBobG2PVqlVRV1cXF198cWzYsCH6+/vLNXbJjGQ/rrjiitizZ8/g7csHDx6MHTt2xLXXXluWmcebyXxNjZAz7ydnCsmafHImn5w5fZP5mhohZ4Yia/LJmXxyppCsOX2jdV2dOppDjcTRo0ejv78/6urq8o7X1dXFgQMHhjynq6tryPVdXV0lm7NcRrIf73fHHXfE7NmzC75BJqKR7McLL7wQjz32WHR2dpZhwvIbyZ4cPHgw/uM//iO++MUvxo4dO+K1116LL3/5y/HOO+9Ea2trOcYumZHsxw033BBHjx6Nz3zmM5FlWZw8eTJuvfXWuPPOO8sx8rgz3DW1t7c3fve738WZZ545RpONDjmTT84UkjX55Ew+OXP65EyhyZwzEbLm/eRMPjlTSNacvtHKmjG/k4zRdd9998XWrVvjmWeeierq6rEep+yOHTsWy5Ytiy1btsT06dPHepxxY2BgIGbMmBGPPvpoLFiwIJYuXRrr1q2LzZs3j/VoY2LXrl2xYcOGeOSRR2Lv3r3x9NNPx/bt2+Pee+8d69Fg3Es9ZyJkzVDkTD45A6cn9ayRM4XkTCFZUxpjfifZ9OnTo7KyMrq7u/OOd3d3x8yZM4c8Z+bMmUWtn0hGsh/veuCBB+K+++6LH//4x3HppZeWcsyyKXY/fvnLX8Ybb7wRixcvHjw2MDAQERFTp06NV155JS644ILSDl1iI/kemTVrVpxxxhlRWVk5eOwTn/hEdHV1RV9fX1RVVZV05lIayX7cfffdsWzZsrjpppsiIuKSSy6J48ePxy233BLr1q2LKVPS+v3BcNfUmpqaCf/b/Qg5835yppCsySdn8smZ0ydnCk3mnImQNe8nZ/LJmUKy5vSNVtaM+a5VVVXFggULor29ffDYwMBAtLe3R2Nj45DnNDY25q2PiHj++eeHXT+RjGQ/IiLuv//+uPfee2Pnzp2xcOHCcoxaFsXux0UXXRQvvfRSdHZ2Dj4+//nPxzXXXBOdnZ1RX19fzvFLYiTfI1deeWW89tprg+EaEfHqq6/GrFmzJnygjGQ/3n777YLQeDdw//C+kGmZzNfUCDnzfnKmkKzJJ2fyyZnTN5mvqRFyZiiyJp+cySdnCsma0zdq19Wi3ua/RLZu3ZrlcrnsiSeeyF5++eXslltuyc4555ysq6sry7IsW7ZsWbZmzZrB9T/72c+yqVOnZg888EC2f//+rLW1dVJ9ZHKx+3HfffdlVVVV2VNPPZX95je/GXwcO3ZsrF7CqCp2P95vsn0STJYVvyeHDh3Kzj777Owf/uEfsldeeSX70Y9+lM2YMSP7xje+MVYvYVQVux+tra3Z2Wefnf3bv/1bdvDgwezf//3fswsuuCD7whe+MFYvYVQdO3Ys27dvX7Zv374sIrKHHnoo27dvX/arX/0qy7IsW7NmTbZs2bLB9e9+XPI//dM/Zfv37882bdo0oo9LHs/kTD45U0jW5JMz+eRMPjlTSM4UkjX55Ew+OVNI1uQbq6wZFyVZlmXZt771rey8887LqqqqskWLFmX/+Z//Ofh/V199dbZixYq89d///vezCy+8MKuqqso+9alPZdu3by/zxKVVzH589KMfzSKi4NHa2lr+wUuk2O+PPzbZAuVdxe7Jiy++mDU0NGS5XC47//zzs29+85vZyZMnyzx16RSzH++88072ta99Lbvggguy6urqrL6+Pvvyl7+c/e///m/5By+Bn/zkJ0NeE97dgxUrVmRXX311wTnz58/PqqqqsvPPPz/713/917LPXWpyJp+cKSRr8smZfHLmPXJmaHKmkKzJJ2fyyZlCsuY9Y5U1FVmW4H14AAAAAPBHxvw9yQAAAABgrCnJAAAAAEiekgwAAACA5CnJAAAAAEiekgwAAACA5CnJAAAAAEiekgwAAACA5CnJAAAAAEiekgwAAACA5CnJAAAAAEiekgwAAACA5CnJAAAAAEiekgwAAACA5CnJAAAAAEiekgwAAACA5CnJAAAAAEiekgwAAACA5CnJAAAAAEiekgwAAACA5CnJAAAAAEiekgwAAACA5CnJAAAAAEiekgwAAACA5CnJAAAAAEiekgwAAACA5CnJAAAAAEiekgwAAACA5CnJAAAAAEiekgwAAACA5CnJAAAAAEhe0SXZT3/601i8eHHMnj07Kioq4tlnn/3Ac3bt2hWf/vSnI5fLxcc+9rF44oknRjAqACmQMwCUkpwBYDhFl2THjx+PefPmxaZNmz7U+tdffz2uu+66uOaaa6KzszO+8pWvxE033RTPPfdc0cMCMPnJGQBKSc4AMJyKLMuyEZ9cURHPPPNMLFmyZNg1d9xxR2zfvj1+8YtfDB7727/923jrrbdi586dI31qABIgZwAoJTkDwB+bWuon6OjoiKamprxjzc3N8ZWvfGXYc06cOBEnTpwY/PfAwED89re/jT/5kz+JioqKUo0KkIwsy+LYsWMxe/bsmDJlYr89pZwBGH/kjJwBKKVS5UzJS7Kurq6oq6vLO1ZXVxe9vb3xu9/9Ls4888yCc9ra2uKee+4p9WgAyTt8+HD82Z/92ViPcVrkDMD4JWcAKKXRzpmSl2QjsXbt2mhpaRn8d09PT5x33nlx+PDhqKmpGcPJACaH3t7eqK+vj7PPPnusRxkTcgagtOSMnAEopVLlTMlLspkzZ0Z3d3fese7u7qipqRnyty4REblcLnK5XMHxmpoaoQIwiibDn3zIGYDxS87kkzMAo2u0c6bkbxDQ2NgY7e3teceef/75aGxsLPVTA5AAOQNAKckZgHQUXZL93//9X3R2dkZnZ2dE/OEjkTs7O+PQoUMR8Ydbi5cvXz64/tZbb42DBw/GV7/61Thw4EA88sgj8f3vfz9Wr149Oq8AgElFzgBQSnIGgOEUXZL9/Oc/j8suuywuu+yyiIhoaWmJyy67LNavXx8REb/5zW8GAyYi4s///M9j+/bt8fzzz8e8efPiwQcfjO985zvR3Nw8Si8BgMlEzgBQSnIGgOFUZFmWjfUQH6S3tzdqa2ujp6fH3/ADjALX1Xz2A2B0ua7msx8Ao6tU19WSvycZAAAAAIx3SjIAAAAAkqckAwAAACB5SjIAAAAAkqckAwAAACB5SjIAAAAAkqckAwAAACB5SjIAAAAAkqckAwAAACB5SjIAAAAAkqckAwAAACB5SjIAAAAAkqckAwAAACB5SjIAAAAAkqckAwAAACB5SjIAAAAAkqckAwAAACB5SjIAAAAAkqckAwAAACB5SjIAAAAAkqckAwAAACB5SjIAAAAAkqckAwAAACB5SjIAAAAAkqckAwAAACB5SjIAAAAAkqckAwAAACB5SjIAAAAAkqckAwAAACB5SjIAAAAAkqckAwAAACB5SjIAAAAAkqckAwAAACB5SjIAAAAAkqckAwAAACB5SjIAAAAAkqckAwAAACB5SjIAAAAAkqckAwAAACB5SjIAAAAAkqckAwAAACB5SjIAAAAAkqckAwAAACB5SjIAAAAAkqckAwAAACB5IyrJNm3aFHPnzo3q6upoaGiI3bt3n3L9xo0b4+Mf/3iceeaZUV9fH6tXr47f//73IxoYgMlPzgBQSnIGgKEUXZJt27YtWlpaorW1Nfbu3Rvz5s2L5ubmePPNN4dc/+STT8aaNWuitbU19u/fH4899lhs27Yt7rzzztMeHoDJR84AUEpyBoDhFF2SPfTQQ3HzzTfHypUr45Of/GRs3rw5zjrrrHj88ceHXP/iiy/GlVdeGTfccEPMnTs3Pve5z8X111//gb+tASBNcgaAUpIzAAynqJKsr68v9uzZE01NTe99gSlToqmpKTo6OoY854orrog9e/YMhsjBgwdjx44dce211w77PCdOnIje3t68BwCTn5wBoJTkDACnMrWYxUePHo3+/v6oq6vLO15XVxcHDhwY8pwbbrghjh49Gp/5zGciy7I4efJk3Hrrrae8PbmtrS3uueeeYkYDYBKQMwCUkpwB4FRK/umWu3btig0bNsQjjzwSe/fujaeffjq2b98e995777DnrF27Nnp6egYfhw8fLvWYAExQcgaAUpIzAOko6k6y6dOnR2VlZXR3d+cd7+7ujpkzZw55zt133x3Lli2Lm266KSIiLrnkkjh+/HjccsstsW7dupgypbCny+VykcvlihkNgElAzgBQSnIGgFMp6k6yqqqqWLBgQbS3tw8eGxgYiPb29mhsbBzynLfffrsgOCorKyMiIsuyYucFYBKTMwCUkpwB4FSKupMsIqKlpSVWrFgRCxcujEWLFsXGjRvj+PHjsXLlyoiIWL58ecyZMyfa2toiImLx4sXx0EMPxWWXXRYNDQ3x2muvxd133x2LFy8eDBcAeJecAaCU5AwAwym6JFu6dGkcOXIk1q9fH11dXTF//vzYuXPn4JtfHjp0KO83LXfddVdUVFTEXXfdFb/+9a/jT//0T2Px4sXxzW9+c/ReBQCThpwBoJTkDADDqcgmwD3Cvb29UVtbGz09PVFTUzPW4wBMeK6r+ewHwOhyXc1nPwBGV6muqyX/dEsAAAAAGO+UZAAAAAAkT0kGAAAAQPKUZAAAAAAkT0kGAAAAQPKUZAAAAAAkT0kGAAAAQPKUZAAAAAAkT0kGAAAAQPKUZAAAAAAkT0kGAAAAQPKUZAAAAAAkT0kGAAAAQPKUZAAAAAAkT0kGAAAAQPKUZAAAAAAkT0kGAAAAQPKUZAAAAAAkT0kGAAAAQPKUZAAAAAAkT0kGAAAAQPKUZAAAAAAkT0kGAAAAQPKUZAAAAAAkT0kGAAAAQPKUZAAAAAAkT0kGAAAAQPKUZAAAAAAkT0kGAAAAQPKUZAAAAAAkT0kGAAAAQPKUZAAAAAAkT0kGAAAAQPKUZAAAAAAkT0kGAAAAQPKUZAAAAAAkT0kGAAAAQPKUZAAAAAAkT0kGAAAAQPKUZAAAAAAkT0kGAAAAQPKUZAAAAAAkT0kGAAAAQPKUZAAAAAAkT0kGAAAAQPJGVJJt2rQp5s6dG9XV1dHQ0BC7d+8+5fq33norVq1aFbNmzYpcLhcXXnhh7NixY0QDAzD5yRkASknOADCUqcWesG3btmhpaYnNmzdHQ0NDbNy4MZqbm+OVV16JGTNmFKzv6+uLv/qrv4oZM2bEU089FXPmzIlf/epXcc4554zG/ABMMnIGgFKSMwAMpyLLsqyYExoaGuLyyy+Phx9+OCIiBgYGor6+Pm677bZYs2ZNwfrNmzfHv/zLv8SBAwfijDPOGNGQvb29UVtbGz09PVFTUzOirwHAe8bzdVXOAEx84/m6KmcAJr5SXVeL+nPLvr6+2LNnTzQ1Nb33BaZMiaampujo6BjynB/+8IfR2NgYq1atirq6urj44otjw4YN0d/fP+zznDhxInp7e/MeAEx+cgaAUpIzAJxKUSXZ0aNHo7+/P+rq6vKO19XVRVdX15DnHDx4MJ566qno7++PHTt2xN133x0PPvhgfOMb3xj2edra2qK2tnbwUV9fX8yYAExQcgaAUpIzAJxKyT/dcmBgIGbMmBGPPvpoLFiwIJYuXRrr1q2LzZs3D3vO2rVro6enZ/Bx+PDhUo8JwAQlZwAoJTkDkI6i3rh/+vTpUVlZGd3d3XnHu7u7Y+bMmUOeM2vWrDjjjDOisrJy8NgnPvGJ6Orqir6+vqiqqio4J5fLRS6XK2Y0ACYBOQNAKckZAE6lqDvJqqqqYsGCBdHe3j54bGBgINrb26OxsXHIc6688sp47bXXYmBgYPDYq6++GrNmzRoyUABIl5wBoJTkDACnUvSfW7a0tMSWLVviu9/9buzfvz++9KUvxfHjx2PlypUREbF8+fJYu3bt4PovfelL8dvf/jZuv/32ePXVV2P79u2xYcOGWLVq1ei9CgAmDTkDQCnJGQCGU9SfW0ZELF26NI4cORLr16+Prq6umD9/fuzcuXPwzS8PHToUU6a8173V19fHc889F6tXr45LL7005syZE7fffnvccccdo/cqAJg05AwApSRnABhORZZl2VgP8UF6e3ujtrY2enp6oqamZqzHAZjwXFfz2Q+A0eW6ms9+AIyuUl1XS/7plgAAAAAw3inJAAAAAEiekgwAAACA5CnJAAAAAEiekgwAAACA5CnJAAAAAEiekgwAAACA5CnJAAAAAEiekgwAAACA5CnJAAAAAEiekgwAAACA5CnJAAAAAEiekgwAAACA5CnJAAAAAEiekgwAAACA5CnJAAAAAEiekgwAAACA5CnJAAAAAEiekgwAAACA5CnJAAAAAEiekgwAAACA5CnJAAAAAEiekgwAAACA5CnJAAAAAEiekgwAAACA5CnJAAAAAEiekgwAAACA5CnJAAAAAEiekgwAAACA5CnJAAAAAEiekgwAAACA5CnJAAAAAEiekgwAAACA5CnJAAAAAEiekgwAAACA5CnJAAAAAEiekgwAAACA5CnJAAAAAEiekgwAAACA5CnJAAAAAEiekgwAAACA5CnJAAAAAEiekgwAAACA5CnJAAAAAEiekgwAAACA5I2oJNu0aVPMnTs3qquro6GhIXbv3v2hztu6dWtUVFTEkiVLRvK0ACRCzgBQarIGgPcruiTbtm1btLS0RGtra+zduzfmzZsXzc3N8eabb57yvDfeeCP+8R//Ma666qoRDwvA5CdnACg1WQPAUIouyR566KG4+eabY+XKlfHJT34yNm/eHGeddVY8/vjjw57T398fX/ziF+Oee+6J888//7QGBmBykzMAlJqsAWAoRZVkfX19sWfPnmhqanrvC0yZEk1NTdHR0THseV//+tdjxowZceONN36o5zlx4kT09vbmPQCY/OQMAKVWjqyRMwATU1El2dGjR6O/vz/q6uryjtfV1UVXV9eQ57zwwgvx2GOPxZYtWz7087S1tUVtbe3go76+vpgxAZig5AwApVaOrJEzABNTST/d8tixY7Fs2bLYsmVLTJ8+/UOft3bt2ujp6Rl8HD58uIRTAjBRyRkASm0kWSNnACamqcUsnj59elRWVkZ3d3fe8e7u7pg5c2bB+l/+8pfxxhtvxOLFiwePDQwM/OGJp06NV155JS644IKC83K5XORyuWJGA2ASkDMAlFo5skbOAExMRd1JVlVVFQsWLIj29vbBYwMDA9He3h6NjY0F6y+66KJ46aWXorOzc/Dx+c9/Pq655pro7Ox02zEAeeQMAKUmawAYTlF3kkVEtLS0xIoVK2LhwoWxaNGi2LhxYxw/fjxWrlwZERHLly+POXPmRFtbW1RXV8fFF1+cd/4555wTEVFwHAAi5AwApSdrABhK0SXZ0qVL48iRI7F+/fro6uqK+fPnx86dOwff+PLQoUMxZUpJ3+oMgElMzgBQarIGgKFUZFmWjfUQH6S3tzdqa2ujp6cnampqxnocgAnPdTWf/QAYXa6r+ewHwOgq1XXVr0cAAAAASJ6SDAAAAIDkKckAAAAASJ6SDAAAAIDkKckAAAAASJ6SDAAAAIDkKckAAAAASJ6SDAAAAIDkKckAAAAASJ6SDAAAAIDkKckAAAAASJ6SDAAAAIDkKckAAAAASJ6SDAAAAIDkKckAAAAASJ6SDAAAAIDkKckAAAAASJ6SDAAAAIDkKckAAAAASJ6SDAAAAIDkKckAAAAASJ6SDAAAAIDkKckAAAAASJ6SDAAAAIDkKckAAAAASJ6SDAAAAIDkKckAAAAASJ6SDAAAAIDkKckAAAAASJ6SDAAAAIDkKckAAAAASJ6SDAAAAIDkKckAAAAASJ6SDAAAAIDkKckAAAAASJ6SDAAAAIDkKckAAAAASJ6SDAAAAIDkKckAAAAASJ6SDAAAAIDkKckAAAAASJ6SDAAAAIDkKckAAAAASJ6SDAAAAIDkKckAAAAASN6ISrJNmzbF3Llzo7q6OhoaGmL37t3Drt2yZUtcddVVMW3atJg2bVo0NTWdcj0AyBkASk3WAPB+RZdk27Zti5aWlmhtbY29e/fGvHnzorm5Od58880h1+/atSuuv/76+MlPfhIdHR1RX18fn/vc5+LXv/71aQ8PwOQjZwAoNVkDwFAqsizLijmhoaEhLr/88nj44YcjImJgYCDq6+vjtttuizVr1nzg+f39/TFt2rR4+OGHY/ny5R/qOXt7e6O2tjZ6enqipqammHEBGMJ4vq7KGYCJb7xfV8udNeN9PwAmmlJdV4u6k6yvry/27NkTTU1N732BKVOiqakpOjo6PtTXePvtt+Odd96Jc889d9g1J06ciN7e3rwHAJOfnAGg1MqRNXIGYGIqqiQ7evRo9Pf3R11dXd7xurq66Orq+lBf44477ojZs2fnhdL7tbW1RW1t7eCjvr6+mDEBmKDkDAClVo6skTMAE1NZP93yvvvui61bt8YzzzwT1dXVw65bu3Zt9PT0DD4OHz5cxikBmKjkDACl9mGyRs4ATExTi1k8ffr0qKysjO7u7rzj3d3dMXPmzFOe+8ADD8R9990XP/7xj+PSSy895dpcLhe5XK6Y0QCYBOQMAKVWjqyRMwATU1F3klVVVcWCBQuivb198NjAwEC0t7dHY2PjsOfdf//9ce+998bOnTtj4cKFI58WgElNzgBQarIGgOEUdSdZRERLS0usWLEiFi5cGIsWLYqNGzfG8ePHY+XKlRERsXz58pgzZ060tbVFRMQ///M/x/r16+PJJ5+MuXPnDv6d/0c+8pH4yEc+MoovBYDJQM4AUGqyBoChFF2SLV26NI4cORLr16+Prq6umD9/fuzcuXPwjS8PHToUU6a8d4Pat7/97ejr64u/+Zu/yfs6ra2t8bWvfe30pgdg0pEzAJSarAFgKBVZlmVjPcQH6e3tjdra2ujp6YmampqxHgdgwnNdzWc/AEaX62o++wEwukp1XS3rp1sCAAAAwHikJAMAAAAgeUoyAAAAAJKnJAMAAAAgeUoyAAAAAJKnJAMAAAAgeUoyAAAAAJKnJAMAAAAgeUoyAAAAAJKnJAMAAAAgeUoyAAAAAJKnJAMAAAAgeUoyAAAAAJKnJAMAAAAgeUoyAAAAAJKnJAMAAAAgeUoyAAAAAJKnJAMAAAAgeUoyAAAAAJKnJAMAAAAgeUoyAAAAAJKnJAMAAAAgeUoyAAAAAJKnJAMAAAAgeUoyAAAAAJKnJAMAAAAgeUoyAAAAAJKnJAMAAAAgeUoyAAAAAJKnJAMAAAAgeUoyAAAAAJKnJAMAAAAgeUoyAAAAAJKnJAMAAAAgeUoyAAAAAJKnJAMAAAAgeUoyAAAAAJKnJAMAAAAgeUoyAAAAAJKnJAMAAAAgeUoyAAAAAJKnJAMAAAAgeUoyAAAAAJKnJAMAAAAgeUoyAAAAAJI3opJs06ZNMXfu3Kiuro6GhobYvXv3Kdf/4Ac/iIsuuiiqq6vjkksuiR07doxoWADSIGcAKDVZA8D7FV2Sbdu2LVpaWqK1tTX27t0b8+bNi+bm5njzzTeHXP/iiy/G9ddfHzfeeGPs27cvlixZEkuWLIlf/OIXpz08AJOPnAGg1GQNAEOpyLIsK+aEhoaGuPzyy+Phhx+OiIiBgYGor6+P2267LdasWVOwfunSpXH8+PH40Y9+NHjsL//yL2P+/PmxefPmD/Wcvb29UVtbGz09PVFTU1PMuAAMYTxfV+UMwMQ33q+r5c6a8b4fABNNqa6rU4tZ3NfXF3v27Im1a9cOHpsyZUo0NTVFR0fHkOd0dHRES0tL3rHm5uZ49tlnh32eEydOxIkTJwb/3dPTExF/2AQATt+719Mif09ScnIGYHIYrzkTUZ6skTMApVWqnCmqJDt69Gj09/dHXV1d3vG6uro4cODAkOd0dXUNub6rq2vY52lra4t77rmn4Hh9fX0x4wLwAf7nf/4namtrx3qMQXIGYHIZbzkTUZ6skTMA5THaOVNUSVYua9euzftNzVtvvRUf/ehH49ChQ+MuZMdCb29v1NfXx+HDh92uHfZjKPYkn/0o1NPTE+edd16ce+65Yz3KmJAzp+ZnppA9yWc/CtmTfHJGznwQPzP57Ec++1HInuQrVc4UVZJNnz49Kisro7u7O+94d3d3zJw5c8hzZs6cWdT6iIhcLhe5XK7geG1trW+GP1JTU2M//oj9KGRP8tmPQlOmjOhDjktGzowvfmYK2ZN89qOQPck33nImojxZI2c+PD8z+exHPvtRyJ7kG+2cKeqrVVVVxYIFC6K9vX3w2MDAQLS3t0djY+OQ5zQ2Nuatj4h4/vnnh10PQLrkDAClJmsAGE7Rf27Z0tISK1asiIULF8aiRYti48aNcfz48Vi5cmVERCxfvjzmzJkTbW1tERFx++23x9VXXx0PPvhgXHfddbF169b4+c9/Ho8++ujovhIAJgU5A0CpyRoAhlJ0SbZ06dI4cuRIrF+/Prq6umL+/Pmxc+fOwTeyPHToUN7tbldccUU8+eSTcdddd8Wdd94Zf/EXfxHPPvtsXHzxxR/6OXO5XLS2tg55y3KK7Ec++1HInuSzH4XG857ImbFnPwrZk3z2o5A9yTfe96PcWTPe92Ms2JN89iOf/ShkT/KVaj8qsvH4ucwAAAAAUEbj7500AQAAAKDMlGQAAAAAJE9JBgAAAEDylGQAAAAAJG/clGSbNm2KuXPnRnV1dTQ0NMTu3btPuf4HP/hBXHTRRVFdXR2XXHJJ7Nixo0yTlkcx+7Fly5a46qqrYtq0aTFt2rRoamr6wP2baIr9/njX1q1bo6KiIpYsWVLaAcdAsXvy1ltvxapVq2LWrFmRy+XiwgsvnFQ/N8Xux8aNG+PjH/94nHnmmVFfXx+rV6+O3//+92WatrR++tOfxuLFi2P27NlRUVERzz777Aees2vXrvj0pz8duVwuPvaxj8UTTzxR8jnLTc7kkzOFZE0+OZNPzrxHzgxNzhSSNfnkTD45U0jWvGfMsiYbB7Zu3ZpVVVVljz/+ePZf//Vf2c0335ydc845WXd395Drf/azn2WVlZXZ/fffn7388svZXXfdlZ1xxhnZSy+9VObJS6PY/bjhhhuyTZs2Zfv27cv279+f/d3f/V1WW1ub/fd//3eZJy+NYvfjXa+//no2Z86c7Kqrrsr++q//ujzDlkmxe3LixIls4cKF2bXXXpu98MIL2euvv57t2rUr6+zsLPPkpVHsfnzve9/Lcrlc9r3vfS97/fXXs+eeey6bNWtWtnr16jJPXho7duzI1q1blz399NNZRGTPPPPMKdcfPHgwO+uss7KWlpbs5Zdfzr71rW9llZWV2c6dO8szcBnImXxyppCsySdn8smZfHKmkJwpJGvyyZl8cqaQrMk3VlkzLkqyRYsWZatWrRr8d39/fzZ79uysra1tyPVf+MIXsuuuuy7vWENDQ/b3f//3JZ2zXIrdj/c7efJkdvbZZ2ff/e53SzViWY1kP06ePJldccUV2Xe+851sxYoVkypQsqz4Pfn2t7+dnX/++VlfX1+5RiyrYvdj1apV2Wc/+9m8Yy0tLdmVV15Z0jnHwocJlK9+9avZpz71qbxjS5cuzZqbm0s4WXnJmXxyppCsySdn8smZ4cmZP5AzhWRNPjmTT84UkjXDK2fWjPmfW/b19cWePXuiqalp8NiUKVOiqakpOjo6hjyno6Mjb31ERHNz87DrJ5KR7Mf7vf322/HOO+/EueeeW6oxy2ak+/H1r389ZsyYETfeeGM5xiyrkezJD3/4w2hsbIxVq1ZFXV1dXHzxxbFhw4bo7+8v19glM5L9uOKKK2LPnj2Dty8fPHgwduzYEddee21ZZh5vJvM1NULOvJ+cKSRr8smZfHLm9E3ma2qEnBmKrMknZ/LJmUKy5vSN1nV16mgONRJHjx6N/v7+qKuryzteV1cXBw4cGPKcrq6uIdd3dXWVbM5yGcl+vN8dd9wRs2fPLvgGmYhGsh8vvPBCPPbYY9HZ2VmGCctvJHty8ODB+I//+I/44he/GDt27IjXXnstvvzlL8c777wTra2t5Ri7ZEayHzfccEMcPXo0PvOZz0SWZXHy5Mm49dZb48477yzHyOPOcNfU3t7e+N3vfhdnnnnmGE02OuRMPjlTSNbkkzP55MzpkzOFJnPORMia95Mz+eRMIVlz+kYra8b8TjJG13333Rdbt26NZ555Jqqrq8d6nLI7duxYLFu2LLZs2RLTp08f63HGjYGBgZgxY0Y8+uijsWDBgli6dGmsW7cuNm/ePNajjYldu3bFhg0b4pFHHom9e/fG008/Hdu3b4977713rEeDcS/1nImQNUORM/nkDJye1LNGzhSSM4VkTWmM+Z1k06dPj8rKyuju7s473t3dHTNnzhzynJkzZxa1fiIZyX6864EHHoj77rsvfvzjH8ell15ayjHLptj9+OUvfxlvvPFGLF68ePDYwMBARERMnTo1XnnllbjgggtKO3SJjeR7ZNasWXHGGWdEZWXl4LFPfOIT0dXVFX19fVFVVVXSmUtpJPtx9913x7Jly+Kmm26KiIhLLrkkjh8/HrfcckusW7cupkxJ6/cHw11Ta2pqJvxv9yPkzPvJmUKyJp+cySdnTp+cKTSZcyZC1ryfnMknZwrJmtM3Wlkz5rtWVVUVCxYsiPb29sFjAwMD0d7eHo2NjUOe09jYmLc+IuL5558fdv1EMpL9iIi4//774957742dO3fGwoULyzFqWRS7HxdddFG89NJL0dnZOfj4/Oc/H9dcc010dnZGfX19OccviZF8j1x55ZXx2muvDYZrRMSrr74as2bNmvCBMpL9ePvttwtC493A/cP7QqZlMl9TI+TM+8mZQrImn5zJJ2dO32S+pkbImaHImnxyJp+cKSRrTt+oXVeLepv/Etm6dWuWy+WyJ554Inv55ZezW265JTvnnHOyrq6uLMuybNmyZdmaNWsG1//sZz/Lpk6dmj3wwAPZ/v37s9bW1kn1kcnF7sd9992XVVVVZU899VT2m9/8ZvBx7NixsXoJo6rY/Xi/yfZJMFlW/J4cOnQoO/vss7N/+Id/yF555ZXsRz/6UTZjxozsG9/4xli9hFFV7H60trZmZ599dvZv//Zv2cGDB7N///d/zy644ILsC1/4wli9hFF17NixbN++fdm+ffuyiMgeeuihbN++fdmvfvWrLMuybM2aNdmyZcsG17/7ccn/9E//lO3fvz/btGnTiD4ueTyTM/nkTCFZk0/O5JMz+eRMITlTSNbkkzP55EwhWZNvrLJmXJRkWZZl3/rWt7Lzzjsvq6qqyhYtWpT953/+5+D/XX311dmKFSvy1n//+9/PLrzwwqyqqir71Kc+lW3fvr3ME5dWMfvx0Y9+NIuIgkdra2v5By+RYr8//thkC5R3FbsnL774YtbQ0JDlcrns/PPPz775zW9mJ0+eLPPUpVPMfrzzzjvZ1772teyCCy7Iqqurs/r6+uzLX/5y9r//+7/lH7wEfvKTnwx5TXh3D1asWJFdffXVBefMnz8/q6qqys4///zsX//1X8s+d6nJmXxyppCsySdn8smZ98iZocmZQrImn5zJJ2cKyZr3jFXWVGRZgvfhAQAAAMAfGfP3JAMAAACAsaYkAwAAACB5SjIAAAAAkqckAwAAACB5SjIAAAAAkqckAwAAACB5SjIAAAAAkqckAwAAACB5SjIAAAAAkqckAwAAACB5SjIAAAAAkqckAwAAACB5/w9SX4DP8AcVvQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 1500x800 with 6 Axes>"
      ]
     },
     "metadata": {
      "filenames": {
       "image/png": "/Users/vibarra/Documents/Cours/FISA/envbook/DLbook/_build/jupyter_execute/PMC_9_1.png"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig,axs = plt.subplots(2, 3,figsize=(15,8))\n",
    "for i,name_train,name_test in zip ([0,1,2],fichiers_train,fichiers_test):\n",
    "    train_data,train_labels = extract_data(name_train)\n",
    "    test_data, test_labels = extract_data(name_test)\n",
    "\n",
    "    model = PMC(train_data.shape[1])\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "    pltloss,acc = train_session(torch.FloatTensor(train_data),torch.FloatTensor(train_labels),model,loss,optimizer)\n",
    "    \n",
    "    titre= \"Précision ={0:5.3f} \".format(acc)\n",
    "    plotResults(axs[0][i],axs[1][i],test_data, test_labels, model, titre, pltloss, name_test)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "md:myst",
   "text_representation": {
    "extension": ".md",
    "format_name": "myst"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "source_map": [
   11,
   737,
   765,
   769,
   794,
   799,
   815,
   819,
   840,
   844
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}