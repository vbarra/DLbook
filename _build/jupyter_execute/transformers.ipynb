{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "419eb73d",
   "metadata": {},
   "source": [
    "# Transformers\n",
    "\n",
    "Nous avons précédemment présenté les réseaux convolutifs, qui sont\n",
    "spécialisés dans le traitement des données qui se trouvent sur une\n",
    "grille régulière. Ils sont particulièrement adaptés au traitement des\n",
    "images, qui comportent un très grand nombre de variables d'entrée, ce\n",
    "qui exclut l'utilisation de réseaux entièrement connectés. Chaque couche\n",
    "d'un réseau convolutif utilise le partage des paramètres de manière à ce\n",
    "que les zones locales de l'image soient traitées de la même manière à\n",
    "chaque position dans l'image.\n",
    "\n",
    "Nous introduisons ici les transformers, initialement destinés aux\n",
    "problèmes de traitement des langues naturelles, où l'entrée du réseau\n",
    "est une série d'encodages en grande dimension représentant des mots ou\n",
    "des fragments de mots. Les ensembles de données linguistiques partagent\n",
    "certaines des caractéristiques des données d'image. Le nombre de\n",
    "variables d'entrée peut être très important et les statistiques sont\n",
    "similaires à chaque position ; il n'est pas judicieux de réapprendre la\n",
    "signification du mot \\\"maison\\\" à chaque position possible dans un corps\n",
    "de texte. Cependant, les ensembles de données linguistiques présentent\n",
    "la complication suivante : les séquences de texte varient en longueur\n",
    "et, contrairement aux images, il n'existe pas de moyen facile de les\n",
    "redimensionner.\n",
    "\n",
    "## Motivation\n",
    "\n",
    "Pour motiver l'utilisation de transformers, considérons le texte suivant\n",
    ": \"Le restaurant a refusé de me servir un sandwich au jambon parce\n",
    "qu'il ne cuisine que des plats végétariens. Finalement, ils m'ont donné\n",
    "deux tranches de pain. L'ambiance était tout aussi bonne que la\n",
    "nourriture et le service.\"\n",
    "\n",
    "L'objectif est de concevoir un réseau pour traiter ce texte dans une\n",
    "représentation adaptée aux tâches ultérieures. Par exemple, il pourrait\n",
    "être utilisé pour classer l'avis comme positif ou négatif ou pour\n",
    "répondre à des questions telles que \"Le restaurant sert-il de la viande\n",
    "?\"\n",
    "\n",
    "Trois observations immédiates peuvent être effectuées :\n",
    "\n",
    "1. l'entrée codée peut être très grande. Dans ce cas, chacun des 41\n",
    "    mots pourrait être représenté par un vecteur de longueur 1024, de\n",
    "    sorte que l'entrée codée serait de longueur\n",
    "    41$\\times$1024=41984, même pour ce petit passage.\n",
    "    Un corps de texte de taille plus réaliste peut comporter des\n",
    "    centaines, voire des milliers de mots, de sorte que les réseaux\n",
    "    complètement connectés ne sont pas utilisables en pratique.\n",
    "\n",
    "2. l'une des caractéristiques des problèmes de reconnaissance\n",
    "    automatique du langage est que chaque entrée (une ou plusieurs\n",
    "    phrases) est de longueur différente ; il n'est donc même pas évident\n",
    "    d'appliquer un réseau complètement connecté. Ces observations\n",
    "    suggèrent que le réseau devrait partager des paramètres entre les\n",
    "    mots à différentes positions d'entrée, de la même manière que les\n",
    "    réseaux convolutifs partagent des paramètres entre différentes\n",
    "    positions d'image.\n",
    "\n",
    "3. le langage est ambigu : la syntaxe seule ne permet pas de savoir si\n",
    "    le pronom \"il\" fait référence au restaurant ou au sandwich au\n",
    "    jambon. Pour comprendre le texte, le mot \\\"il\\\" doit être relié\n",
    "    d'une manière ou d'une autre au mot \\\"restaurant\\\". Dans le langage\n",
    "    des transformers, le premier mot doit prêter attention au second.\n",
    "    Cela implique qu'il doit y avoir des liens entre les mots et que la\n",
    "    force de ces liens dépend des mots eux-mêmes. En outre, ces liens\n",
    "    doivent s'étendre sur de grandes parties du texte. \n",
    "    \n",
    "\n",
    "## Mécanisme d'attention\n",
    "\n",
    "### Mise en place\n",
    "\n",
    "Un modèle de traitement de texte (i) utilise le partage des paramètres\n",
    "pour traiter les longs passages d'entrée de différentes longueurs et\n",
    "(ii) contient des connexions entre les représentations de mots qui\n",
    "dépendent des mots eux-mêmes. Le transformer acquiert ces deux\n",
    "propriétés en utilisant un mécanisme d'(auto-)attention.\n",
    "\n",
    "Une couche standard d'un réseau de neurones prend en entrée un vecteur\n",
    "$\\mathbf x\\in\\mathbb R^d$ et calcule une sortie du type\n",
    "\n",
    "$$f(\\mathbf x) = ReLU(\\mathbf w^T\\mathbf x + b)$$\n",
    "\n",
    "Un bloc d'attention $A()$ prend $N$ entrées\n",
    "$\\mathbf x_1,\\cdots \\mathbf x_N$ de $\\mathbb R^d$ et renvoie $N$\n",
    "vecteurs de sortie de la même taille. Dans le contexte du traitement\n",
    "automatique du langage, chaque entrée représente un mot ou un fragment\n",
    "de mot. Tout d'abord, un ensemble de vecteurs est calculé pour chaque\n",
    "entrée\n",
    "\n",
    "$$\\forall i\\in[\\![1,N]\\!]\\; \\mathbf v_i=\\mathbf W_v^T\\mathbf x_i + \\mathbf b_v, \\mathbf b_v\\in \\mathbb R^d, \\mathbf W_v\\in\\mathcal{M}_d(\\mathbb R)$$\n",
    "\n",
    "Les poids et biais $\\mathbf W_v,\\mathbf b_v$ sont les mêmes pour toutes\n",
    "les entrées. Puis les vecteurs de sortie sont calculés par\n",
    "\n",
    "$$\\forall j\\in[\\![1,N]\\!]\\; \\mathbf A_j(\\mathbf x_1,\\cdots \\mathbf x_N) = \\displaystyle\\sum_{i=1}^N a(\\mathbf x_i,\\mathbf x_j)\\mathbf v_i$$\n",
    "\n",
    "le scalaire $a(\\mathbf x_i,\\mathbf x_j)$ étant l'attention que la\n",
    "$j$-ième sortie accorde à l'entrée $x_i$. Les $N$ valeurs\n",
    "$a(\\bullet ,\\mathbf x_j)$ sont positifs et de somme unité.\n",
    "\n",
    "Pour calculer l'attention, on définit deux transformations linéaires des\n",
    "entrées : \n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\forall i\\in[\\![1,N]\\!]\\; \\mathbf q_i&=&\\mathbf W_q^T\\mathbf x_i + \\mathbf b_q\\\\ \n",
    "\\mathbf k_i&=&\\mathbf W_k^T\\mathbf x_i + \\mathbf b_k\n",
    "\\end{aligned}$$ \n",
    "\n",
    "où les $\\mathbf q_i$ (respectivement $\\mathbf k_i$) sont\n",
    "les requêtes (resp. clés). Ces dénominations proviennent de la théorie\n",
    "des bases de données.\n",
    "\n",
    "Un produit scalaire entre requêtes et clés est alors effectué, et passé\n",
    "à un softmax (pour assurer la positivité et la somme à un)\n",
    "\n",
    "$$a(\\mathbf x_i ,\\mathbf x_j) = \\frac{exp(\\mathbf k_i^T\\mathbf q_j)}{\\displaystyle\\sum_{l=1}^N exp(\\mathbf k_l^T\\mathbf q_j)}$$\n",
    "\n",
    "Le produit scalaire entre requête et clé donne une mesure de similarité\n",
    "entre ces deux entités, et l'attention $a(\\bullet ,\\mathbf x_j)$ dépend\n",
    "donc de la similarité entre $q_j$ et toutes les clés.\n",
    "\n",
    "Cette approche permet d'avoir un jeu de paramètres partagé entre toutes\n",
    "les entrées ($\\mathbf W_v,\\mathbf b_v,\\mathbf W_q,\\mathbf b_q,\\mathbf W_k,\\mathbf b_k$),\n",
    "indépendant de $N$ et le réseau correspondant peut être appliqué à des\n",
    "entrées de longueur quelconque.\n",
    "\n",
    "### Formulation matricielle\n",
    "\n",
    "En formant la matrice $\\mathbf X\\in\\mathcal{M}_{d,N}(\\mathbb R)$ dont\n",
    "les colonnes sont les $\\mathbf x_i$, alors le mécanisme d'attention peut\n",
    "s'écrire : \n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\mathbf V(\\mathbf X) &=& \\mathbf b_v \\mathbf 1^T + \\mathbf W_v \\mathbf X\\\\\n",
    "\\mathbf Q(\\mathbf X) &=& \\mathbf b_q \\mathbf 1^T + \\mathbf W_q \\mathbf X\\\\\n",
    "\\mathbf K(\\mathbf X) &=& \\mathbf b_k \\mathbf 1^T + \\mathbf W_k \\mathbf X\\\\\n",
    "\\mathbf A(\\mathbf X) &=& \\mathbf V(\\mathbf X).Softmax(\\mathbf K(\\mathbf X)^T\\mathbf Q(\\mathbf X)) \n",
    "\\end{aligned}$$ \n",
    "\n",
    "$Softmax$ appliquant des fonctions softmax indépendantes\n",
    "sur chaque colonne de la matrice argument.\n",
    "\n",
    "### Extensions\n",
    "\n",
    "Le mécanisme précédent, dit d'auto-attention (self attention) est\n",
    "décliné en plusieurs variantes très utilisées en pratique.\n",
    "\n",
    "#### Encodage positionnel\n",
    "\n",
    "Le mécanisme d'auto-attention ne tient pas compte d'une information\n",
    "importante : le calcul est le même quel que soit l'ordre des entrées\n",
    "$\\mathbf x_i$. Plus précisément, il est équivariant par rapport aux\n",
    "permutations des entrées. Cependant, l'ordre est important lorsque les\n",
    "entrées correspondent aux mots d'une phrase. Il existe alors deux\n",
    "approches principales pour intégrer les informations de position :\n",
    "\n",
    "1.  l'encodage positionnel absolu : une matrice encodant l'information\n",
    "    positionnelle $\\mathbf P$ est ajoutée aux entrées $\\mathbf X$.\n",
    "    Chaque colonne de $\\mathbf P$ est unique et contient une information\n",
    "    de position de l'entrée correspondante. $\\mathbf P$ peut être\n",
    "    apprise ou fixée.\n",
    "\n",
    "2.  l'encodage positionnel relatif : l'entrée d'un mécanisme\n",
    "    d'auto-attention peut être une phrase entière, plusieurs phrases ou\n",
    "    un simple fragment de phrase, et la position absolue d'un mot est\n",
    "    beaucoup moins importante que la position relative entre deux\n",
    "    entrées. Si le système connaît la position absolue des deux entrées,\n",
    "    la position relative peut être déterminée, mais les codages\n",
    "    positionnels relatifs encodent directement cette information. Chaque\n",
    "    élément de la matrice d'attention correspond à un décalage\n",
    "    particulier entre la position $pq$ de la requête et la position $pk$\n",
    "    de la clé. Les codages positionnels relatifs apprennent un paramètre\n",
    "    $\\pi_{pq,pk}$ pour chaque décalage et l'utilisent pour modifier la\n",
    "    matrice d'attention en ajoutant ces valeurs, en les multipliant ou\n",
    "    en les utilisant pour modifier la matrice d'attention d'une autre\n",
    "    manière.\n",
    "\n",
    "#### Auto attention par produit scalaire mis à l'échelle\n",
    "\n",
    "Les produits scalaires dans le calcul de l'attention peuvent avoir de\n",
    "grandes amplitudes et déplacer les arguments de la fonction softmax dans\n",
    "une région où la plus grande valeur domine. De petites modifications des\n",
    "entrées de la fonction softmax ont désormais peu d'effet sur la sortie\n",
    "(les gradients sont très faibles), ce qui rend le modèle difficile à\n",
    "entraîner. Pour éviter cet inconvénient, les produits scalaires sont mis\n",
    "à l'échelle par la racine carrée de la dimension $d_q$ des requêtes et\n",
    "des clés :\n",
    "\n",
    "$$A(\\mathbf X) = \\mathbf V(\\mathbf X).Softmax\\left (\\frac{\\mathbf K(\\mathbf X)^T\\mathbf Q(\\mathbf X)}{\\sqrt{d_q}}\\right )$$\n",
    "\n",
    "#### Mécanisme d'auto attention multiple\n",
    "\n",
    "$H$ ensembles de requêtes et clés sont calculés \n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\mathbf V_h(\\mathbf X) &=& \\mathbf b_{vh} \\mathbf 1^T + \\mathbf W_{vh} \\mathbf X\\\\\n",
    "\\mathbf Q_h(\\mathbf X) &=& \\mathbf b_{qh} \\mathbf 1^T + \\mathbf W_{qh} \\mathbf X\\\\\n",
    "\\mathbf H_h(\\mathbf X) &=& \\mathbf b_{kh} \\mathbf 1^T + \\mathbf W_{kh} \\mathbf X\\\\\n",
    "\\mathbf A_h(\\mathbf X) &=& \\mathbf V_h(\\mathbf X).Softmax\\left (\\frac{\\mathbf K_h(\\mathbf X)^T\\mathbf Q_h(\\mathbf X)}{\\sqrt{d_q}}\\right) \n",
    "\\end{aligned}$$ \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "$\\mathbf A_h(\\mathbf X)$ est le $h$-ième mécanisme\n",
    "d'attention ou tête (head). Typiquement, si la dimension des entrées\n",
    "$\\mathbf x_i$ est $d$ et qu'il y a $H$ têtes, les valeurs, les requêtes\n",
    "et les clés seront toutes de taille $d/H$ pour une implémentation\n",
    "efficiente. Les sorties de ces mécanismes d'auto-attention sont\n",
    "concaténées verticalement, et une autre transformation linéaire\n",
    "$\\mathbf W_c$ est appliquée pour les combiner\n",
    "\n",
    "$$M_h\\mathbf A(X) = \\mathbf W_c [\\mathbf A_1(X)^T \\cdots \\mathbf A_H(X)^T]^T$$\n",
    "\n",
    "```{figure} ./images/multihead.png\n",
    ":name: multihead\n",
    "Mécanisme d'auto attention multiple.\n",
    "```\n",
    "\n",
    "Les têtes multiples semblent être nécessaires au bon fonctionnement du\n",
    "transformer, on pense qu'elles rendent le réseau d'auto-attention plus\n",
    "résistant aux mauvaises initialisations.\n",
    "\n",
    "### Transformers\n",
    "\n",
    "L'auto-attention n'est qu'une partie d'un mécanisme plus large : les\n",
    "transformers ({numref}`transformer`). Celui-ci se compose d'une unité d'auto-attention à\n",
    "plusieurs têtes (qui permet aux représentations de mots d'interagir les\n",
    "unes avec les autres) suivie d'un perceptron multicouches $MLP$ qui\n",
    "opère séparément sur chaque mot. Les deux unités sont des réseaux\n",
    "résiduels (leur sortie est ajoutée à l'entrée d'origine). En outre, il\n",
    "est courant d'ajouter une opération de normalisation de couche\n",
    "$LayerNorm$ après les réseaux d'auto-attention et les perceptrons\n",
    "multicouches. La séquence d'opérations complète peut être décrite par\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\mathbf X &=& \\mathbf X+ M_h\\mathbf A(X)\\\\ \n",
    "\\mathbf X &=& LayerNorm(\\mathbf X)\\\\\n",
    "\\mathbf x_i &=& \\mathbf x_i + MLP(\\mathbf x_i),\\; i\\in[\\![1,N]\\!]\\\\\n",
    "\\mathbf X &=& LayerNorm(\\mathbf X)\n",
    "\\end{aligned}$$\n",
    "\n",
    "\n",
    "```{figure} ./images/transformerblock.png\n",
    ":name: transformer\n",
    "Bloc transformer.\n",
    "```\n",
    "\n",
    "En pratique, les données passent par plusieurs de ces transformers.\n",
    "\n",
    "## Les transformers en traitement automatique du langage\n",
    "\n",
    "Une chaîne classique de traitement en classique en NLP commence par un\n",
    "tokenizer qui divise le texte en mots ou en fragments de mots. Chacun de\n",
    "ces tokens est ensuite mis en correspondance avec une représentation\n",
    "apprise. Ces représentations passent par une série de transformers.\n",
    "\n",
    "### Tokenizer\n",
    "\n",
    "Le texte est tout d'abord divisé en unités constitutives plus petites\n",
    "(jetons ou tokens) à partir d'un vocabulaire de jetons possibles. Ces\n",
    "jetons ne représentent pas nécessairement ces mots car :\n",
    "\n",
    "-   inévitablement, certains mots (par exemple, des noms propres) ne\n",
    "    figureront pas dans le vocabulaire.\n",
    "\n",
    "-   la manière de gérer la ponctuation n'est pas claire, mais elle est\n",
    "    importante. Si une phrase se termine par un point d'interrogation,\n",
    "    il faut encoder cette information.\n",
    "\n",
    "-   Le vocabulaire aurait besoin de différents jetons pour les versions\n",
    "    d'un même mot avec des suffixes différents (par exemple, marche,\n",
    "    marches, marché, marchés), et il n'y a aucun moyen de clarifier que\n",
    "    ces variations sont liées.\n",
    "\n",
    "Une approche consisterait à utiliser les lettres et les signes de\n",
    "ponctuation comme vocabulaire, mais cela impliquerait de découper le\n",
    "texte en très petites parties et d'exiger du réseau subséquent qu'il\n",
    "réapprenne les relations entre elles.\n",
    "\n",
    "Dans la pratique, un compromis entre les lettres et les mots complets\n",
    "est utilisé, et le vocabulaire final comprend à la fois des mots\n",
    "courants et des fragments de mots à partir desquels des mots plus grands\n",
    "et moins fréquents peuvent être composés. Le vocabulaire est calculé à\n",
    "l'aide d'un tokeniser de sous-mots tel que le codage par paires\n",
    "d'octets, qui fusionne de manière gloutonne les sous-chaînes les plus\n",
    "courantes en fonction de leur fréquence.\n",
    "\n",
    "### Représentation\n",
    "\n",
    "Chaque jeton du vocabulaire $V$ est associé à une représentation\n",
    "(embedding) de mot unique, et les représentations pour l'ensemble du\n",
    "vocabulaire sont stockées dans une matrice\n",
    "$\\mathbf W_e\\in\\mathcal{M}_{d,|V|}(\\mathbb{R})$. Pour ce faire, les $N$\n",
    "jetons d'entrée sont d'abord encodés dans une matrice\n",
    "$\\mathbf T \\in\\mathcal{M}_{|V|,N}(\\mathbb{R})$, où la $n$-ième colonne\n",
    "correspond au $n$-ième jeton et est un vecteur one-hot (un vecteur où\n",
    "chaque entrée est zéro, sauf l'entrée correspondant au jeton, de valeur\n",
    "1). Les représentations des entrées sont calculées sous la forme\n",
    "$\\mathbf X =\\mathbf W_e\\mathbf T$ et $\\mathbf W_e$ est appris comme\n",
    "n'importe quel autre paramètre du réseau. Une taille $d$ typique est de\n",
    "1024, et une taille totale de vocabulaire $|V|$ typique est de 30 000,\n",
    "donc ce modèle nécessite de nombreux paramètres à apprendre, avant même\n",
    "la mise en place des transformers.\n",
    "\n",
    "### Transformers\n",
    "\n",
    "Enfin, la matrice $X$ représentant le texte passe par une série de $K$\n",
    "transformers (transformer model). Il existe trois types de ces modèles,\n",
    "décrits dans les paragraphes suivants. Globalement, un encodeur\n",
    "transforme la représentation du texte en une représentation qui peut\n",
    "prendre en charge une variété de tâches. Un décodeur prédit le prochain\n",
    "jeton pour poursuivre le texte d'entrée. Les encodeurs-décodeurs sont\n",
    "utilisés dans les tâches de séquence à séquence, où une chaîne de texte\n",
    "est convertie en une autre (par exemple, traduction automatique).\n",
    "\n",
    "#### Exemple de modèle à encodeur : BERT\n",
    "\n",
    "[BERT](https://arxiv.org/abs/1810.04805) (Bidirectional Encoder Representations from Transformers) est un modèle d'encodeur qui utilise un vocabulaire de 30 000 mots.\n",
    "Les jetons d'entrée sont convertis en représentations de mots à 1024\n",
    "dimensions et passent par $K$=24 transformers. Chacun d'eux contient un\n",
    "mécanisme d'auto-attention avec 16 têtes. Les requêtes, les clés et les\n",
    "valeurs de chaque tête sont de dimension 64. La dimension de la couche\n",
    "cachée unique dans le réseau complètement connecté du transformer est de\n",
    "4096. Le nombre total de paramètres est de 340 millions. Lors de la\n",
    "publication de BERT, ce nombre était considéré comme élevé, mais il est\n",
    "aujourd'hui bien inférieur à celui des modèles les plus récents. Les\n",
    "modèles d'encodeurs comme BERT exploitent l'apprentissage par transfert.\n",
    "Pendant le préapprentissage, les paramètres de l'architecture du\n",
    "transformer sont appris par auto-supervision (pas besoin de labels) à\n",
    "partir d'un large corpus de texte. L'objectif est ici que le modèle\n",
    "apprenne des informations générales sur les statistiques de la langue.\n",
    "Au cours de la phase de fine tuning, le réseau résultant est adapté pour\n",
    "résoudre une tâche particulière à l'aide d'un plus petit corpus de\n",
    "données d'apprentissage supervisé.\n",
    "\n",
    "#### Exemple de modèle à décodeur : GPT3\n",
    "\n",
    "On présente ici une description de haut niveau de [GPT3](https://arxiv.org/pdf/2005.14165). L'architecture\n",
    "de base est très similaire à celle du modèle d'encodage et comprend une\n",
    "série de transformers qui opèrent sur les représentations de mots\n",
    "appris. Cependant, l'objectif est différent. L'encodeur vise à\n",
    "construire une représentation du texte qui peut être affinée pour\n",
    "résoudre une variété de tâches plus spécifiques. À l'inverse, le\n",
    "décodeur n'a qu'un seul objectif : générer le jeton suivant dans une\n",
    "séquence. Il peut générer un passage de texte cohérent en réinjectant la\n",
    "séquence étendue dans le modèle.\n",
    "\n",
    "**Modélisation du langage** : GPT3 construit un modèle linguistique autorégressif. Pour illustrer ce\n",
    "modèle, considérons la phrase $\\mathcal{P}$ = \\\"Henri mange beaucoup de\n",
    "viande le soir\\\". Pour simplifier, supposons que les jetons sont les\n",
    "mots complets. La probabilité de la phrase complète est :\n",
    "\n",
    "$$P(\\mathcal{P}) = P(\\textrm{Henri})P(\\textrm{mange}|\\textrm{Henri})P(\\textrm{beaucoup}|\\textrm{Henri mange})\\cdots P(\\textrm{soir}|\\textrm{Henri mange beaucoup de viande le })$$\n",
    "\n",
    "**Auto-attention masquée** : pour entraîner un décodeur, on maximise la log-probabilité du texte\n",
    "d'entrée dans le cadre du modèle autorégressif. L'idéal serait de\n",
    "transmettre la phrase entière et de calculer simultanément toutes les\n",
    "log-probabilités et tous les gradients. Cependant, cela pose un problème\n",
    ": si on transmet la phrase complète, le terme qui calcule\n",
    "$log P(\\textrm{beaucoup}|\\textrm{Henri mange})$ a accès à la fois à la\n",
    "réponse attendue \\\"beaucoup\\\" et au contexte suivant \\\"de viande le\n",
    "soir\\\". Par conséquent, le système peut tricher au lieu d'apprendre à\n",
    "prédire les mots suivants et ne s'entraînera pas correctement. Fort\n",
    "heureusement, les jetons n'interagissent que dans les couches\n",
    "d'auto-attention d'un réseau transformer. Le problème peut donc être\n",
    "résolu en s'assurant que l'attention portée à la réponse et au contexte\n",
    "est nulle. Pour ce faire, les produits scalaires correspondants dans le\n",
    "calcul de l'auto-attention sont réglés à $-\\infty$ avant d'être passés à\n",
    "la fonction softmax. C'est le principe de l'auto-attention masquée.\n",
    "L'ensemble du réseau du décodeur fonctionne comme suit. Le texte\n",
    "d'entrée est encodé en jetons, et les jetons sont convertis en\n",
    "embeddings. Ces derniers sont transmis au réseau de transformers, qui\n",
    "utilisent l'auto-attention masquée, de sorte qu'ils ne peuvent\n",
    "s'intéresser qu'aux jetons actuels et précédents. Chacune des\n",
    "représentations de sortie peut être considérée comme représentant une\n",
    "phrase partielle, et pour chacune d'entre elles, l'objectif est de\n",
    "prédire le jeton suivant dans la séquence.\n",
    "\n",
    "Après les transformers, une couche linéaire fait correspondre chaque\n",
    "représentation de mot à la taille du vocabulaire, suivie d'une fonction\n",
    "softmax qui convertit ces valeurs en probabilités. Pendant\n",
    "l'apprentissage, on cherche à maximiser la somme des log-probabilités de\n",
    "l'élément suivant dans la séquence de référence à chaque position en\n",
    "utilisant une fonction de perte d'entropie croisée multiclasse standard.\n",
    "\n",
    "**Génération de texte** : puisque ce modèle définit un modèle de probabilité sur des séquences\n",
    "de texte, il peut être utilisé pour échantillonner de nouveaux exemples\n",
    "de texte plausibles. Pour générer à partir du modèle, on débute par une\n",
    "séquence de texte en entrée (qui peut être simplement un jeton spécial\n",
    "$<$start$>$ indiquant le début de la séquence) et on l'introduit dans le\n",
    "réseau, qui produit alors les probabilités sur les jetons suivants\n",
    "possibles. On peut alors choisir le jeton le plus probable ou\n",
    "échantillonner à partir de la distribution de probabilités construite.\n",
    "La nouvelle séquence étendue peut être réinjectée dans le réseau du\n",
    "décodeur qui fournit la distribution de probabilités sur le jeton\n",
    "suivant. En répétant ce processus, on génère un texte entier.\n",
    "\n",
    "En pratique, de nombreuses stratégies peuvent rendre le texte de sortie\n",
    "plus cohérent. Par exemple, la recherche par faisceau tient compte des\n",
    "nombreux compléments de phrases possibles pour trouver le plus probable\n",
    "(qui n'est pas nécessairement trouvé en choisissant de manière gloutonne\n",
    "le mot suivant le plus probable à chaque étape). L'échantillonnage top-k\n",
    "tire aléatoirement le mot suivant parmi les $k$ possibilités les plus\n",
    "probables afin d'éviter que le système ne choisisse accidentellement\n",
    "dans la longue traîne des jetons à faible probabilité, ce qui conduirait\n",
    "à une impasse linguistique.\n",
    "\n",
    "GPT3 est un exemple de LLM (Large Language Model). La longueur des\n",
    "séquences est de 2048 jetons. Il y a $K$=96 transformers, chacun\n",
    "calculant une représentation de taille 12288. Dans les couches d'auto\n",
    "attention, on compte 96 têtes et la dimension des requêtes et des clés\n",
    "est de 128. Tout celà amène à un nombre de paramètres de 175 milliards,\n",
    "entraînés sur 300 milliards de jetons (taille d'un batch : 3.2 million\n",
    "de jetons)\n",
    "\n",
    "#### Exemple de modèle à encodeur-décodeur : traduction automatique\n",
    "\n",
    "La traduction entre langues est un exemple de tâche de séquence à\n",
    "séquence. Cette tâche nécessite un encodeur (pour calculer une bonne\n",
    "représentation de la phrase source) et un décodeur (pour générer la\n",
    "phrase dans la langue cible). Cette tâche peut être abordée à l'aide\n",
    "d'un modèle encodeur-décodeur. Prenons l'exemple d'une traduction de\n",
    "l'anglais vers le français. L'encodeur reçoit la phrase en anglais et la\n",
    "traite à travers une série de transformers pour créer une représentation\n",
    "de sortie pour chaque jeton. Pendant l'entraînement, le décodeur reçoit\n",
    "la traduction de référence en français et la fait passer par une série\n",
    "de transformers qui utilisent l'auto-attention masquée et prédisent le\n",
    "mot suivant à chaque position.\n",
    "\n",
    "Cependant, les couches du décodeur s'occupent également de la sortie de\n",
    "l'encodeur. Par conséquent, chaque mot français en sortie est\n",
    "conditionné par les mots précédents en sortie et par l'ensemble de la\n",
    "phrase anglaise qu'il traduit. Pour ce faire, on modifie les\n",
    "transformers du décodeur. Le transformer original du décodeur consistait\n",
    "en une couche d'auto-attention masquée suivie d'un réseau appliqué\n",
    "individuellement à chaque représentation. Une nouvelle couche\n",
    "d'auto-attention est alors ajoutée entre ces deux composants, dans\n",
    "laquelle les représentations du décodeur s'intéressent aux\n",
    "représentations de l'encodeur. Cette méthode utilise une version de\n",
    "l'auto-attention connue sous le nom d'attention encodeur-décodeur ou\n",
    "d'attention croisée, où les requêtes sont calculées à partir des\n",
    "représentations du décodeur et les clés et valeurs à partir des\n",
    "représentations de l'encodeur.\n",
    "\n",
    "## Les transformers en traitement d'images\n",
    "\n",
    "Le succès des transformers en traitement automatique du langage a conduit à se pencher sur leur\n",
    "utilisation sur des images. L'idée semblait incongrue, et ce pour deux\n",
    "raisons : il y a beaucoup plus de pixels dans une image que de mots dans\n",
    "une phrase, de sorte que la complexité quadratique de l'auto-attention\n",
    "constitue un goulot d'étranglement pratique. De plus, les réseaux\n",
    "convolutifs ont un bon biais inductif parce que chaque couche est\n",
    "équivariante à la translation spatiale et qu'ils prennent en compte la\n",
    "structure 2D de l'image. Ce qu'il faudrait apprendre dans un réseau\n",
    "transformer. Malgré cela, les réseaux transformers ont désormais éclipsé\n",
    "les performances des réseaux convolutifs pour entre autres la\n",
    "classification d'images. Cela s'explique en partie par l'échelle à\n",
    "laquelle ils peuvent être construits et par les grandes quantités de\n",
    "données qui peuvent être utilisées pour pré-entraîner les réseaux.\n",
    "\n",
    "### ImageGPT\n",
    "\n",
    "[ImageGPT](https://openai.com/index/image-gpt/) est un décodeur. Il construit un modèle autorégressif de pixels\n",
    "qui assimile une image partielle et prédit la valeur du pixel suivant.\n",
    "La complexité quadratique du réseau de transformers signifie que le plus\n",
    "grand modèle (qui contient 6,8 milliards de paramètres) ne peut\n",
    "fonctionner que sur des images de taille 64$\\times$64.\n",
    "En outre, pour que le système soit viable, l'espace colorimétrique RVB\n",
    "original de 24 bits a dû originellement être quantifié en un espace\n",
    "colorimétrique de neuf bits, de sorte que le système assimile (et\n",
    "prédit) l'un des 512 jetons possibles à chaque position. Les images sont\n",
    "naturellement des objets 2D, mais ImageGPT apprend simplement un codage\n",
    "positionnel différent pour chaque pixel. Il doit donc apprendre que\n",
    "chaque pixel a une relation étroite avec ses voisins précédents ainsi\n",
    "qu'avec les pixels voisins de la rangée supérieure.\n",
    "\n",
    "La représentation interne de ce décodeur a été utilisée comme base pour\n",
    "la classification des images. Les représentations finales des pixels sont moyennées et une couche linéaire les met en correspondance avec des\n",
    "activations qui passent par une couche softmax pour prédire les\n",
    "probabilités de classe. Le système est pré-entraîné sur un large corpus\n",
    "d'images web, puis affiné sur la base de données ImageNet redimensionnée\n",
    "à 48$\\times$48 pixels à l'aide d'une fonction de perte\n",
    "qui contient à la fois un terme d'entropie croisée pour la\n",
    "classification des images et un terme de perte générative pour la\n",
    "prédiction des pixels. Malgré l'utilisation d'une grande quantité de\n",
    "données d'apprentissage externes, le système a obtenu un taux d'erreur\n",
    "de 27,4% sur ImageNet. Ce taux est inférieur à celui des architectures\n",
    "convolutives de l'époque, mais reste impressionnant compte tenu de la\n",
    "petite taille de l'image d'entrée ; sans surprise, il ne parvient pas à\n",
    "classer les images où l'objet cible est petit ou mince.\n",
    "\n",
    "### ViT : Vision Transformer\n",
    "\n",
    "[ViT](https://arxiv.org/abs/2010.11929) s'est attaqué au problème de la résolution de l'image en divisant\n",
    "l'image en patchs de 16$\\times$16 . Chaque patch est\n",
    "mis en correspondance avec une dimension inférieure par le biais d'une\n",
    "transformation linéaire apprise, et ces représentations sont introduites\n",
    "dans le réseau du transformer. Les codages positionnels 1D standard sont\n",
    "appris. Il s'agit d'un modèle encodeur avec un jeton $<$cls$>$.\n",
    "Cependant, contrairement à BERT, il utilise un pré-entraînement\n",
    "supervisé sur une grande base de données de 303 millions d'images\n",
    "étiquetées provenant de 18 000 classes. Le jeton $<$cls$>$ est mappé via\n",
    "une couche finale du réseau pour créer des activations qui sont\n",
    "introduites dans une fonction softmax pour générer des probabilités de\n",
    "classe.\n",
    "\n",
    "Après le pré-entraînement, le système est appliqué en classification en\n",
    "remplaçant la couche finale par une couche qui correspond au nombre de\n",
    "classes souhaité et qui est ajustée par fine tuning. Ce système a obtenu\n",
    "un taux d'erreur de 11,45% pour le top 1 sur ImageNet. Cependant, il n'a\n",
    "pas obtenu d'aussi bons résultats que les meilleurs réseaux convolutifs\n",
    "contemporains sans pré-entraînement supervisé. Le fort biais inductif\n",
    "des réseaux convolutifs ne peut être surmonté que que par l'utilisation\n",
    "de quantités extrêmement importantes de données d'entraînement.\n",
    "\n",
    "\n",
    "## Implémentation\n",
    "Vu les temps d'entraînement et les bases de données nécessaires, il est impossible de proposer de faire tourner un transformer sans beaucoup de ressources.\n",
    "On se propose donc ici d'implémenter un Transformer de bout en bout, et plus particulièrement le travail séminal sur ce sujet {cite:p}`Vaswani17`. Dans la mesure du possible, on propose en illustration des codes les figures de l'article.\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import numpy as np\n",
    "```\n",
    "\n",
    "\n",
    "On implémente le mécanisme d’auto-attention multiple (multihead attention)\n",
    "\n",
    "```python\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d, H):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        assert d % H == 0, \n",
    "        \n",
    "        self.d = d\n",
    "        self.H = H\n",
    "        self.d_q = d // H\n",
    "        \n",
    "        self.W_q = nn.Linear(d, d)\n",
    "        self.W_k = nn.Linear(d, d)\n",
    "        self.W_v = nn.Linear(d, d)\n",
    "        self.W_o = nn.Linear(d, d)\n",
    "\n",
    "    # Calcul des attentions     \n",
    "    def Ah(self, Q, K, V, mask=None):\n",
    "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_q)\n",
    "        # Cas de l'auto-attention masquée\n",
    "        if mask is not None:\n",
    "            attn_scores = attn_scores.masked_fill(mask == 0, -1e10)\n",
    "        attn_probs = torch.softmax(attn_scores, dim=-1)\n",
    "        output = torch.matmul(attn_probs, V)\n",
    "        return output\n",
    "        \n",
    "    # Redimensionne le tenseur d'entrée pour les têtes\n",
    "    def tetes(self, x):\n",
    "        batch_size, N, d = x.size()\n",
    "        return x.view(batch_size, N, self.H, self.d_q).transpose(1, 2)\n",
    "\n",
    "    # Combine les attentions de toutes les têtes   \n",
    "    def concateneAttention(self, x):\n",
    "        batch_size, _, N, d_q = x.size()\n",
    "        return x.transpose(1, 2).contiguous().view(batch_size, N, self.d)\n",
    "\n",
    "    # Calcul l'auto-attention multiple. \n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        Q = self.tetes(self.W_q(Q))\n",
    "        K = self.tetes(self.W_k(K))\n",
    "        V = self.tetes(self.W_v(V))\n",
    "        \n",
    "        attn_output = self.Ah(Q, K, V, mask)\n",
    "        output = self.W_o(self.concateneAttention(attn_output))\n",
    "        return output\n",
    "```\n",
    "\n",
    "On s'intéresse ensuite au codage positionnel des mots. On encode la position d’un mot à l’aide des formules proposées dans {cite:p}`Vaswani17`. Pour ce faire, on créé une classe `PositionWiseFFN` qui permet de prendre en compte la position des éléments dans le calcul des prédictions.\n",
    "\n",
    "```python\n",
    "class PositionFFN(nn.Module):\n",
    "    def __init__(self, d, d_ff):\n",
    "        super(PositionWiseFeedForward, self).__init__()\n",
    "        self.fc1 = nn.Linear(d, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.relu(self.fc1(x)))\n",
    "```\n",
    "\n",
    "Cette classe est ensuite utilisée dans l'encodage positionnel à proprement parler. On initialise un tenseur de taille $d\\times max\\_N$ permettant de stocker les valeurs d'encodage.\n",
    "\n",
    "```python\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d, max_N):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        \n",
    "        # Codage de l'encodage proposé dans l'article\n",
    "        pe = torch.zeros(max_N, d)\n",
    "        p = torch.arange(0, max_N, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d, 2).float() * -(math.log(10000.0) / d))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(p * div_term)\n",
    "        pe[:, 1::2] = torch.cos(p * div_term)\n",
    "        \n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)]\n",
    "```\n",
    "\n",
    "On construit ensuite l'encodeur ({numref}`encodeur`) et le décodeur ({numref}`decodeur`). \n",
    "\n",
    "```{figure} ./images/encoder.png\n",
    ":name: encodeur\n",
    "Bloc encodeur (source : {cite:p}`Vaswani17`).\n",
    "```\n",
    "\n",
    "```python\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d, H, d_ff, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(d, H)\n",
    "        self.feed_forward = PositionWiseFeedForward(d, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d)\n",
    "        self.norm2 = nn.LayerNorm(d)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        attn_output = self.self_attn(x, x, x, mask)\n",
    "        # connexion résiduelle\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        ff_output = self.feed_forward(x)\n",
    "        # connexion résiduelle\n",
    "        x = self.norm2(x + self.dropout(ff_output))\n",
    "        return x\n",
    "```\n",
    "\n",
    "```{figure} ./images/decoder.png\n",
    ":name: decodeur\n",
    "Bloc décodeur (source : {cite:p}`Vaswani17`).\n",
    "```\n",
    "\n",
    "```python\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d, H, d_ff, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(d, H)\n",
    "        self.cross_attn = MultiHeadAttention(d, H)\n",
    "        self.feed_forward = PositionWiseFeedForward(d, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d)\n",
    "        self.norm2 = nn.LayerNorm(d)\n",
    "        self.norm3 = nn.LayerNorm(d)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, enc_output, src_mask, tgt_mask):\n",
    "        attn_output = self.self_attn(x, x, x, tgt_mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        attn_output = self.cross_attn(x, enc_output, enc_output, src_mask)\n",
    "        x = self.norm2(x + self.dropout(attn_output))\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm3(x + self.dropout(ff_output))\n",
    "        return x\n",
    "```\n",
    "et on assemble le tout en un bloc transformer ({numref}`transfo`) \n",
    "\n",
    "\n",
    "```{figure} ./images/transformer.png\n",
    ":name: transfo\n",
    "Bloc transformer (source : {cite:p}`Vaswani17`).\n",
    "```\n",
    "\n",
    "```python\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, taille_voc_source, taille_voc_cible, d, H, nb_layers, d_ff, max_N, dropout):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.encoder_embedding = nn.Embedding(taille_voc_source, d)\n",
    "        self.decoder_embedding = nn.Embedding(taille_voc_cible, d)\n",
    "        self.positional_encoding = PositionalEncoding(d, max_N)\n",
    "\n",
    "        self.encoder_layers = nn.ModuleList([EncoderLayer(d, H, d_ff, dropout) for _ in range(nb_layers)])\n",
    "        self.decoder_layers = nn.ModuleList([DecoderLayer(d, H, d_ff, dropout) for _ in range(nb_layers)])\n",
    "\n",
    "        self.fc = nn.Linear(d, taille_voc_cible)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def generate_mask(self, src, cible):\n",
    "        src_mask = (src != 0).unsqueeze(1).unsqueeze(2)\n",
    "        cible_mask = (cible != 0).unsqueeze(1).unsqueeze(3)\n",
    "        N = cible.size(1)\n",
    "        nopeak_mask = (1 - torch.triu(torch.ones(1, N, N), diagonal=1)).bool()\n",
    "        cible_mask = cible_mask & nopeak_mask\n",
    "        return src_mask, cible_mask\n",
    "\n",
    "    def forward(self, src, cible):\n",
    "        src_mask, cible_mask = self.generate_mask(src, cible)\n",
    "        src_embedded = self.dropout(self.positional_encoding(self.encoder_embedding(src)))\n",
    "        cible_embedded = self.dropout(self.positional_encoding(self.decoder_embedding(cible)))\n",
    "\n",
    "        enc_output = src_embedded\n",
    "        for enc_layer in self.encoder_layers:\n",
    "            enc_output = enc_layer(enc_output, src_mask)\n",
    "\n",
    "        dec_output = cible_embedded\n",
    "        for dec_layer in self.decoder_layers:\n",
    "            dec_output = dec_layer(dec_output, enc_output, src_mask, cible_mask)\n",
    "\n",
    "        output = self.fc(dec_output)\n",
    "        return output\n",
    "```\n",
    "\n",
    "On peut alors tester ce transformer sur un jeu de données (ici simple et de synthèse)\n",
    "\n",
    "```python\n",
    "taille_voc_source = 5000\n",
    "taille_voc_cible = 5000\n",
    "d = 512\n",
    "H = 8\n",
    "nb_layers = 6\n",
    "d_ff = 2048\n",
    "max_N = 100\n",
    "dropout = 0.1\n",
    "bath_size = 64\n",
    "\n",
    "transformer = Transformer(taille_voc_source, taille_voc_cible, d, H, nb_layers, d_ff, max_N, dropout)\n",
    "\n",
    "# Calcul du nombre de paramètres entraînables\n",
    "model_parameters = filter(lambda p: p.requires_grad, transformer.parameters())\n",
    "params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "print(params)\n",
    "\n",
    "# Génération de deux séquences\n",
    "src_data = torch.randint(1, taille_voc_source, (batch_size, max_N))  \n",
    "cible_data = torch.randint(1, taille_voc_cible, (batch_size, max_N))  \n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
    "\n",
    "l = []\n",
    "transformer.train()\n",
    "\n",
    "nb_epochs = 100\n",
    "for epoch in range(nb_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    output = transformer(src_data, cible_data[:, :-1])\n",
    "    loss = criterion(output.contiguous().view(-1, taille_voc_cible), cible_data[:, 1:].contiguous().view(-1))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    l.append(loss.item())\n",
    "    print(f\"Epoch: {epoch+1}, Loss: {loss.item()}\")\n",
    "\n",
    "plt.plot(np.arange(nb_epochs),l)\n",
    "```\n",
    "\n",
    "```{figure} ./images/plot.png\n",
    ":name: plot\n",
    "Fonction de perte.\n",
    "```\n",
    "\n",
    "```python\n",
    "src = torch.tensor([[0, 2, 5, 6, 4, 3, 9, 5, 2, 9, 10, 1]])\n",
    "trg = torch.tensor([[0]])\n",
    "print(src.shape,trg.shape)\n",
    "out = transformer.forward(src, trg)\n",
    "out\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "md:myst",
   "text_representation": {
    "extension": ".md",
    "format_name": "myst"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "source_map": [
   11
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}