{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e95a2833",
   "metadata": {},
   "source": [
    "# Utilisation de réseaux existants\n",
    "\n",
    "Nous présentons dans la suite quatre réseaux profonds classiques. Nous montrons ensuite comment les utiliser directement, ou comment les adapter pour répondre à une problématique précise, en lien avec leur utilisation originale ou non. Nous introduisons enfin une manière\n",
    "d'apprendre un réseau à partir de peu de données.\n",
    "\n",
    "## Quelques réseaux profonds classiques\n",
    "\n",
    "Les réseaux présentés ici ont prouvé leur efficacité, notamment lors des compétitions organisées depuis 2010 sur une base de données d'images nommée [ImageNet](http://www.image-net.org/). Initiée à l'Université de Stanford, cette base de données comporte aujourd'hui plus de 14 millions d'images, classées en 21841 catégories (avions, voitures, chats,...). Dans les compétitions [ILSVRC](https://image-net.org/challenges/LSVRC/) ( ImageNet Large Scale Visual Recognition Challenge), les chercheurs se voient proposer\n",
    "une extraction de 1,2 millions d'images d'entraînement, 100 000 images de test et 50 000 images de validation, catégorisées en 1000 classes. Le gagnant est celui qui atteint la meilleure précision de reconnaissance sur les 5 premières classes (top-5).\n",
    "\n",
    "La {numref}`perf` donne un aperçu des performances de plusieurs réseaux profonds suivant cette métrique.\n",
    "\n",
    "\n",
    "```{figure} ./images/classifImagenet.png\n",
    ":name: perf\n",
    "Performance de réseaux profonds sur une tache de classification ([source](https://theaisummer.com/cnn-architectures/))\n",
    "```\n",
    "\n",
    "\n",
    "### AlexNet\n",
    "\n",
    "En 2012, Krizhevsky et al  {cite:p}`Krizhevsky12` remportent ILSVRC avec un taux de reconnaissance de 84.6%, en utilisant AlexNet, un réseau convolutif composé de 5 couches de convolution et de pooling, suivies de 3 couches complètement connectées ({numref}`alexnet`).\n",
    "\n",
    "\n",
    "```{figure} ./images/AlexNet.png\n",
    ":name: alexnet\n",
    "Architecture du réseau AlexNet. Les couches de convolution\n",
    "et d’activation sont en orange clair, les couches d’agrégation en orange\n",
    "foncé. Les couches complètement connectées sont en violet.\n",
    "```\n",
    "\n",
    "Si la profondeur du réseau reste faible, le nombre de paramètres était\n",
    "déjà important. En regardant uniquement la première couche de\n",
    "convolution, on constate que :\n",
    "\n",
    "-   l'entrée est composée d'images 227$\\times$227$\\times$3\n",
    "-   les filtres de convolution sont de taille 11\n",
    "-   le pas de convolution (stride) est de 4\n",
    "\n",
    "Ainsi la sortie de la couche de convolution est de taille 55$\\times$55$\\times$96=290 400 neurones, chacun ayant\n",
    "11$\\times$11$\\times$3=363 poids et un biais. Cela implique, sur cette couche de convolution seulement, 105 705 600 paramètres à ajuster.\n",
    "\n",
    "Ce réseau, amélioration d'un réseau existant (LeNet), apportait de\n",
    "nombreuses contributions, comme l'utilisation de couches ReLU, de\n",
    "dropout, ou du GPU (NVIDIA GTX 580) pendant la phase d'entraînement.\n",
    "\n",
    "### VGG\n",
    "\n",
    "Les réseaux VGG (Visual Geometry Group, université d'Oxford)\n",
    "{cite:p}`Simonyan14` ont été les premiers réseaux à utiliser de petits filtres de convolution (3$\\times$3) et à les combiner pour\n",
    "décrire des séquences de convolution, l'idée étant d'émuler l'effet de\n",
    "larges champs réceptifs par cette séquence. Cette technique amène\n",
    "malheureusement à un nombre exponentiel de paramètres (le modèle\n",
    "entraîné qui peut être téléchargé a une taille de plus de 500 Mo). VGG a concouru à ILSVRC 2014, a obtenu un taux de bonne classification de\n",
    "92.3% mais n'a pas remporté le challenge. Aujourd'hui VGG et une famille de réseaux profonds (de A à E) qui varient par leur architecture\n",
    "\n",
    "::: center\n",
    "<figure id=\"F:VGG\">\n",
    "<style type=\"text/css\">\n",
    ".myTable {style=\"border:1px solid blue; border-collapse:collapse;\" }\n",
    ".myTable th { background-color:#000;color:white;width:50%; }\n",
    ".myTable td, .myTable th { padding:5px;border:1px solid #000; }\n",
    "</style>\n",
    "\n",
    "<table class=\"myTable\">\n",
    "<thead>\n",
    "<tr class=\"header\">\n",
    "<th style=\"text-align: center;\"><strong>A</strong></th>\n",
    "<th style=\"text-align: center;\"><strong><span>A-LRN</span></strong></th>\n",
    "<th style=\"text-align: center;\"><strong><span>B</span></strong></th>\n",
    "<th style=\"text-align: center;\"><strong><span>C</span></strong></th>\n",
    "<th style=\"text-align: center;\"><strong><span>D</span></strong></th>\n",
    "<th style=\"text-align: center;\"><strong><span>E</span></strong></th>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "<tr class=\"odd\">\n",
    "<td style=\"text-align: center;\">11 couches</td>\n",
    "<td style=\"text-align: center;\">11 couches</td>\n",
    "<td style=\"text-align: center;\">13 couches</td>\n",
    "<td style=\"text-align: center;\">16 couches</td>\n",
    "<td style=\"text-align: center;\">16 couches</td>\n",
    "<td style=\"text-align: center;\">19 couches</td>\n",
    "</tr>\n",
    "<tr class=\"even\">\n",
    "<td colspan=\"6\" style=\"text-align: center;\">Entrée : image 224<span\n",
    "class=\"math inline\">×</span>224 RGB</td>\n",
    "</tr>\n",
    "<tr class=\"odd\">\n",
    "<td rowspan=\"2\" style=\"text-align: center;\"></td>\n",
    "<td style=\"text-align: center;\">conv3-64</td>\n",
    "<td style=\"text-align: center;\">conv3-64</td>\n",
    "<td style=\"text-align: center;\">conv3-64</td>\n",
    "<td style=\"text-align: center;\">conv3-64</td>\n",
    "<td style=\"text-align: center;\">conv3-64</td>\n",
    "</tr>\n",
    "<tr class=\"even\">\n",
    "<td style=\"text-align: center;\">LRN</td>\n",
    "<td style=\"text-align: center;\">conv3-64</td>\n",
    "<td style=\"text-align: center;\">conv3-64</td>\n",
    "<td style=\"text-align: center;\">conv3-64</td>\n",
    "<td style=\"text-align: center;\">conv3-64</td>\n",
    "</tr>\n",
    "<tr class=\"odd\">\n",
    "<td colspan=\"6\" style=\"text-align: center;\">max pooling</td>\n",
    "</tr>\n",
    "<tr class=\"even\">\n",
    "<td rowspan=\"2\" style=\"text-align: center;\"></td>\n",
    "<td style=\"text-align: center;\">conv3-128</td>\n",
    "<td style=\"text-align: center;\">conv3-128</td>\n",
    "<td style=\"text-align: center;\">conv3-128</td>\n",
    "<td style=\"text-align: center;\">conv3-128</td>\n",
    "<td style=\"text-align: center;\">conv3-128</td>\n",
    "</tr>\n",
    "<tr class=\"odd\">\n",
    "<td style=\"text-align: center;\"></td>\n",
    "<td style=\"text-align: center;\">conv3-128</td>\n",
    "<td style=\"text-align: center;\">conv3-128</td>\n",
    "<td style=\"text-align: center;\">conv3-128</td>\n",
    "<td style=\"text-align: center;\">conv3-128</td>\n",
    "</tr>\n",
    "<tr class=\"even\">\n",
    "<td colspan=\"6\" style=\"text-align: center;\">max pooling</td>\n",
    "</tr>\n",
    "<tr class=\"odd\">\n",
    "<td rowspan=\"4\" style=\"text-align: center;\"></td>\n",
    "<td style=\"text-align: center;\">conv3-256</td>\n",
    "<td style=\"text-align: center;\">conv3-256</td>\n",
    "<td style=\"text-align: center;\">conv3-256</td>\n",
    "<td style=\"text-align: center;\">conv3-256</td>\n",
    "<td style=\"text-align: center;\">conv3-256</td>\n",
    "</tr>\n",
    "<tr class=\"even\">\n",
    "<td style=\"text-align: center;\">conv3-256</td>\n",
    "<td style=\"text-align: center;\">conv3-256</td>\n",
    "<td style=\"text-align: center;\">conv3-256</td>\n",
    "<td style=\"text-align: center;\">conv3-256</td>\n",
    "<td style=\"text-align: center;\">conv3-256</td>\n",
    "</tr>\n",
    "<tr class=\"odd\">\n",
    "<td style=\"text-align: center;\"></td>\n",
    "<td style=\"text-align: center;\"></td>\n",
    "<td style=\"text-align: center;\">conv1-256</td>\n",
    "<td style=\"text-align: center;\">conv3-256</td>\n",
    "<td style=\"text-align: center;\">conv3-256</td>\n",
    "</tr>\n",
    "<tr class=\"even\">\n",
    "<td style=\"text-align: center;\"></td>\n",
    "<td style=\"text-align: center;\"></td>\n",
    "<td style=\"text-align: center;\"></td>\n",
    "<td style=\"text-align: center;\"></td>\n",
    "<td style=\"text-align: center;\">conv3-256</td>\n",
    "</tr>\n",
    "<tr class=\"odd\">\n",
    "<td colspan=\"6\" style=\"text-align: center;\">max pooling</td>\n",
    "</tr>\n",
    "<tr class=\"even\">\n",
    "<td rowspan=\"4\" style=\"text-align: center;\"></td>\n",
    "<td style=\"text-align: center;\">conv3-512</td>\n",
    "<td style=\"text-align: center;\">conv3-512</td>\n",
    "<td style=\"text-align: center;\">conv3-512</td>\n",
    "<td style=\"text-align: center;\">conv3-512</td>\n",
    "<td style=\"text-align: center;\">conv3-512</td>\n",
    "</tr>\n",
    "<tr class=\"odd\">\n",
    "<td style=\"text-align: center;\">conv3-512</td>\n",
    "<td style=\"text-align: center;\">conv3-512</td>\n",
    "<td style=\"text-align: center;\">conv3-512</td>\n",
    "<td style=\"text-align: center;\">conv3-512</td>\n",
    "<td style=\"text-align: center;\">conv3-512</td>\n",
    "</tr>\n",
    "<tr class=\"even\">\n",
    "<td style=\"text-align: center;\"></td>\n",
    "<td style=\"text-align: center;\"></td>\n",
    "<td style=\"text-align: center;\">conv1-512</td>\n",
    "<td style=\"text-align: center;\">conv3-512</td>\n",
    "<td style=\"text-align: center;\">conv3-512</td>\n",
    "</tr>\n",
    "<tr class=\"odd\">\n",
    "<td style=\"text-align: center;\"></td>\n",
    "<td style=\"text-align: center;\"></td>\n",
    "<td style=\"text-align: center;\"></td>\n",
    "<td style=\"text-align: center;\"></td>\n",
    "<td style=\"text-align: center;\">conv3-512</td>\n",
    "</tr>\n",
    "<tr class=\"even\">\n",
    "<td colspan=\"6\" style=\"text-align: center;\">max pooling</td>\n",
    "</tr>\n",
    "<tr class=\"odd\">\n",
    "<td rowspan=\"4\" style=\"text-align: center;\"></td>\n",
    "<td style=\"text-align: center;\">conv3-512</td>\n",
    "<td style=\"text-align: center;\">conv3-512</td>\n",
    "<td style=\"text-align: center;\">conv3-512</td>\n",
    "<td style=\"text-align: center;\">conv3-512</td>\n",
    "<td style=\"text-align: center;\">conv3-512</td>\n",
    "</tr>\n",
    "<tr class=\"even\">\n",
    "<td style=\"text-align: center;\">conv3-512</td>\n",
    "<td style=\"text-align: center;\">conv3-512</td>\n",
    "<td style=\"text-align: center;\">conv3-512</td>\n",
    "<td style=\"text-align: center;\">conv3-512</td>\n",
    "<td style=\"text-align: center;\">conv3-512</td>\n",
    "</tr>\n",
    "<tr class=\"odd\">\n",
    "<td style=\"text-align: center;\"></td>\n",
    "<td style=\"text-align: center;\"></td>\n",
    "<td style=\"text-align: center;\">conv1-512</td>\n",
    "<td style=\"text-align: center;\">conv3-512</td>\n",
    "<td style=\"text-align: center;\">conv3-512</td>\n",
    "</tr>\n",
    "<tr class=\"even\">\n",
    "<td style=\"text-align: center;\"></td>\n",
    "<td style=\"text-align: center;\"></td>\n",
    "<td style=\"text-align: center;\"></td>\n",
    "<td style=\"text-align: center;\"></td>\n",
    "<td style=\"text-align: center;\">conv3-512</td>\n",
    "</tr>\n",
    "<tr class=\"odd\">\n",
    "<td colspan=\"6\" style=\"text-align: center;\">max pooling</td>\n",
    "</tr>\n",
    "<tr class=\"even\">\n",
    "<td colspan=\"6\" style=\"text-align: center;\">Couche complètement\n",
    "connectée 4096 neurones</td>\n",
    "</tr>\n",
    "<tr class=\"odd\">\n",
    "<td colspan=\"6\" style=\"text-align: center;\">Couche complètement\n",
    "connectée 4096 neurones</td>\n",
    "</tr>\n",
    "<tr class=\"even\">\n",
    "<td colspan=\"6\" style=\"text-align: center;\">Couche complètement\n",
    "connectée 1000 neurones</td>\n",
    "</tr>\n",
    "<tr class=\"odd\">\n",
    "<td colspan=\"6\" style=\"text-align: center;\">Classifieur softmax</td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>\n",
    "</figure>\n",
    ":::\n",
    "\n",
    "\n",
    "Le nombre de paramètres (en millions) pour les réseaux de A à E est 133, 133, 134, 138 et 144. Les réseaux VGG-D et VGG-E sont les plus précis et populaires.\n",
    "\n",
    "\n",
    "```{figure} ./images/VGG16.png\n",
    ":name: vgg16\n",
    "Réseau VGG16\n",
    "```\n",
    "\n",
    "### Inception\n",
    "\n",
    "Inception, proposé par Google, est le premier réseau dont les\n",
    "performances ont été augmentées non seulement en augmentant le nombre de couches, mais en pensant et optimisant le design et l'architecture.\n",
    "L'idée est ici d'utiliser plusieurs filtres, de tailles différentes, sur la même image et de concaténer les résultats pour générer une\n",
    "représentation plus robuste.\n",
    "\n",
    "Inception n'est pas un réseau, c'est une famille de réseaux : Network in Network {cite:p}`Lin13`, Inception V1 {cite:p}`Szegedy14`, Inception V2 {cite:p}`Szegedy15`,\n",
    "Xception {cite:p}`Chollet16`,...\n",
    "\n",
    "L'idée du premier réseau ({numref}`nin`) est de connecter les couches de convolution par des perceptrons multicouches, introduisant des non linéarités dans les réseaux profonds. Mathématiquement, ces perceptrons sont équivalents à des convolutions par des filtres 1$\\times 1$ et gardent donc la cohérence des réseaux. Cette nouvelle architecture rend moins indispensable les couches complètement connectées en fin de réseau. Les auteurs moyennent spatialement les cartes finales et donnent le résultat au classifier softmax. Le nombre de paramètres est alors réduit, diminuant de ce fait le risque de sur apprentissage.\n",
    "\n",
    "\n",
    "```{figure} ./images/NIN.png\n",
    ":name: nin\n",
    "Réseau Network in Network\n",
    "```\n",
    "\n",
    "Inception V1, implémenté dans le réseau GoogLeNet vainqueur d'ILSVRC\n",
    "2014, est une extension à des réseaux plus profonds de Network to\n",
    "Network. Le réseau est composé de 22 couches et atteint 93.3% de taux de reconnaissance. D'autres améliorations théoriques (fonctions de pertes associées aux couches intermédiaires dans la phase d'apprentissage, introduction de caractères épars dans le réseau) ont également permis d'améliorer les performances (de calcul et de classification).\n",
    "\n",
    "Inception V2, puis V3 ({numref}`inceptionv3`) adoptent des techniques de factorisation (toute convolution par un filtre de taille plus grande que 3$\\times$ 3 peut être exprimée de manière plus efficace\n",
    "avec une série de filtres de taille réduite) et de normalisation pour\n",
    "améliorer encore les performances.\n",
    "\n",
    "Inception V4 {cite:p}`Szegedy16` propose une version rationalisée, à\n",
    "l'architecture uniforme et aux performances accrues.\n",
    "\n",
    "```{figure} ./images/inceptionv3.png\n",
    ":name: inceptionv3\n",
    "Architecture d’inception V3\n",
    "```\n",
    "\n",
    "\n",
    "### ResNet\n",
    "\n",
    "En 2015, Microsoft remporte la compétition ILSVRC avec ResNet {cite:p}`He15`, un réseau à 152 couches qui utilise un module ResNet. Le taux de bonne reconnaissance est de 96.4%. Un réseau résiduel (ou ResNet) résout le problème de vanishing gradient de la manière la plus simple possible, en permettant des raccourcis entre chaque couche du réseau. Dans un réseau classique, l'activation en sortie de couche est de la forme $y=\\sigma(x)$, et lors de la rétropropagation, le gradient doit nécessairement repasser par $\\sigma(x)$, ce qui peut causer des\n",
    "problèmes en raison de la (forte) non linéarité induite par $\\sigma$.\n",
    "Dans un réseau résiduel, la sortie de chaque couche est calculée par\n",
    "$y=\\sigma()+x$, où $+x$ est le raccourci entre chaque couche, qui permet\n",
    "au gradient de transiter directement sans passer par $\\sigma$.\\\n",
    "Cette représentation donne l'idée générale, mais la réalité est un peu\n",
    "plus complexe, et prend la forme d'un module ResNet ({numref}`resnet`).\n",
    "\n",
    "\n",
    "```{figure} ./images/resNet.png\n",
    ":name: resnet\n",
    "Module ResNet (source : {cite:p}`He15`)\n",
    "```\n",
    "\n",
    "## Comment utiliser ces réseaux ?\n",
    "\n",
    "Il est possible de définir les réseaux classiques en décrivant une à une les couches et leur paramètres, qui sont proposés dans les articles correspondants. On imagine assez bien le travail que cela peut représenter sur ResNet par exemple...\n",
    "\n",
    "Fort heureusement, il existe d'autres manières d'utiliser ces réseaux.\n",
    "\n",
    "### Utilisation de réseaux pré-entraînés\n",
    "\n",
    "Il est possible de charger / sauvegarder des\n",
    "réseaux qui ont été entraînés sur des grandes bases de données et de les utiliser directement. Il est également possible, pendant\n",
    "l'entraînement, de créer des sauvegardes (checkpoints) pour reprendre\n",
    "éventuellement l'entraînement en cours d'itérations. On peut sauvegarder\n",
    "tout le réseau (architecture + optimiseur + poids), ou seulement les\n",
    "poids.\n",
    "\n",
    "`torchvision`donne accès à de nombreux modèles pré-entrainés :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ed636d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AlexNet',\n",
       " 'AlexNet_Weights',\n",
       " 'ConvNeXt',\n",
       " 'ConvNeXt_Base_Weights',\n",
       " 'ConvNeXt_Large_Weights',\n",
       " 'ConvNeXt_Small_Weights',\n",
       " 'ConvNeXt_Tiny_Weights',\n",
       " 'DenseNet',\n",
       " 'DenseNet121_Weights',\n",
       " 'DenseNet161_Weights',\n",
       " 'DenseNet169_Weights',\n",
       " 'DenseNet201_Weights',\n",
       " 'EfficientNet',\n",
       " 'EfficientNet_B0_Weights',\n",
       " 'EfficientNet_B1_Weights',\n",
       " 'EfficientNet_B2_Weights',\n",
       " 'EfficientNet_B3_Weights',\n",
       " 'EfficientNet_B4_Weights',\n",
       " 'EfficientNet_B5_Weights',\n",
       " 'EfficientNet_B6_Weights',\n",
       " 'EfficientNet_B7_Weights',\n",
       " 'EfficientNet_V2_L_Weights',\n",
       " 'EfficientNet_V2_M_Weights',\n",
       " 'EfficientNet_V2_S_Weights',\n",
       " 'GoogLeNet',\n",
       " 'GoogLeNetOutputs',\n",
       " 'GoogLeNet_Weights',\n",
       " 'Inception3',\n",
       " 'InceptionOutputs',\n",
       " 'Inception_V3_Weights',\n",
       " 'MNASNet',\n",
       " 'MNASNet0_5_Weights',\n",
       " 'MNASNet0_75_Weights',\n",
       " 'MNASNet1_0_Weights',\n",
       " 'MNASNet1_3_Weights',\n",
       " 'MaxVit',\n",
       " 'MaxVit_T_Weights',\n",
       " 'MobileNetV2',\n",
       " 'MobileNetV3',\n",
       " 'MobileNet_V2_Weights',\n",
       " 'MobileNet_V3_Large_Weights',\n",
       " 'MobileNet_V3_Small_Weights',\n",
       " 'RegNet',\n",
       " 'RegNet_X_16GF_Weights',\n",
       " 'RegNet_X_1_6GF_Weights',\n",
       " 'RegNet_X_32GF_Weights',\n",
       " 'RegNet_X_3_2GF_Weights',\n",
       " 'RegNet_X_400MF_Weights',\n",
       " 'RegNet_X_800MF_Weights',\n",
       " 'RegNet_X_8GF_Weights',\n",
       " 'RegNet_Y_128GF_Weights',\n",
       " 'RegNet_Y_16GF_Weights',\n",
       " 'RegNet_Y_1_6GF_Weights',\n",
       " 'RegNet_Y_32GF_Weights',\n",
       " 'RegNet_Y_3_2GF_Weights',\n",
       " 'RegNet_Y_400MF_Weights',\n",
       " 'RegNet_Y_800MF_Weights',\n",
       " 'RegNet_Y_8GF_Weights',\n",
       " 'ResNeXt101_32X8D_Weights',\n",
       " 'ResNeXt101_64X4D_Weights',\n",
       " 'ResNeXt50_32X4D_Weights',\n",
       " 'ResNet',\n",
       " 'ResNet101_Weights',\n",
       " 'ResNet152_Weights',\n",
       " 'ResNet18_Weights',\n",
       " 'ResNet34_Weights',\n",
       " 'ResNet50_Weights',\n",
       " 'ShuffleNetV2',\n",
       " 'ShuffleNet_V2_X0_5_Weights',\n",
       " 'ShuffleNet_V2_X1_0_Weights',\n",
       " 'ShuffleNet_V2_X1_5_Weights',\n",
       " 'ShuffleNet_V2_X2_0_Weights',\n",
       " 'SqueezeNet',\n",
       " 'SqueezeNet1_0_Weights',\n",
       " 'SqueezeNet1_1_Weights',\n",
       " 'SwinTransformer',\n",
       " 'Swin_B_Weights',\n",
       " 'Swin_S_Weights',\n",
       " 'Swin_T_Weights',\n",
       " 'Swin_V2_B_Weights',\n",
       " 'Swin_V2_S_Weights',\n",
       " 'Swin_V2_T_Weights',\n",
       " 'VGG',\n",
       " 'VGG11_BN_Weights',\n",
       " 'VGG11_Weights',\n",
       " 'VGG13_BN_Weights',\n",
       " 'VGG13_Weights',\n",
       " 'VGG16_BN_Weights',\n",
       " 'VGG16_Weights',\n",
       " 'VGG19_BN_Weights',\n",
       " 'VGG19_Weights',\n",
       " 'ViT_B_16_Weights',\n",
       " 'ViT_B_32_Weights',\n",
       " 'ViT_H_14_Weights',\n",
       " 'ViT_L_16_Weights',\n",
       " 'ViT_L_32_Weights',\n",
       " 'VisionTransformer',\n",
       " 'Weights',\n",
       " 'WeightsEnum',\n",
       " 'Wide_ResNet101_2_Weights',\n",
       " 'Wide_ResNet50_2_Weights',\n",
       " '_GoogLeNetOutputs',\n",
       " '_InceptionOutputs',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " '_api',\n",
       " '_meta',\n",
       " '_utils',\n",
       " 'alexnet',\n",
       " 'convnext',\n",
       " 'convnext_base',\n",
       " 'convnext_large',\n",
       " 'convnext_small',\n",
       " 'convnext_tiny',\n",
       " 'densenet',\n",
       " 'densenet121',\n",
       " 'densenet161',\n",
       " 'densenet169',\n",
       " 'densenet201',\n",
       " 'detection',\n",
       " 'efficientnet',\n",
       " 'efficientnet_b0',\n",
       " 'efficientnet_b1',\n",
       " 'efficientnet_b2',\n",
       " 'efficientnet_b3',\n",
       " 'efficientnet_b4',\n",
       " 'efficientnet_b5',\n",
       " 'efficientnet_b6',\n",
       " 'efficientnet_b7',\n",
       " 'efficientnet_v2_l',\n",
       " 'efficientnet_v2_m',\n",
       " 'efficientnet_v2_s',\n",
       " 'get_model',\n",
       " 'get_model_builder',\n",
       " 'get_model_weights',\n",
       " 'get_weight',\n",
       " 'googlenet',\n",
       " 'inception',\n",
       " 'inception_v3',\n",
       " 'list_models',\n",
       " 'maxvit',\n",
       " 'maxvit_t',\n",
       " 'mnasnet',\n",
       " 'mnasnet0_5',\n",
       " 'mnasnet0_75',\n",
       " 'mnasnet1_0',\n",
       " 'mnasnet1_3',\n",
       " 'mobilenet',\n",
       " 'mobilenet_v2',\n",
       " 'mobilenet_v3_large',\n",
       " 'mobilenet_v3_small',\n",
       " 'mobilenetv2',\n",
       " 'mobilenetv3',\n",
       " 'optical_flow',\n",
       " 'quantization',\n",
       " 'regnet',\n",
       " 'regnet_x_16gf',\n",
       " 'regnet_x_1_6gf',\n",
       " 'regnet_x_32gf',\n",
       " 'regnet_x_3_2gf',\n",
       " 'regnet_x_400mf',\n",
       " 'regnet_x_800mf',\n",
       " 'regnet_x_8gf',\n",
       " 'regnet_y_128gf',\n",
       " 'regnet_y_16gf',\n",
       " 'regnet_y_1_6gf',\n",
       " 'regnet_y_32gf',\n",
       " 'regnet_y_3_2gf',\n",
       " 'regnet_y_400mf',\n",
       " 'regnet_y_800mf',\n",
       " 'regnet_y_8gf',\n",
       " 'resnet',\n",
       " 'resnet101',\n",
       " 'resnet152',\n",
       " 'resnet18',\n",
       " 'resnet34',\n",
       " 'resnet50',\n",
       " 'resnext101_32x8d',\n",
       " 'resnext101_64x4d',\n",
       " 'resnext50_32x4d',\n",
       " 'segmentation',\n",
       " 'shufflenet_v2_x0_5',\n",
       " 'shufflenet_v2_x1_0',\n",
       " 'shufflenet_v2_x1_5',\n",
       " 'shufflenet_v2_x2_0',\n",
       " 'shufflenetv2',\n",
       " 'squeezenet',\n",
       " 'squeezenet1_0',\n",
       " 'squeezenet1_1',\n",
       " 'swin_b',\n",
       " 'swin_s',\n",
       " 'swin_t',\n",
       " 'swin_transformer',\n",
       " 'swin_v2_b',\n",
       " 'swin_v2_s',\n",
       " 'swin_v2_t',\n",
       " 'vgg',\n",
       " 'vgg11',\n",
       " 'vgg11_bn',\n",
       " 'vgg13',\n",
       " 'vgg13_bn',\n",
       " 'vgg16',\n",
       " 'vgg16_bn',\n",
       " 'vgg19',\n",
       " 'vgg19_bn',\n",
       " 'video',\n",
       " 'vision_transformer',\n",
       " 'vit_b_16',\n",
       " 'vit_b_32',\n",
       " 'vit_h_14',\n",
       " 'vit_l_16',\n",
       " 'vit_l_32',\n",
       " 'wide_resnet101_2',\n",
       " 'wide_resnet50_2']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchvision import models\n",
    "dir(models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13424f3",
   "metadata": {},
   "source": [
    "Il est alors facile de charger un tel réseau, par exemple\n",
    "\n",
    "```python\n",
    "resnet = models.resnet101(pretrained=True)\n",
    "```\n",
    "\n",
    "\n",
    "### Transfer learning et fine tuning\n",
    "\n",
    "Il est possible d'utiliser les réseaux classiques pré-entrainés pour de\n",
    "nouvelles tâches. L'idée sous-jacente et que les premières couches\n",
    "capturent des caractéristiques bas niveau, et que la sémantique vient\n",
    "avec les couches profondes. Ainsi, dans un problème de classification,\n",
    "où les classes n'ont pas été apprises, on peut supposer qu'en conservant les premières couches on extraira des caractéristiques communes des\n",
    "images, et qu'en changeant les dernières couches (information sémantique\n",
    "et haut niveau et étage de classification), c'est à dire en réapprenant\n",
    "les connexions, on spécifiera le nouveau réseau pour la nouvelle tâche\n",
    "de classification.\n",
    "\n",
    "Cette approche rentre dans le cadre des méthodes d'apprentissage par transfert (Transfer Learning)\n",
    "{cite:p}`Pan10` et de fine tuning, cas particulier d'adaptation de domaine.\n",
    "\n",
    "L'apprentissage par transfert comporte généralement deux étapes principales :\n",
    "\n",
    "- **Extraction des caractéristiques** : dans cette étape,le modèle pré-entraîné est utilisé comme un extracteur de caractéristiques fixes. On supprime les couches finales (MLP, responsable de la classification) et on les remplaçe par de nouvelles couches spécifiques à la tâche adressée ({numref}`tl2`). Les poids du modèle pré-entraîné sont gelés et seuls les poids des couches nouvellement ajoutées sont entraînés sur l'ensemble de données du problème.\n",
    "\n",
    "```{figure} ./images/tl2.png\n",
    ":name: tl2\n",
    "Réentrainement d'un classifieur sur des caractéristiques extraites.\n",
    "```\n",
    "\n",
    "\n",
    "- **fine tuning** : le fine tuning pousse le processus un peu plus loin en dégelant certaines des couches du modèle pré-entraîné et en leur permettant d'être mises à jour avec le nouvel ensemble de données. Cette étape permet au modèle de s'adapter et d'apprendre des caractéristiques plus spécifiques liées à la nouvelle tâche ou au nouveau domaine.\n",
    "\n",
    "\n",
    "Plusieurs facteurs influent sur le choix de la méthode à utiliser, parmi lesquels : \n",
    "\n",
    "- la taille des données d'apprentissage du nouveau problème ({numref}`tl`) \n",
    "\n",
    "```{figure} ./images/tl.png\n",
    ":name: tl\n",
    "Stratégies d'apprentissage par transfert.\n",
    "```\n",
    "\n",
    "- la ressemblance du nouveau jeu de données avec celui qui a servi à\n",
    "entraîner le réseau initial ({numref}`domaintask`).\n",
    "\n",
    "```{figure} ./images/domaintask.png\n",
    ":name: domaintask\n",
    "Changement de domaine / tâche.\n",
    "```\n",
    "\n",
    "Pour un jeu de données similaire de petite taille, on utilise du\n",
    "    transfer learning, avec un classifieur utilisé sur les\n",
    "    caractéristiques calculées sur les dernières couches du réseau\n",
    "    initial. Pour un jeu de données de petite taille et un problème différent, on utilise du transfer learning, avec un classifieur utilisé sur les caractéristiques calculées sur les premières couches du réseau initial. Pour un jeu de données, similaire ou non de grande taille, on\n",
    "    utilise le fine tuning\n",
    "\n",
    "A noter que, si peu de données sont disponibles sur la nouvelle tâche/le nouveau domaine, il est toujours possible :\n",
    "- d'augmenter la taille du jeu de données par des technique de \\\"Data Augmentation\\\" (changement de\n",
    "couleurs des pixels, rotations, cropping, homothéties, translations\\...) ({numref}`dataaugment`).\n",
    "\n",
    "```{figure} ./images/dataaugment.png\n",
    ":name: dataaugment\n",
    "Augmentation de données : à partir d'un exemple (image de gauche), on construit plusieurs autres exemples par rotation, flip, ajout de bruit, déformation, changement colorimétrique.\n",
    "```\n",
    "\n",
    "- d'utiliser plus généralement des méthodes de Few shot / Zero shot learning {cite:p}`Song23`, dont l'augmentation de données est un exemple.\n",
    "\n",
    "## Implémentation\n",
    "\n",
    "On propose ici d'implémenter deux stratégies : \n",
    "- une première d'entraînement d'un réseau en initialisant les poids à ceux du même réseau préentraîné sur ImageNet. Une couche de classification spécifique au problème est ajoutée, et tout le réseau est entraîné.\n",
    "- une seconde ({numref}`tl2`) qui remplace le réseau de classification du réseau préentraîné par un nouveau réseau de classification, dont les poids sont entrâinés sur la nouvelle tâche. Les poids du réseau initial (hors couche de classification) sont conservés (le réseaux convolutif agit donc comme un extracteur de caractéristiques)\n",
    "\n",
    "L'objectif est d'apprendre un réseau à reconnaître des images de bananes, tomates, pizza et sushis.\n",
    "\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from tempfile import TemporaryDirectory\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torchvision import datasets, models, transforms\n",
    "```\n",
    "\n",
    "On utilise une technique d'[augmentation de données](https://pytorch.org/vision/stable/transforms.html) pour augmenter la taille de la base d'entraînement. On normalise les images d'entraînement et de validation.\n",
    "\n",
    "```python\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "data_dir = './data'\n",
    "batch_size = 4\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),data_transforms[x]) for x in ['train', 'val']}\n",
    "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size, shuffle=True, num_workers=6) for x in ['train', 'val']}\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
    "class_names = image_datasets['train'].classes\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "```\n",
    "\n",
    "On donne une fonction d'affichage d'images ({numref}`sushis`), renormalisées.\n",
    "\n",
    "```python\n",
    "def imshow(I, title=None):\n",
    "    I = I.numpy().transpose((1, 2, 0))\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    I = mean + std * I \n",
    "    I = np.clip(I, 0, 1)\n",
    "    ax = plt.gca()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    plt.imshow(I)\n",
    "    plt.savefig('./images.png',dpi=100)\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "\n",
    "inputs, classes = next(iter(dataloaders['train']))\n",
    "out = torchvision.utils.make_grid(inputs)\n",
    "imshow(out, title=[class_names[x] for x in classes])\n",
    "```\n",
    "\n",
    "```{figure} ./images/sushis.png\n",
    ":name: sushis\n",
    "Quelques exemples d'images.\n",
    "```\n",
    "\n",
    "\n",
    "Et une fonction d'affichage des prédictions des modèles sur l'ensemble de validation\n",
    "\n",
    "```python\n",
    "def predict(model):\n",
    "    was_training = model.training\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Extraction d'un batch d'évaluation\n",
    "        inputs, labels = next(iter(dataloaders['val']))\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "\n",
    "        out = torchvision.utils.make_grid(inputs)\n",
    "        imshow(out, title=[class_names[preds[j]] for j in range(inputs.size()[0])])\n",
    "\n",
    "        model.train(mode=was_training)\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "### Premier entraînement\n",
    "\n",
    "Dans ce premier entraînement, on utilise un réseau pré-entraîné, qui sert d'initialisation à un entraînement complet sur la base d'entraînement.\n",
    "\n",
    "```python\n",
    "def train1(model, criterion, optimizer, scheduler, num_epochs=25):\n",
    "\n",
    "    # Répertoire temporaire pour les checkpoints \n",
    "    with TemporaryDirectory() as tempdir:\n",
    "        best_model_params_path = os.path.join(tempdir, 'best_params.pt')\n",
    "        torch.save(model.state_dict(), best_model_params_path)\n",
    "\n",
    "        best_acc = 0.0\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            print(f'Epoch {epoch}/{num_epochs - 1}')\n",
    "            print('*' * 10)\n",
    "\n",
    "            # Le modèle est utilisé en entraînement et en évaluation\n",
    "            for phase in ['train', 'val']:\n",
    "                if phase == 'train':\n",
    "                    model.train()  \n",
    "                else:\n",
    "                    model.eval()   \n",
    "\n",
    "                running_loss = 0.0\n",
    "                running_corrects = 0\n",
    "\n",
    "                for inputs, labels in dataloaders[phase]:\n",
    "                    inputs = inputs.to(device)\n",
    "                    labels = labels.to(device)\n",
    "\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                    # passe avant \n",
    "                    with torch.set_grad_enabled(phase == 'train'):\n",
    "                        outputs = model(inputs)\n",
    "                        _, preds = torch.max(outputs, 1)\n",
    "                        loss = criterion(outputs, labels)\n",
    "\n",
    "                        # rétropropagation et mise à jour des poids en entraînement \n",
    "                        if phase == 'train':\n",
    "                            loss.backward()\n",
    "                            optimizer.step()\n",
    "\n",
    "                    running_loss += loss.item() * inputs.size(0)\n",
    "                    running_corrects += torch.sum(preds == labels.data)\n",
    "                # Mise à jour du learning rate\n",
    "                if phase == 'train':\n",
    "                    scheduler.step()\n",
    "\n",
    "                epoch_loss = running_loss / dataset_sizes[phase]\n",
    "                epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "                print(f'{phase} Perte: {epoch_loss:.4f} Précision: {epoch_acc:.4f}')\n",
    "\n",
    "                # Sauvegarde du modèle si meilleur\n",
    "                if phase == 'val' and epoch_acc > best_acc:\n",
    "                    best_acc = epoch_acc\n",
    "                    torch.save(model.state_dict(), best_model_params_path)\n",
    "                    \n",
    "        # Chargement du meilleur modèle\n",
    "        model.load_state_dict(torch.load(best_model_params_path))\n",
    "    return model\n",
    "\n",
    "```\n",
    "\n",
    "Le réseau utilisé est ResNet34, entraîné sur ImageNet. On ajoute une couche de classification spécifique au problème et on entraîne le tout.\n",
    "\n",
    "```python\n",
    "model1 = models.resnet34(weights='IMAGENET1K_V1')\n",
    "# Nombre de caractéristiques extraites avant le réseau de classification\n",
    "nb = model1.fc.in_features\n",
    "\n",
    "# Ajout d'une couche de classification spécifique\n",
    "model1.fc = nn.Linear(nb, len(class_names))\n",
    "model1 = model1.to(device)\n",
    "\n",
    "# Fonction de perte\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Tous les poids vont être optimisés, y compris ceux du réseau convolutif.\n",
    "optimizer = optim.SGD(model1.parameters(), lr=0.001, momentum=0.9)\n",
    "lr_sch = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "\n",
    "model1 = train_model(model1, criterion, optimizer,lr_sch, num_epochs=25)\n",
    "\n",
    "predict(model1)\n",
    "```\n",
    "\n",
    "```{figure} ./images/val.png\n",
    ":name: val\n",
    "Quelques exemples d'images de validation étiquetées.\n",
    "```\n",
    "\n",
    "\n",
    "### Second entraînement\n",
    "\n",
    "Ici, on ne réentraîne pas les poids du réseau convolutif. On laisse donc ce réseau agir comme un extracteur de caractéristiques et on entraîne uniquement les poids de la couche de classification ajouté en bout.\n",
    "\n",
    "```python\n",
    "model2 = torchvision.models.resnet34(weights='IMAGENET1K_V1')\n",
    "# On fige les poids du réseau convolutif\n",
    "for param in model2.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# On ajoute une couche de classification\n",
    "nb = model2.fc.in_features\n",
    "model2.fc = nn.Linear(nb, len(class_names))\n",
    "\n",
    "model2 = model2.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# On optimise juste les poids de la couche de classification\n",
    "optimizer = optim.SGD(model2.fc.parameters(), lr=0.001, momentum=0.9)\n",
    "lr_sch = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "model2 = train_model(model2, criterion, optimizer,lr_sch, num_epochs=25)\n",
    "\n",
    "predict(model2)\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "```{bibliography}\n",
    ":style: unsrt\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "md:myst",
   "text_representation": {
    "extension": ".md",
    "format_name": "myst"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "source_map": [
   11,
   329,
   332
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}