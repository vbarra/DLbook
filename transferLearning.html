
<!DOCTYPE html>


<html lang="fr" data-content_root="./" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>Utilisation de réseaux existants &#8212; Apprentissage profond</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=5b4479735964841361fd" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=5b4479735964841361fd" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=5b4479735964841361fd" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=5b4479735964841361fd" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="_static/styles/sphinx-book-theme.css?v=384b581d" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="_static/proof.css?v=ca93fcec" />
    <link rel="stylesheet" type="text/css" href="_static/design-style.1e8bd061cd6da7fc9cf755528e8ffc24.min.css?v=0a3b3ea7" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=5b4479735964841361fd" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=5b4479735964841361fd" />
  <script src="_static/vendor/fontawesome/6.1.2/js/all.min.js?digest=5b4479735964841361fd"></script>

    <script src="_static/documentation_options.js?v=72dce1d2"></script>
    <script src="_static/doctools.js?v=888ff710"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="_static/copybutton.js?v=f281be69"></script>
    <script src="_static/scripts/sphinx-book-theme.js?v=efea14e4"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js?v=4a39c7ea"></script>
    <script src="_static/translations.js?v=d99ca74e"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js?v=36754332"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="_static/sphinx-thebe.js?v=afe5de03"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'transferLearning';</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Recherche" href="search.html" />
    <link rel="prev" title="Réseaux récurrents" href="rnn.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="fr"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Passer au contenu principal</a>
  
  <div id="pst-scroll-pixel-helper"></div>

  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  

<a class="navbar-brand logo" href="intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="_static/logo.png" class="logo__image only-light" alt="Apprentissage profond - Home"/>
    <script>document.write(`<img src="_static/logo.png" class="logo__image only-dark" alt="Apprentissage profond - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn navbar-btn search-button-field search-button__button" title="Recherche" aria-label="Recherche" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Recherche</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Introduction</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="NN.html">Introduction aux réseaux de neurones</a></li>
<li class="toctree-l1"><a class="reference internal" href="PMC.html">Perceptrons multicouches</a></li>
<li class="toctree-l1"><a class="reference internal" href="cnn.html">Réseaux convolutifs</a></li>
<li class="toctree-l1"><a class="reference internal" href="ae.html">Auto-encodeurs</a></li>
<li class="toctree-l1"><a class="reference internal" href="rnn.html">Réseaux récurrents</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Utilisation de réseaux existants</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">



<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Mode plein écran"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="clair/sombre" aria-label="clair/sombre" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Recherche" aria-label="Recherche" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Utilisation de réseaux existants</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contenu </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#quelques-reseaux-profonds-classiques">Quelques réseaux profonds classiques</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#alexnet">AlexNet</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#vgg">VGG</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#inception">Inception</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#resnet">ResNet</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#comment-utiliser-ces-reseaux">Comment utiliser ces réseaux ?</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#utilisation-de-reseaux-pre-entraines">Utilisation de réseaux pré-entraînés</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#transfer-learning-et-fine-tuning">Transfer learning et fine tuning</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#que-faire-si-j-ai-peu-de-donnees">Que faire si j’ai peu de données ?</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="utilisation-de-reseaux-existants">
<h1>Utilisation de réseaux existants<a class="headerlink" href="#utilisation-de-reseaux-existants" title="Link to this heading">#</a></h1>
<p>Nous présentons dans la suite quatre réseaux profonds classiques. Nous montrons ensuite comment les utiliser directement, ou comment les adapter pour répondre à une problématique précise, en lien avec leur utilisation originale ou non. Nous introduisons enfin une manière
d’apprendre un réseau à partir de peu de données.</p>
<section id="quelques-reseaux-profonds-classiques">
<h2>Quelques réseaux profonds classiques<a class="headerlink" href="#quelques-reseaux-profonds-classiques" title="Link to this heading">#</a></h2>
<p>Les réseaux présentés ici ont prouvé leur efficacité, notamment lors des compétitions organisées depuis 2010 sur une base de données d’images nommée <a class="reference external" href="http://www.image-net.org/">ImageNet</a>. Initiée à l’Université de Stanford, cette base de données comporte aujourd’hui plus de 14 millions d’images, classées en 21841 catégories (avions, voitures, chats,…). Dans les compétitions <a class="reference external" href="https://image-net.org/challenges/LSVRC/">ILSVRC</a> ( ImageNet Large Scale Visual Recognition Challenge), les chercheurs se voient proposer
une extraction de 1,2 millions d’images d’entraînement, 100 000 images de test et 50 000 images de validation, catégorisées en 1000 classes. Le gagnant est celui qui atteint la meilleure précision de reconnaissance sur les 5 premières classes (top-5).</p>
<p>La <a class="reference internal" href="#perf"><span class="std std-numref">Fig. 40</span></a> donne un aperçu des performances de plusieurs réseaux profonds suivant cette métrique.</p>
<figure class="align-default" id="perf">
<img alt="_images/classifImagenet.png" src="_images/classifImagenet.png" />
<figcaption>
<p><span class="caption-number">Fig. 40 </span><span class="caption-text">Performance de réseaux profonds sur une tache de classification (<a class="reference external" href="https://theaisummer.com/cnn-architectures/">source</a>)</span><a class="headerlink" href="#perf" title="Link to this image">#</a></p>
</figcaption>
</figure>
<section id="alexnet">
<h3>AlexNet<a class="headerlink" href="#alexnet" title="Link to this heading">#</a></h3>
<p>En 2012, Krizhevsky et al  <span id="id1">[<a class="reference internal" href="#id44" title="Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. Imagenet classification with deep convolutional neural networks. In Advances in Neural Information Processing Systems, 2012. 2012.">13</a>]</span> remportent ILSVRC avec un taux de reconnaissance de 84.6%, en utilisant AlexNet, un réseau convolutif composé de 5 couches de convolution et de pooling, suivies de 3 couches complètement connectées (<a class="reference internal" href="#id2"><span class="std std-numref">Fig. 41</span></a>).</p>
<figure class="align-default" id="id2">
<img alt="_images/AlexNet.png" src="_images/AlexNet.png" />
<figcaption>
<p><span class="caption-number">Fig. 41 </span><span class="caption-text">Architecture du réseau AlexNet. Les couches de convolution
et d’activation sont en orange clair, les couches d’agrégation en orange
foncé. Les couches complètement connectées sont en violet.</span><a class="headerlink" href="#id2" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>Si la profondeur du réseau reste faible, le nombre de paramètres était
déjà important. En regardant uniquement la première couche de
convolution, on constate que :</p>
<ul class="simple">
<li><p>l’entrée est composée d’images 227<span class="math notranslate nohighlight">\(\times\)</span>227<span class="math notranslate nohighlight">\(\times\)</span>3</p></li>
<li><p>les filtres de convolution sont de taille 11</p></li>
<li><p>le pas de convolution (stride) est de 4</p></li>
</ul>
<p>Ainsi la sortie de la couche de convolution est de taille 55<span class="math notranslate nohighlight">\(\times\)</span>55<span class="math notranslate nohighlight">\(\times\)</span>96=290 400 neurones, chacun ayant
11<span class="math notranslate nohighlight">\(\times\)</span>11<span class="math notranslate nohighlight">\(\times\)</span>3=363 poids et un biais. Cela implique, sur cette couche de convolution seulement, 105 705 600 paramètres à ajuster.</p>
<p>Ce réseau, amélioration d’un réseau existant (LeNet), apportait de
nombreuses contributions, comme l’utilisation de couches ReLU, de
dropout, ou du GPU (NVIDIA GTX 580) pendant la phase d’entraînement.</p>
</section>
<section id="vgg">
<h3>VGG<a class="headerlink" href="#vgg" title="Link to this heading">#</a></h3>
<p>Les réseaux VGG (Visual Geometry Group, université d’Oxford)
<span id="id3">[<a class="reference internal" href="#id42" title="Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. CoRR, 2014. URL: http://arxiv.org/abs/1409.1556.">14</a>]</span> ont été les premiers réseaux à utiliser de petits filtres de convolution (3<span class="math notranslate nohighlight">\(\times\)</span>3) et à les combiner pour
décrire des séquences de convolution, l’idée étant d’émuler l’effet de
larges champs réceptifs par cette séquence. Cette technique amène
malheureusement à un nombre exponentiel de paramètres (le modèle
entraîné qui peut être téléchargé a une taille de plus de 500 Mo). VGG a concouru à ILSVRC 2014, a obtenu un taux de bonne classification de
92.3% mais n’a pas remporté le challenge. Aujourd’hui VGG et une famille de réseaux profonds (de A à E) qui varient par leur architecture</p>
<div class="center docutils">
<figure id="F:VGG">
<style type="text/css">
.myTable {style="border:1px solid blue; border-collapse:collapse;" }
.myTable th { background-color:#000;color:white;width:50%; }
.myTable td, .myTable th { padding:5px;border:1px solid #000; }
</style>
<table class="myTable">
<thead>
<tr class="header">
<th style="text-align: center;"><strong>A</strong></th>
<th style="text-align: center;"><strong><span>A-LRN</span></strong></th>
<th style="text-align: center;"><strong><span>B</span></strong></th>
<th style="text-align: center;"><strong><span>C</span></strong></th>
<th style="text-align: center;"><strong><span>D</span></strong></th>
<th style="text-align: center;"><strong><span>E</span></strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">11 couches</td>
<td style="text-align: center;">11 couches</td>
<td style="text-align: center;">13 couches</td>
<td style="text-align: center;">16 couches</td>
<td style="text-align: center;">16 couches</td>
<td style="text-align: center;">19 couches</td>
</tr>
<tr class="even">
<td colspan="6" style="text-align: center;">Entrée : image 224<span
class="math inline">×</span>224 RGB</td>
</tr>
<tr class="odd">
<td rowspan="2" style="text-align: center;"></td>
<td style="text-align: center;">conv3-64</td>
<td style="text-align: center;">conv3-64</td>
<td style="text-align: center;">conv3-64</td>
<td style="text-align: center;">conv3-64</td>
<td style="text-align: center;">conv3-64</td>
</tr>
<tr class="even">
<td style="text-align: center;">LRN</td>
<td style="text-align: center;">conv3-64</td>
<td style="text-align: center;">conv3-64</td>
<td style="text-align: center;">conv3-64</td>
<td style="text-align: center;">conv3-64</td>
</tr>
<tr class="odd">
<td colspan="6" style="text-align: center;">max pooling</td>
</tr>
<tr class="even">
<td rowspan="2" style="text-align: center;"></td>
<td style="text-align: center;">conv3-128</td>
<td style="text-align: center;">conv3-128</td>
<td style="text-align: center;">conv3-128</td>
<td style="text-align: center;">conv3-128</td>
<td style="text-align: center;">conv3-128</td>
</tr>
<tr class="odd">
<td style="text-align: center;"></td>
<td style="text-align: center;">conv3-128</td>
<td style="text-align: center;">conv3-128</td>
<td style="text-align: center;">conv3-128</td>
<td style="text-align: center;">conv3-128</td>
</tr>
<tr class="even">
<td colspan="6" style="text-align: center;">max pooling</td>
</tr>
<tr class="odd">
<td rowspan="4" style="text-align: center;"></td>
<td style="text-align: center;">conv3-256</td>
<td style="text-align: center;">conv3-256</td>
<td style="text-align: center;">conv3-256</td>
<td style="text-align: center;">conv3-256</td>
<td style="text-align: center;">conv3-256</td>
</tr>
<tr class="even">
<td style="text-align: center;">conv3-256</td>
<td style="text-align: center;">conv3-256</td>
<td style="text-align: center;">conv3-256</td>
<td style="text-align: center;">conv3-256</td>
<td style="text-align: center;">conv3-256</td>
</tr>
<tr class="odd">
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">conv1-256</td>
<td style="text-align: center;">conv3-256</td>
<td style="text-align: center;">conv3-256</td>
</tr>
<tr class="even">
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">conv3-256</td>
</tr>
<tr class="odd">
<td colspan="6" style="text-align: center;">max pooling</td>
</tr>
<tr class="even">
<td rowspan="4" style="text-align: center;"></td>
<td style="text-align: center;">conv3-512</td>
<td style="text-align: center;">conv3-512</td>
<td style="text-align: center;">conv3-512</td>
<td style="text-align: center;">conv3-512</td>
<td style="text-align: center;">conv3-512</td>
</tr>
<tr class="odd">
<td style="text-align: center;">conv3-512</td>
<td style="text-align: center;">conv3-512</td>
<td style="text-align: center;">conv3-512</td>
<td style="text-align: center;">conv3-512</td>
<td style="text-align: center;">conv3-512</td>
</tr>
<tr class="even">
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">conv1-512</td>
<td style="text-align: center;">conv3-512</td>
<td style="text-align: center;">conv3-512</td>
</tr>
<tr class="odd">
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">conv3-512</td>
</tr>
<tr class="even">
<td colspan="6" style="text-align: center;">max pooling</td>
</tr>
<tr class="odd">
<td rowspan="4" style="text-align: center;"></td>
<td style="text-align: center;">conv3-512</td>
<td style="text-align: center;">conv3-512</td>
<td style="text-align: center;">conv3-512</td>
<td style="text-align: center;">conv3-512</td>
<td style="text-align: center;">conv3-512</td>
</tr>
<tr class="even">
<td style="text-align: center;">conv3-512</td>
<td style="text-align: center;">conv3-512</td>
<td style="text-align: center;">conv3-512</td>
<td style="text-align: center;">conv3-512</td>
<td style="text-align: center;">conv3-512</td>
</tr>
<tr class="odd">
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">conv1-512</td>
<td style="text-align: center;">conv3-512</td>
<td style="text-align: center;">conv3-512</td>
</tr>
<tr class="even">
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">conv3-512</td>
</tr>
<tr class="odd">
<td colspan="6" style="text-align: center;">max pooling</td>
</tr>
<tr class="even">
<td colspan="6" style="text-align: center;">Couche complètement
connectée 4096 neurones</td>
</tr>
<tr class="odd">
<td colspan="6" style="text-align: center;">Couche complètement
connectée 4096 neurones</td>
</tr>
<tr class="even">
<td colspan="6" style="text-align: center;">Couche complètement
connectée 1000 neurones</td>
</tr>
<tr class="odd">
<td colspan="6" style="text-align: center;">Classifieur softmax</td>
</tr>
</tbody>
</table>
</figure>
</div>
<p>Le nombre de paramètres (en millions) pour les réseaux de A à E est 133, 133, 134, 138 et 144. Les réseaux VGG-D et VGG-E sont les plus précis et populaires.</p>
<figure class="align-default" id="vgg16">
<img alt="_images/VGG16.png" src="_images/VGG16.png" />
<figcaption>
<p><span class="caption-number">Fig. 42 </span><span class="caption-text">Réseau VGG16</span><a class="headerlink" href="#vgg16" title="Link to this image">#</a></p>
</figcaption>
</figure>
</section>
<section id="inception">
<h3>Inception<a class="headerlink" href="#inception" title="Link to this heading">#</a></h3>
<p>Inception, proposé par Google, est le premier réseau dont les
performances ont été augmentées non seulement en augmentant le nombre de couches, mais en pensant et optimisant le design et l’architecture.
L’idée est ici d’utiliser plusieurs filtres, de tailles différentes, sur la même image et de concaténer les résultats pour générer une
représentation plus robuste.</p>
<p>Inception n’est pas un réseau, c’est une famille de réseaux : Network in Network <span id="id4">[<a class="reference internal" href="#id41" title="Min Lin, Qiang Chen, and Shuicheng Yan. Network in network. CoRR, 2013. URL: http://arxiv.org/abs/1312.4400.">15</a>]</span>, Inception V1 <span id="id5">[<a class="reference internal" href="#id40" title="Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott E. Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. CoRR, 2014. URL: http://arxiv.org/abs/1409.4842.">16</a>]</span>, Inception V2 <span id="id6">[<a class="reference internal" href="#id39" title="Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna. Rethinking the inception architecture for computer vision. CoRR, 2015. URL: http://arxiv.org/abs/1512.00567.">17</a>]</span>,
Xception <span id="id7">[<a class="reference internal" href="#id48" title="François Chollet. Xception: deep learning with depthwise separable convolutions. CoRR, 2016. URL: http://arxiv.org/abs/1610.02357, arXiv:1610.02357.">18</a>]</span>,…</p>
<p>L’idée du premier réseau (<a class="reference internal" href="#nin"><span class="std std-numref">Fig. 43</span></a>) est de connecter les couches de convolution par des perceptrons multicouches, introduisant des non linéarités dans les réseaux profonds. Mathématiquement, ces perceptrons sont équivalents à des convolutions par des filtres 1<span class="math notranslate nohighlight">\(\times 1\)</span> et gardent donc la cohérence des réseaux. Cette nouvelle architecture rend moins indispensable les couches complètement connectées en fin de réseau. Les auteurs moyennent spatialement les cartes finales et donnent le résultat au classifier softmax. Le nombre de paramètres est alors réduit, diminuant de ce fait le risque de sur apprentissage.</p>
<figure class="align-default" id="nin">
<img alt="_images/NIN.png" src="_images/NIN.png" />
<figcaption>
<p><span class="caption-number">Fig. 43 </span><span class="caption-text">Réseau Network in Network</span><a class="headerlink" href="#nin" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>Inception V1, implémenté dans le réseau GoogLeNet vainqueur d’ILSVRC
2014, est une extension à des réseaux plus profonds de Network to
Network. Le réseau est composé de 22 couches et atteint 93.3% de taux de reconnaissance. D’autres améliorations théoriques (fonctions de pertes associées aux couches intermédiaires dans la phase d’apprentissage, introduction de caractères épars dans le réseau) ont également permis d’améliorer les performances (de calcul et de classification).</p>
<p>Inception V2, puis V3 (<a class="reference internal" href="#inceptionv3"><span class="std std-numref">Fig. 44</span></a>) adoptent des techniques de factorisation (toute convolution par un filtre de taille plus grande que 3<span class="math notranslate nohighlight">\(\times\)</span> 3 peut être exprimée de manière plus efficace
avec une série de filtres de taille réduite) et de normalisation pour
améliorer encore les performances.</p>
<p>Inception V4 <span id="id8">[<a class="reference internal" href="#id38" title="Christian Szegedy, Sergey Ioffe, and Vincent Vanhoucke. Inception-v4, inception-resnet and the impact of residual connections on learning. CoRR, 2016. URL: http://arxiv.org/abs/1602.07261.">19</a>]</span> propose une version rationalisée, à
l’architecture uniforme et aux performances accrues.</p>
<figure class="align-default" id="inceptionv3">
<img alt="_images/inceptionv3.png" src="_images/inceptionv3.png" />
<figcaption>
<p><span class="caption-number">Fig. 44 </span><span class="caption-text">Architecture d’inception V3</span><a class="headerlink" href="#inceptionv3" title="Link to this image">#</a></p>
</figcaption>
</figure>
</section>
<section id="resnet">
<h3>ResNet<a class="headerlink" href="#resnet" title="Link to this heading">#</a></h3>
<p>En 2015, Microsoft remporte la compétition ILSVRC avec ResNet <span id="id9">[<a class="reference internal" href="#id37" title="Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. CoRR, 2015. URL: http://arxiv.org/abs/1512.03385.">20</a>]</span>, un réseau à 152 couches qui utilise un module ResNet. Le taux de bonne reconnaissance est de 96.4%. Un réseau résiduel (ou ResNet) résout le problème de vanishing gradient de la manière la plus simple possible, en permettant des raccourcis entre chaque couche du réseau. Dans un réseau classique, l’activation en sortie de couche est de la forme <span class="math notranslate nohighlight">\(y=\sigma(x)\)</span>, et lors de la rétropropagation, le gradient doit nécessairement repasser par <span class="math notranslate nohighlight">\(\sigma(x)\)</span>, ce qui peut causer des
problèmes en raison de la (forte) non linéarité induite par <span class="math notranslate nohighlight">\(\sigma\)</span>.
Dans un réseau résiduel, la sortie de chaque couche est calculée par
<span class="math notranslate nohighlight">\(y=\sigma()+x\)</span>, où <span class="math notranslate nohighlight">\(+x\)</span> est le raccourci entre chaque couche, qui permet
au gradient de transiter directement sans passer par <span class="math notranslate nohighlight">\(\sigma\)</span>.<br />
Cette représentation donne l’idée générale, mais la réalité est un peu
plus complexe, et prend la forme d’un module ResNet (<a class="reference internal" href="#id11"><span class="std std-numref">Fig. 45</span></a>).</p>
<figure class="align-default" id="id11">
<img alt="_images/resNet.png" src="_images/resNet.png" />
<figcaption>
<p><span class="caption-number">Fig. 45 </span><span class="caption-text">Module ResNet (source : <span id="id10">[<a class="reference internal" href="#id37" title="Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. CoRR, 2015. URL: http://arxiv.org/abs/1512.03385.">20</a>]</span>)</span><a class="headerlink" href="#id11" title="Link to this image">#</a></p>
</figcaption>
</figure>
</section>
</section>
<section id="comment-utiliser-ces-reseaux">
<h2>Comment utiliser ces réseaux ?<a class="headerlink" href="#comment-utiliser-ces-reseaux" title="Link to this heading">#</a></h2>
<p>Il est possible de définir les réseaux classiques en décrivant une à une les couches et leur paramètres, qui sont proposés dans les articles correspondants. On imagine assez bien le travail que cela peut représenter sur ResNet par exemple…</p>
<p>Fort heureusement, il existe d’autres manières d’utiliser ces réseaux.</p>
<section id="utilisation-de-reseaux-pre-entraines">
<h3>Utilisation de réseaux pré-entraînés<a class="headerlink" href="#utilisation-de-reseaux-pre-entraines" title="Link to this heading">#</a></h3>
<p>Il est possible de charger / sauvegarder des
réseaux qui ont été entraînés sur des grandes bases de données et de les utiliser directement. Il est également possible, pendant
l’entraînement, de créer des sauvegardes (checkpoints) pour reprendre
éventuellement l’entraînement en cours d’itérations. On peut sauvegarder
tout le réseau (architecture + optimiseur + poids), ou seulement les
poids.</p>
<p><code class="docutils literal notranslate"><span class="pre">torchvision</span></code>donne accès à de nombreux modèles pré-entrainés :</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">torchvision</span> <span class="kn">import</span> <span class="n">models</span>
<span class="nb">dir</span><span class="p">(</span><span class="n">models</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[&#39;AlexNet&#39;,
 &#39;AlexNet_Weights&#39;,
 &#39;ConvNeXt&#39;,
 &#39;ConvNeXt_Base_Weights&#39;,
 &#39;ConvNeXt_Large_Weights&#39;,
 &#39;ConvNeXt_Small_Weights&#39;,
 &#39;ConvNeXt_Tiny_Weights&#39;,
 &#39;DenseNet&#39;,
 &#39;DenseNet121_Weights&#39;,
 &#39;DenseNet161_Weights&#39;,
 &#39;DenseNet169_Weights&#39;,
 &#39;DenseNet201_Weights&#39;,
 &#39;EfficientNet&#39;,
 &#39;EfficientNet_B0_Weights&#39;,
 &#39;EfficientNet_B1_Weights&#39;,
 &#39;EfficientNet_B2_Weights&#39;,
 &#39;EfficientNet_B3_Weights&#39;,
 &#39;EfficientNet_B4_Weights&#39;,
 &#39;EfficientNet_B5_Weights&#39;,
 &#39;EfficientNet_B6_Weights&#39;,
 &#39;EfficientNet_B7_Weights&#39;,
 &#39;EfficientNet_V2_L_Weights&#39;,
 &#39;EfficientNet_V2_M_Weights&#39;,
 &#39;EfficientNet_V2_S_Weights&#39;,
 &#39;GoogLeNet&#39;,
 &#39;GoogLeNetOutputs&#39;,
 &#39;GoogLeNet_Weights&#39;,
 &#39;Inception3&#39;,
 &#39;InceptionOutputs&#39;,
 &#39;Inception_V3_Weights&#39;,
 &#39;MNASNet&#39;,
 &#39;MNASNet0_5_Weights&#39;,
 &#39;MNASNet0_75_Weights&#39;,
 &#39;MNASNet1_0_Weights&#39;,
 &#39;MNASNet1_3_Weights&#39;,
 &#39;MaxVit&#39;,
 &#39;MaxVit_T_Weights&#39;,
 &#39;MobileNetV2&#39;,
 &#39;MobileNetV3&#39;,
 &#39;MobileNet_V2_Weights&#39;,
 &#39;MobileNet_V3_Large_Weights&#39;,
 &#39;MobileNet_V3_Small_Weights&#39;,
 &#39;RegNet&#39;,
 &#39;RegNet_X_16GF_Weights&#39;,
 &#39;RegNet_X_1_6GF_Weights&#39;,
 &#39;RegNet_X_32GF_Weights&#39;,
 &#39;RegNet_X_3_2GF_Weights&#39;,
 &#39;RegNet_X_400MF_Weights&#39;,
 &#39;RegNet_X_800MF_Weights&#39;,
 &#39;RegNet_X_8GF_Weights&#39;,
 &#39;RegNet_Y_128GF_Weights&#39;,
 &#39;RegNet_Y_16GF_Weights&#39;,
 &#39;RegNet_Y_1_6GF_Weights&#39;,
 &#39;RegNet_Y_32GF_Weights&#39;,
 &#39;RegNet_Y_3_2GF_Weights&#39;,
 &#39;RegNet_Y_400MF_Weights&#39;,
 &#39;RegNet_Y_800MF_Weights&#39;,
 &#39;RegNet_Y_8GF_Weights&#39;,
 &#39;ResNeXt101_32X8D_Weights&#39;,
 &#39;ResNeXt101_64X4D_Weights&#39;,
 &#39;ResNeXt50_32X4D_Weights&#39;,
 &#39;ResNet&#39;,
 &#39;ResNet101_Weights&#39;,
 &#39;ResNet152_Weights&#39;,
 &#39;ResNet18_Weights&#39;,
 &#39;ResNet34_Weights&#39;,
 &#39;ResNet50_Weights&#39;,
 &#39;ShuffleNetV2&#39;,
 &#39;ShuffleNet_V2_X0_5_Weights&#39;,
 &#39;ShuffleNet_V2_X1_0_Weights&#39;,
 &#39;ShuffleNet_V2_X1_5_Weights&#39;,
 &#39;ShuffleNet_V2_X2_0_Weights&#39;,
 &#39;SqueezeNet&#39;,
 &#39;SqueezeNet1_0_Weights&#39;,
 &#39;SqueezeNet1_1_Weights&#39;,
 &#39;SwinTransformer&#39;,
 &#39;Swin_B_Weights&#39;,
 &#39;Swin_S_Weights&#39;,
 &#39;Swin_T_Weights&#39;,
 &#39;Swin_V2_B_Weights&#39;,
 &#39;Swin_V2_S_Weights&#39;,
 &#39;Swin_V2_T_Weights&#39;,
 &#39;VGG&#39;,
 &#39;VGG11_BN_Weights&#39;,
 &#39;VGG11_Weights&#39;,
 &#39;VGG13_BN_Weights&#39;,
 &#39;VGG13_Weights&#39;,
 &#39;VGG16_BN_Weights&#39;,
 &#39;VGG16_Weights&#39;,
 &#39;VGG19_BN_Weights&#39;,
 &#39;VGG19_Weights&#39;,
 &#39;ViT_B_16_Weights&#39;,
 &#39;ViT_B_32_Weights&#39;,
 &#39;ViT_H_14_Weights&#39;,
 &#39;ViT_L_16_Weights&#39;,
 &#39;ViT_L_32_Weights&#39;,
 &#39;VisionTransformer&#39;,
 &#39;Weights&#39;,
 &#39;WeightsEnum&#39;,
 &#39;Wide_ResNet101_2_Weights&#39;,
 &#39;Wide_ResNet50_2_Weights&#39;,
 &#39;_GoogLeNetOutputs&#39;,
 &#39;_InceptionOutputs&#39;,
 &#39;__builtins__&#39;,
 &#39;__cached__&#39;,
 &#39;__doc__&#39;,
 &#39;__file__&#39;,
 &#39;__loader__&#39;,
 &#39;__name__&#39;,
 &#39;__package__&#39;,
 &#39;__path__&#39;,
 &#39;__spec__&#39;,
 &#39;_api&#39;,
 &#39;_meta&#39;,
 &#39;_utils&#39;,
 &#39;alexnet&#39;,
 &#39;convnext&#39;,
 &#39;convnext_base&#39;,
 &#39;convnext_large&#39;,
 &#39;convnext_small&#39;,
 &#39;convnext_tiny&#39;,
 &#39;densenet&#39;,
 &#39;densenet121&#39;,
 &#39;densenet161&#39;,
 &#39;densenet169&#39;,
 &#39;densenet201&#39;,
 &#39;detection&#39;,
 &#39;efficientnet&#39;,
 &#39;efficientnet_b0&#39;,
 &#39;efficientnet_b1&#39;,
 &#39;efficientnet_b2&#39;,
 &#39;efficientnet_b3&#39;,
 &#39;efficientnet_b4&#39;,
 &#39;efficientnet_b5&#39;,
 &#39;efficientnet_b6&#39;,
 &#39;efficientnet_b7&#39;,
 &#39;efficientnet_v2_l&#39;,
 &#39;efficientnet_v2_m&#39;,
 &#39;efficientnet_v2_s&#39;,
 &#39;get_model&#39;,
 &#39;get_model_builder&#39;,
 &#39;get_model_weights&#39;,
 &#39;get_weight&#39;,
 &#39;googlenet&#39;,
 &#39;inception&#39;,
 &#39;inception_v3&#39;,
 &#39;list_models&#39;,
 &#39;maxvit&#39;,
 &#39;maxvit_t&#39;,
 &#39;mnasnet&#39;,
 &#39;mnasnet0_5&#39;,
 &#39;mnasnet0_75&#39;,
 &#39;mnasnet1_0&#39;,
 &#39;mnasnet1_3&#39;,
 &#39;mobilenet&#39;,
 &#39;mobilenet_v2&#39;,
 &#39;mobilenet_v3_large&#39;,
 &#39;mobilenet_v3_small&#39;,
 &#39;mobilenetv2&#39;,
 &#39;mobilenetv3&#39;,
 &#39;optical_flow&#39;,
 &#39;quantization&#39;,
 &#39;regnet&#39;,
 &#39;regnet_x_16gf&#39;,
 &#39;regnet_x_1_6gf&#39;,
 &#39;regnet_x_32gf&#39;,
 &#39;regnet_x_3_2gf&#39;,
 &#39;regnet_x_400mf&#39;,
 &#39;regnet_x_800mf&#39;,
 &#39;regnet_x_8gf&#39;,
 &#39;regnet_y_128gf&#39;,
 &#39;regnet_y_16gf&#39;,
 &#39;regnet_y_1_6gf&#39;,
 &#39;regnet_y_32gf&#39;,
 &#39;regnet_y_3_2gf&#39;,
 &#39;regnet_y_400mf&#39;,
 &#39;regnet_y_800mf&#39;,
 &#39;regnet_y_8gf&#39;,
 &#39;resnet&#39;,
 &#39;resnet101&#39;,
 &#39;resnet152&#39;,
 &#39;resnet18&#39;,
 &#39;resnet34&#39;,
 &#39;resnet50&#39;,
 &#39;resnext101_32x8d&#39;,
 &#39;resnext101_64x4d&#39;,
 &#39;resnext50_32x4d&#39;,
 &#39;segmentation&#39;,
 &#39;shufflenet_v2_x0_5&#39;,
 &#39;shufflenet_v2_x1_0&#39;,
 &#39;shufflenet_v2_x1_5&#39;,
 &#39;shufflenet_v2_x2_0&#39;,
 &#39;shufflenetv2&#39;,
 &#39;squeezenet&#39;,
 &#39;squeezenet1_0&#39;,
 &#39;squeezenet1_1&#39;,
 &#39;swin_b&#39;,
 &#39;swin_s&#39;,
 &#39;swin_t&#39;,
 &#39;swin_transformer&#39;,
 &#39;swin_v2_b&#39;,
 &#39;swin_v2_s&#39;,
 &#39;swin_v2_t&#39;,
 &#39;vgg&#39;,
 &#39;vgg11&#39;,
 &#39;vgg11_bn&#39;,
 &#39;vgg13&#39;,
 &#39;vgg13_bn&#39;,
 &#39;vgg16&#39;,
 &#39;vgg16_bn&#39;,
 &#39;vgg19&#39;,
 &#39;vgg19_bn&#39;,
 &#39;video&#39;,
 &#39;vision_transformer&#39;,
 &#39;vit_b_16&#39;,
 &#39;vit_b_32&#39;,
 &#39;vit_h_14&#39;,
 &#39;vit_l_16&#39;,
 &#39;vit_l_32&#39;,
 &#39;wide_resnet101_2&#39;,
 &#39;wide_resnet50_2&#39;]
</pre></div>
</div>
</div>
</div>
<p>Il est alors facile de charger un tel réseau, par exemple</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">resnet</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">resnet101</span><span class="p">(</span><span class="n">pretrained</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="transfer-learning-et-fine-tuning">
<h3>Transfer learning et fine tuning<a class="headerlink" href="#transfer-learning-et-fine-tuning" title="Link to this heading">#</a></h3>
<p>Il est possible d’utiliser les réseaux classiques pré-entrainés pour de
nouvelles tâches. L’idée sous-jacente et que les premières couches
capturent des caractéristiques bas niveau, et que la sémantique vient
avec les couches profondes. Ainsi, dans un problème de classification,
où les classes n’ont pas été apprises, on peut supposer qu’en conservant les premières couches on extraira des caractéristiques communes des
images, et qu’en changeant les dernières couches (information sémantique
et haut niveau et étage de classification), c’est à dire en réapprenant
les connexions, on spécifiera le nouveau réseau pour la nouvelle tâche
de classification.</p>
<p>Cette approche rentre dans le cadre des méthodes d’apprentissage par transfert (Transfer Learning)
<span id="id12">[<a class="reference internal" href="#id35" title="Sinno Jialin Pan and Qiang Yang. A survey on transfer learning. IEEE Trans. on Knowl. and Data Eng., 22(10):1345–1359, October 2010. URL: http://dx.doi.org/10.1109/TKDE.2009.191, doi:10.1109/TKDE.2009.191.">21</a>]</span> et de fine tuning, cas particulier d’adaptation de domaine.</p>
<p>L’apprentissage par transfert comporte généralement deux étapes principales :</p>
<ul class="simple">
<li><p><strong>Extraction des caractéristiques</strong> : dans cette étape,le modèle pré-entraîné est utilisé comme un extracteur de caractéristiques fixes. On supprime les couches finales (MLP, responsable de la classification) et on les remplaçe par de nouvelles couches spécifiques à la tâche adressée (<a class="reference internal" href="#tl2"><span class="std std-numref">Fig. 46</span></a>). Les poids du modèle pré-entraîné sont gelés et seuls les poids des couches nouvellement ajoutées sont entraînés sur l’ensemble de données du problème.</p></li>
</ul>
<figure class="align-default" id="tl2">
<img alt="_images/tl2.png" src="_images/tl2.png" />
<figcaption>
<p><span class="caption-number">Fig. 46 </span><span class="caption-text">Réentrainement d’un classifieur sur des caractéristiques extraites.</span><a class="headerlink" href="#tl2" title="Link to this image">#</a></p>
</figcaption>
</figure>
<ul class="simple">
<li><p><strong>fine tuning</strong> : le fine tuning pousse le processus un peu plus loin en dégelant certaines des couches du modèle pré-entraîné et en leur permettant d’être mises à jour avec le nouvel ensemble de données. Cette étape permet au modèle de s’adapter et d’apprendre des caractéristiques plus spécifiques liées à la nouvelle tâche ou au nouveau domaine.</p></li>
</ul>
<p>Plusieurs facteurs influent sur le choix de la méthode à utiliser, parmi lesquels :</p>
<ul class="simple">
<li><p>la taille des données d’apprentissage du nouveau problème (<a class="reference internal" href="#tl"><span class="std std-numref">Fig. 47</span></a>)</p></li>
</ul>
<figure class="align-default" id="tl">
<img alt="_images/tl.png" src="_images/tl.png" />
<figcaption>
<p><span class="caption-number">Fig. 47 </span><span class="caption-text">Stratégies d’apprentissage par transfert.</span><a class="headerlink" href="#tl" title="Link to this image">#</a></p>
</figcaption>
</figure>
<ul class="simple">
<li><p>la ressemblance du nouveau jeu de données avec celui qui a servi à
entraîner le réseau initial (<a class="reference internal" href="#domaintask"><span class="std std-numref">Fig. 48</span></a>).</p></li>
</ul>
<figure class="align-default" id="domaintask">
<img alt="_images/domaintask.png" src="_images/domaintask.png" />
<figcaption>
<p><span class="caption-number">Fig. 48 </span><span class="caption-text">Changement de domaine / tâche.</span><a class="headerlink" href="#domaintask" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>Pour un jeu de données similaire de petite taille, on utilise du
transfer learning, avec un classifieur utilisé sur les
caractéristiques calculées sur les dernières couches du réseau
initial. Pour un jeu de données de petite taille et un problème différent, on utilise du transfer learning, avec un classifieur utilisé sur les caractéristiques calculées sur les premières couches du réseau initial. Pour un jeu de données, similaire ou non de grande taille, on
utilise le fine tuning</p>
<p>A noter qu’il est toujours possible d’augmenter la taille du jeu de
données par des technique de « Data Augmentation » (changement de
couleurs des pixels, rotations, cropping, homothéties, translations…) (<a class="reference internal" href="#dataaugment"><span class="std std-numref">Fig. 49</span></a>).</p>
<figure class="align-default" id="dataaugment">
<img alt="_images/dataaugment.png" src="_images/dataaugment.png" />
<figcaption>
<p><span class="caption-number">Fig. 49 </span><span class="caption-text">Augmentation de données : à partir d’un exemple (image de gauche), on construit plusieurs autres exemples par rotation, flip, ajout de bruit, déformation, changement colorimétrique.</span><a class="headerlink" href="#dataaugment" title="Link to this image">#</a></p>
</figcaption>
</figure>
</section>
</section>
<section id="que-faire-si-j-ai-peu-de-donnees">
<h2>Que faire si j’ai peu de données ?<a class="headerlink" href="#que-faire-si-j-ai-peu-de-donnees" title="Link to this heading">#</a></h2>
<p>Les méthodes supervisées nécessitent pour de bonnes performances un
ensemble d’apprentissage <span class="math notranslate nohighlight">\(\mathcal{S}\)</span> de grand cardinal. Si seulement
peu d’exemples
<span class="math notranslate nohighlight">\(\mathcal{S}_s = \{(\mathbf{x}_i,y_i), i\in[\![1\cdots m ]\!]\}\)</span> sont
disponibles, avec <span class="math notranslate nohighlight">\(m\)</span> petit, les techniques précédemment décrites ne
sont pour la plupart plus applicables.<br />
Les méthodes de Few-Shot Learning ont été introduites pour traiter ce
manque de données. Les exemples applicatifs sont nombreux, allant de la
classification d’images à l’analyse de sentiments à partir de textes, ou
encore à la reconnaissance d’objets.<br />
Vu sous l’angle de la minimisation du risque empirique, l’hypothèse <span class="math notranslate nohighlight">\(h\)</span>
construite sur la minimisation de
$<span class="math notranslate nohighlight">\(R(h) = \displaystyle\sum_{i=1}^m \ell(y_i,h(\mathbf{x}_i))\)</span><span class="math notranslate nohighlight">\( conduit à
un sur apprentissage et un risque \)</span>R(h)<span class="math notranslate nohighlight">\( très loin du risque réel. Pour
pallier ce problème, des connaissances *a priori* doivent être
utilisées. Le Few-shot learning propose trois alternatives. Nous
détaillons ici l'une d'entre elles, l'augmentation de données.\
Les approches de cette catégorie utilisent des connaissances *a priori*
sur les données pour enrichir \)</span>\mathcal{S}_s$. On les regroupe parfois
sous le vocable de méthodes d”<em>augmentation de données</em>. Si elles sont
faciles à mettre en oeuvre et à comprendre, ces méthodes restent
cependant dépendantes du domaine d’étude et ne peuvent être facilement
généralisées.</p>
<p>Les principales stratégies sont résumées dans le tableau
<a class="reference internal" href="#T:dataAug"><span class="xref myst">[T:dataAug]</span></a>{reference-type= »ref » reference= »T:dataAug »}
et un exemple d’illustration est donné figure
<a class="reference internal" href="#Fig:dataAug"><span class="xref myst">1.10</span></a>{reference-type= »ref » reference= »Fig:dataAug »}.</p>
<div class="tabular docutils">
<p>M4cm|M2.5cm|M5cm|M2.5cm **Transformation… &amp; **Entrée &amp; **Opérateur
&amp; **Sortie\</p>
<hr class="docutils" />
<p>… de données de <span class="math notranslate nohighlight">\(\mathcal{S}_s\)</span></p>
<p>&amp; <span class="math notranslate nohighlight">\((\mathbf{x}_i,y_i)\in \mathcal{S}_s\)</span> &amp;
<span class="math notranslate nohighlight">\(t:\mathcal{X}\rightarrow \mathcal{X}\)</span> &amp; <span class="math notranslate nohighlight">\((\mathbf{t(x_i)},y_i)\)</span>\</p>
<p>… d’un ensemble de données non étiquetées</p>
<p>&amp; <span class="math notranslate nohighlight">\((\mathbf{x},-)\)</span> &amp;</p>
<p><span class="math notranslate nohighlight">\(h:\mathcal{X}\rightarrow \mathcal{Y}\)</span> entraîné sur <span class="math notranslate nohighlight">\(\mathcal{S}_s\)</span></p>
<p>&amp; <span class="math notranslate nohighlight">\((\mathbf{x},h(\mathbf{x}))\)</span>\</p>
<p>… d’un ensemble de données similaires</p>
<p>&amp; <span class="math notranslate nohighlight">\(\{(\mathbf{\hat{x}_j},\hat{y}_j)\}\)</span> &amp;</p>
<p>Opérateur de combinaison <span class="math notranslate nohighlight">\(c\)</span></p>
<p>&amp; <span class="math notranslate nohighlight">\((c(\{\mathbf{\hat{x}_j}\}),c(\{\hat{y}_j\}))\)</span>\</p>
</div>
<figure id="Fig:dataAug">
<img src="images/DataAugmentation" />
<figcaption>Exemple d’augmentation de données. De gauche à droite :
image originale, rotation de 20<span
class="math inline"><sup>∘</sup></span>, flip, ajout de bruit gaussien,
déformation élastique, changement de contraste par canal
RGB.</figcaption>
</figure>
<div class="docutils container" id="id13">
<div class="citation" id="id50" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>1<span class="fn-bracket">]</span></span>
<p>V. Nair and G. Hinton. Rectified linear units improve restricted boltzmann machines. In Johannes Fürnkranz and Thorsten Joachims, editors, <em>ICML</em>, 807–814. Omnipress, 2010. URL: <a class="reference external" href="http://dblp.uni-trier.de/db/conf/icml/icml2010.html#NairH10">http://dblp.uni-trier.de/db/conf/icml/icml2010.html#NairH10</a>.</p>
</div>
<div class="citation" id="id18" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>2<span class="fn-bracket">]</span></span>
<p>Xavier Glorot, Antoine Bordes, and Yoshua Bengio. Deep sparse rectifier neural networks. In Geoffrey J. Gordon, David B. Dunson, and Miroslav Dudík, editors, <em>AISTATS</em>, volume 15 of JMLR Proceedings, 315–323. JMLR.org, 2011. URL: <a class="reference external" href="http://dblp.uni-trier.de/db/journals/jmlr/jmlrp15.html#GlorotBB11">http://dblp.uni-trier.de/db/journals/jmlr/jmlrp15.html#GlorotBB11</a>.</p>
</div>
<div class="citation" id="id19" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>3<span class="fn-bracket">]</span></span>
<p>Sergey Ioffe and Christian Szegedy. Batch normalization: accelerating deep network training by reducing internal covariate shift. In Francis R. Bach and David M. Blei, editors, <em>ICML</em>, volume 37 of JMLR Workshop and Conference Proceedings, 448–456. JMLR.org, 2015. URL: <a class="reference external" href="http://dblp.uni-trier.de/db/conf/icml/icml2015.html#IoffeS15">http://dblp.uni-trier.de/db/conf/icml/icml2015.html#IoffeS15</a>.</p>
</div>
<div class="citation" id="id51" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>4<span class="fn-bracket">]</span></span>
<p>R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich feature hierarchies for accurate object detection and semantic segmentation. In <em>2014 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em>, volume 00, 580–587. June 2014. URL: <a class="reference external" href="https://ieeexplore.ieee.org/abstract/document/6909475/">https://ieeexplore.ieee.org/abstract/document/6909475/</a>, <a class="reference external" href="https://doi.org/10.1109/CVPR.2014.81">doi:10.1109/CVPR.2014.81</a>.</p>
</div>
<div class="citation" id="id16" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>5<span class="fn-bracket">]</span></span>
<p>Matthew D. Zeiler and Rob Fergus. Visualizing and understanding convolutional networks. <em>CoRR</em>, 2013. URL: <a class="reference external" href="http://dblp.uni-trier.de/db/journals/corr/corr1311.html#ZeilerF13">http://dblp.uni-trier.de/db/journals/corr/corr1311.html#ZeilerF13</a>.</p>
</div>
<div class="citation" id="id52" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>6<span class="fn-bracket">]</span></span>
<p>Ramprasaath R. Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, and Dhruv Batra. Grad-cam: visual explanations from deep networks via gradient-based localization. In <em>ICCV</em>, 618–626. IEEE Computer Society, 2017. URL: <a class="reference external" href="http://dblp.uni-trier.de/db/conf/iccv/iccv2017.html#SelvarajuCDVPB17">http://dblp.uni-trier.de/db/conf/iccv/iccv2017.html#SelvarajuCDVPB17</a>.</p>
</div>
<div class="citation" id="id53" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>7<span class="fn-bracket">]</span></span>
<p>Jason Yosinski, Jeff Clune, Anh Mai Nguyen, Thomas J. Fuchs, and Hod Lipson. Understanding neural networks through deep visualization. <em>CoRR</em>, 2015. URL: <a class="reference external" href="http://dblp.uni-trier.de/db/journals/corr/corr1506.html#YosinskiCNFL15">http://dblp.uni-trier.de/db/journals/corr/corr1506.html#YosinskiCNFL15</a>.</p>
</div>
<div class="citation" id="id29" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>8<span class="fn-bracket">]</span></span>
<p>D. Kingma and M. Welling. Auto-encoding variational bayes. <em>CoRR</em>, 2013. URL: <a class="reference external" href="http://dblp.uni-trier.de/db/journals/corr/corr1312.html#KingmaW13">http://dblp.uni-trier.de/db/journals/corr/corr1312.html#KingmaW13</a>.</p>
</div>
<div class="citation" id="id15" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>9<span class="fn-bracket">]</span></span>
<p>S. Hochreiter and J. Schmidhuber. Long short-term memory. <em>Neural Comput.</em>, 9(8):1735–1780, November 1997. URL: <a class="reference external" href="http://dx.doi.org/10.1162/neco.1997.9.8.1735">http://dx.doi.org/10.1162/neco.1997.9.8.1735</a>, <a class="reference external" href="https://doi.org/10.1162/neco.1997.9.8.1735">doi:10.1162/neco.1997.9.8.1735</a>.</p>
</div>
<div class="citation" id="id45" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>10<span class="fn-bracket">]</span></span>
<p>J Chung, Ç Gülçehre, K Cho, and Y Bengio. Empirical evaluation of gated recurrent neural networks on sequence modeling. <em>CoRR</em>, 2014. URL: <a class="reference external" href="http://arxiv.org/abs/1412.3555">http://arxiv.org/abs/1412.3555</a>.</p>
</div>
<div class="citation" id="id14" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>11<span class="fn-bracket">]</span></span>
<p>M. Schuster and K.K. Paliwal. Bidirectional recurrent neural networks. <em>Trans. Sig. Proc.</em>, 45(11):2673–2681, November 1997. URL: <a class="reference external" href="http://dx.doi.org/10.1109/78.650093">http://dx.doi.org/10.1109/78.650093</a>, <a class="reference external" href="https://doi.org/10.1109/78.650093">doi:10.1109/78.650093</a>.</p>
</div>
<div class="citation" id="id22" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>12<span class="fn-bracket">]</span></span>
<p>A Graves, G Wayne, and I Danihelka. Neural turing machines. <em>CoRR</em>, 2014. URL: <a class="reference external" href="http://arxiv.org/abs/1410.5401">http://arxiv.org/abs/1410.5401</a>.</p>
</div>
<div class="citation" id="id44" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id1">13</a><span class="fn-bracket">]</span></span>
<p>Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. Imagenet classification with deep convolutional neural networks. In <em>Advances in Neural Information Processing Systems</em>, 2012. 2012.</p>
</div>
<div class="citation" id="id42" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id3">14</a><span class="fn-bracket">]</span></span>
<p>Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. <em>CoRR</em>, 2014. URL: <a class="reference external" href="http://arxiv.org/abs/1409.1556">http://arxiv.org/abs/1409.1556</a>.</p>
</div>
<div class="citation" id="id41" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id4">15</a><span class="fn-bracket">]</span></span>
<p>Min Lin, Qiang Chen, and Shuicheng Yan. Network in network. <em>CoRR</em>, 2013. URL: <a class="reference external" href="http://arxiv.org/abs/1312.4400">http://arxiv.org/abs/1312.4400</a>.</p>
</div>
<div class="citation" id="id40" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id5">16</a><span class="fn-bracket">]</span></span>
<p>Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott E. Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. <em>CoRR</em>, 2014. URL: <a class="reference external" href="http://arxiv.org/abs/1409.4842">http://arxiv.org/abs/1409.4842</a>.</p>
</div>
<div class="citation" id="id39" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id6">17</a><span class="fn-bracket">]</span></span>
<p>Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna. Rethinking the inception architecture for computer vision. <em>CoRR</em>, 2015. URL: <a class="reference external" href="http://arxiv.org/abs/1512.00567">http://arxiv.org/abs/1512.00567</a>.</p>
</div>
<div class="citation" id="id48" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id7">18</a><span class="fn-bracket">]</span></span>
<p>François Chollet. Xception: deep learning with depthwise separable convolutions. <em>CoRR</em>, 2016. URL: <a class="reference external" href="http://arxiv.org/abs/1610.02357">http://arxiv.org/abs/1610.02357</a>, <a class="reference external" href="https://arxiv.org/abs/1610.02357">arXiv:1610.02357</a>.</p>
</div>
<div class="citation" id="id38" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id8">19</a><span class="fn-bracket">]</span></span>
<p>Christian Szegedy, Sergey Ioffe, and Vincent Vanhoucke. Inception-v4, inception-resnet and the impact of residual connections on learning. <em>CoRR</em>, 2016. URL: <a class="reference external" href="http://arxiv.org/abs/1602.07261">http://arxiv.org/abs/1602.07261</a>.</p>
</div>
<div class="citation" id="id37" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>20<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id9">1</a>,<a role="doc-backlink" href="#id10">2</a>)</span>
<p>Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. <em>CoRR</em>, 2015. URL: <a class="reference external" href="http://arxiv.org/abs/1512.03385">http://arxiv.org/abs/1512.03385</a>.</p>
</div>
<div class="citation" id="id35" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id12">21</a><span class="fn-bracket">]</span></span>
<p>Sinno Jialin Pan and Qiang Yang. A survey on transfer learning. <em>IEEE Trans. on Knowl. and Data Eng.</em>, 22(10):1345–1359, October 2010. URL: <a class="reference external" href="http://dx.doi.org/10.1109/TKDE.2009.191">http://dx.doi.org/10.1109/TKDE.2009.191</a>, <a class="reference external" href="https://doi.org/10.1109/TKDE.2009.191">doi:10.1109/TKDE.2009.191</a>.</p>
</div>
</div>
</div>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="rnn.html"
       title="page précédente">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">précédent</p>
        <p class="prev-next-title">Réseaux récurrents</p>
      </div>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contenu
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#quelques-reseaux-profonds-classiques">Quelques réseaux profonds classiques</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#alexnet">AlexNet</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#vgg">VGG</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#inception">Inception</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#resnet">ResNet</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#comment-utiliser-ces-reseaux">Comment utiliser ces réseaux ?</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#utilisation-de-reseaux-pre-entraines">Utilisation de réseaux pré-entraînés</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#transfer-learning-et-fine-tuning">Transfer learning et fine tuning</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#que-faire-si-j-ai-peu-de-donnees">Que faire si j’ai peu de données ?</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
Par Vincent BARRA
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=5b4479735964841361fd"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=5b4479735964841361fd"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>