
<!DOCTYPE html>

<html lang="fr">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Réseaux convolutifs &#8212; Apprentissage profond</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" href="_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script src="_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js"></script>
    <script src="_static/translations.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Recherche" href="search.html" />
    <link rel="prev" title="Perceptrons multicouches" href="PMC.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="fr">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="_static/logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Apprentissage profond</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search the docs ..." aria-label="Search the docs ..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Introduction
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="NN.html">
   Introduction aux réseaux de neurones
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="PMC.html">
   Perceptrons multicouches
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Réseaux convolutifs
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Theme by the <a href="https://ebp.jupyterbook.org">Executable Book Project</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#introduction">
   Introduction
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#inspiration-biologique">
     Inspiration biologique
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#convolution-discrete">
     Convolution discrète
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#definition-des-couches">
   Définition des couches
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#couche-de-convolution">
     Couche de convolution
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#couche-non-lineaire">
     Couche non linéaire
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#couches-de-normalisation">
     Couches de normalisation
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#couche-d-agregation-et-de-sous-echantillonnage">
     Couche d’agrégation et de sous-échantillonnage
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#couche-completement-connectee">
     Couche complètement connectée
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#regularisation">
   Régularisation
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#regularisation-de-la-fonction-de-cout">
     Régularisation de la fonction de coût
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#dropout">
     Dropout
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#partage-de-parametres">
     Partage de paramètres
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#initialisation">
   Initialisation
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#initialisation-prenant-en-compte-les-variations-des-neurones">
     Initialisation prenant en compte les variations des neurones
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#initialisation-de-he">
     Initialisation de He
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#apprentissage">
   Apprentissage
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#le-probleme-de-l-entrainement">
     Le problème de l’entraînement
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#apprentissage-glouton-par-couche">
     Apprentissage glouton par couche
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#visualisation-du-mecanisme-des-reseaux-convolutifs">
     Visualisation du mécanisme des réseaux convolutifs
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#methodes-de-visualisation-de-base">
     Méthodes de visualisation de base
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#methodes-fondees-sur-les-activations">
     Méthodes fondées sur les activations
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#methodes-fondees-sur-le-gradient">
     Méthodes fondées sur le gradient
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#quelques-applications-subsubsec-applis">
   Quelques applications {#subsubsec:applis}
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#la-classification-d-images">
     La classification d’images
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#l-annotation-de-scenes">
     L’annotation de scènes
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#la-reconnaissance-d-actions">
     La reconnaissance d’actions
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#l-analyse-de-documents">
     L’analyse de documents
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#l-augmentation-de-donnees">
     L’augmentation de données
    </a>
   </li>
  </ul>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Réseaux convolutifs</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#introduction">
   Introduction
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#inspiration-biologique">
     Inspiration biologique
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#convolution-discrete">
     Convolution discrète
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#definition-des-couches">
   Définition des couches
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#couche-de-convolution">
     Couche de convolution
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#couche-non-lineaire">
     Couche non linéaire
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#couches-de-normalisation">
     Couches de normalisation
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#couche-d-agregation-et-de-sous-echantillonnage">
     Couche d’agrégation et de sous-échantillonnage
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#couche-completement-connectee">
     Couche complètement connectée
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#regularisation">
   Régularisation
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#regularisation-de-la-fonction-de-cout">
     Régularisation de la fonction de coût
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#dropout">
     Dropout
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#partage-de-parametres">
     Partage de paramètres
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#initialisation">
   Initialisation
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#initialisation-prenant-en-compte-les-variations-des-neurones">
     Initialisation prenant en compte les variations des neurones
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#initialisation-de-he">
     Initialisation de He
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#apprentissage">
   Apprentissage
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#le-probleme-de-l-entrainement">
     Le problème de l’entraînement
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#apprentissage-glouton-par-couche">
     Apprentissage glouton par couche
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#visualisation-du-mecanisme-des-reseaux-convolutifs">
     Visualisation du mécanisme des réseaux convolutifs
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#methodes-de-visualisation-de-base">
     Méthodes de visualisation de base
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#methodes-fondees-sur-les-activations">
     Méthodes fondées sur les activations
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#methodes-fondees-sur-le-gradient">
     Méthodes fondées sur le gradient
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#quelques-applications-subsubsec-applis">
   Quelques applications {#subsubsec:applis}
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#la-classification-d-images">
     La classification d’images
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#l-annotation-de-scenes">
     L’annotation de scènes
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#la-reconnaissance-d-actions">
     La reconnaissance d’actions
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#l-analyse-de-documents">
     L’analyse de documents
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#l-augmentation-de-donnees">
     L’augmentation de données
    </a>
   </li>
  </ul>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <div class="tex2jax_ignore mathjax_ignore section" id="reseaux-convolutifs">
<h1>Réseaux convolutifs<a class="headerlink" href="#reseaux-convolutifs" title="Lien permanent vers ce titre">#</a></h1>
<div class="section" id="introduction">
<h2>Introduction<a class="headerlink" href="#introduction" title="Lien permanent vers ce titre">#</a></h2>
<div class="section" id="inspiration-biologique">
<h3>Inspiration biologique<a class="headerlink" href="#inspiration-biologique" title="Lien permanent vers ce titre">#</a></h3>
<p>Un réseau de neurones convolutif (CNN, <em>Convolutional Neural Network</em> ou
ConvNet) est un type de réseau de neurones artificiels acyclique à
propagation avant, dans lequel le motif de connexion entre les neurones
est inspiré par le cortex visuel des animaux. Les neurones de cette
région du cerveau sont arrangés de sorte à ce qu’ils correspondent à des
régions (appelés champs réceptifs) qui se chevauchent lors du pavage du
champ visuel. Ils sont de plus organisés de manière hiérarchique, en
couches (aire visuelle primaire V1, secondaire V2, puis aires V3, V4, V5
et V6, gyrus temporal inférieur), chacune des couches étant spécialisée
dans une tâche, de plus en plus abstraite en allant de l’entrée vers la
sortie. En simplifiant à l’extrême, une fois que les signaux lumineux
sont reçus par la rétine et convertis en potentiels d’action :</p>
<ul class="simple">
<li><p>L’aire primaire V1 s’intéresse principalement à la détection de
contours, ces contours étant définis comme des zones de fort
contraste de signaux visuels reçus.</p></li>
<li><p>L’aire V2 reçoit les informations de V1 et extrait des informations
telles que la fréquence spatiale, l’orientation, ou encore la
couleur.</p></li>
<li><p>L’aire V4, qui reçoit des informations de V2, mais aussi de V1
directement, détecte des caractéristiques plus complexes et
abstraites liées par exemple à la forme.</p></li>
<li><p>Le gyrus temporal inférieur est chargé de la partie sémantique
(reconnaissance des objets), à partir des informations reçues des
aires précédentes et d’une mémoire des informations stockées sur des
objets.</p></li>
</ul>
<p>L’architecture et le fonctionnement des réseaux convolutifs sont
inspirés par ces processus biologiques. Ces réseaux consistent en un
empilage multicouche de perceptrons, dont le but est de prétraiter de
petites quantités d’informations. Les réseaux convolutifs ont de larges
applications dans la reconnaissance d’image et vidéo, les systèmes de
recommandation et le traitement du langage naturel.</p>
<p>Un réseau convolutif se compose de deux types de neurones, agencés en
couches traitant successivement l’information. Dans le cas du traitement
de données de type images, on a ainsi :</p>
<ul class="simple">
<li><p>des <em>neurones de traitement</em>, qui traitent une portion limitée de
l’image (le champ réceptif) au travers d’une fonction de
convolution;</p></li>
<li><p>des <em>neurones</em> de mise en commun des sorties dits <em>d’agrégation
totale ou partielle</em> (<em>pooling</em>).</p></li>
</ul>
<p>Un traitement correctif non linéaire est appliqué entre chaque couche
pour améliorer la pertinence du résultat. L’ensemble des sorties d’une
couche de traitement permet de reconstituer une image intermédiaire,
dite carte de caractéristiques (feature map), qui sert de base à la
couche suivante. Les couches et leurs connexions apprennent des niveaux
d’abstraction croissants et extraient des caractéristiques de plus en
plus haut niveau des données d’entrée.</p>
<p>Dans la suite, le propos sera illustré sur des images 2D en niveaux de
gris, de taille <span class="math notranslate nohighlight">\(n_1 \times n_2\)</span> :</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
    \mathbf{I} : [\![1\cdots n_1]\!]\times [\![1\cdots n_2]\!] &amp;\rightarrow&amp;  \mathbb{R}\\
     (i,j) &amp;\mapsto&amp; I_{i,j}
\end{aligned}\end{split}\]</div>
<p><span class="math notranslate nohighlight">\(\mathbf{I}\)</span> sera indifféremment vue comme une fonction ou une matrice.</p>
</div>
<div class="section" id="convolution-discrete">
<h3>Convolution discrète<a class="headerlink" href="#convolution-discrete" title="Lien permanent vers ce titre">#</a></h3>
<p>Pour reproduire la notion de champ réceptif, et ainsi permettre aux
neurones de détecter des caractéristiques de petite taille mais porteurs
d’information, l’idée est de laisser un neurone caché voir et traiter
seulement une petite portion de l’image qu’il prend en entrée. L’outil
retenu dans les réseaux convolutifs est la convolution discrète.</p>
<div class="proof definition admonition" id="definition-0">
<p class="admonition-title"><span class="caption-number">Definition 5 </span> (Convolution discrète)</p>
<div class="definition-content section" id="proof-content">
<p>Soient
<span class="math notranslate nohighlight">\(h_1,h_2\in\mathbb{N}, \mathbf{K} \in \mathbb{R}^{(2h_1+1) \times (2h_2+1)}\)</span>.
La convolution discrète de <span class="math notranslate nohighlight">\(\mathbf{I}\)</span> par le filtre <span class="math notranslate nohighlight">\(\mathbf{K}\)</span> est
donnée par :</p>
<div class="math notranslate nohighlight">
\[\begin{aligned}
    \label{eq:convolution}
    \left(\mathbf{K} \ast \mathbf{I}\right)_{r,s} = \displaystyle\sum _{u = -h_1} ^{h_1} \displaystyle\sum _{v = -h_2}^{h_2} K_{u,v} I_{r+u,s+v}
\end{aligned}\]</div>
<p>où <span class="math notranslate nohighlight">\(\mathbf{K}\)</span> est donné par :</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
    \mathbf{K} =
    \begin{pmatrix}
        K_{-h_1,-h_2} &amp; \ldots &amp; K_{-h_1,h_2}\\
        \vdots &amp; K_{0,0} &amp; \vdots\\
        K_{h_1,-h_2} &amp; \ldots &amp; K_{h_1,h_2}\\
    \end{pmatrix}.
\end{aligned}\end{split}\]</div>
</div>
</div><p>La taille du filtre <span class="math notranslate nohighlight">\((2h_1+1) \times (2h_2+1)\)</span> précise le champ visuel
capturé et traité par <span class="math notranslate nohighlight">\(\mathbf{K}\)</span>.<br />
Lorsque <span class="math notranslate nohighlight">\(\mathbf{K}\)</span> parcourt <span class="math notranslate nohighlight">\(\mathbf{I}\)</span>, le déplacement du filtre est
réglé par deux paramètres de <em>stride</em> (horizontal et vertical). Un
stride de 1 horizontal (respectivement vertical) signifie que
<span class="math notranslate nohighlight">\(\mathbf{K}\)</span> se déplace d’une position horizontale (resp. verticale) à
chaque application de
<a class="reference external" href="#eq:convolution">[eq:convolution]</a>{reference-type= »ref »
reference= »eq:convolution »}. Les valeurs de stride peuvent également
être supérieures et ainsi sous-échantillonner <span class="math notranslate nohighlight">\(\mathbf{I}\)</span>.</p>
<p>Le comportement du filtre sur les bords de <span class="math notranslate nohighlight">\(\mathbf{I}\)</span> doit également
être précisé, par l’intermédiaire d’un paramètre de <em>padding</em>. Si
l’image convoluée <span class="math notranslate nohighlight">\(\left(\mathbf{K} \ast \mathbf{I}\right)\)</span> doit
posséder la même taille que <span class="math notranslate nohighlight">\(\mathbf{I}\)</span>, alors <span class="math notranslate nohighlight">\(2h_1\)</span> lignes de 0
(<span class="math notranslate nohighlight">\(h_1\)</span> en haut et <span class="math notranslate nohighlight">\(h_1\)</span> en bas) et <span class="math notranslate nohighlight">\(2h_2\)</span> colonnes de 0 (<span class="math notranslate nohighlight">\(h_2\)</span> à gauche
et <span class="math notranslate nohighlight">\(h_2\)</span> à droite) doivent être ajoutées. Dans le cas où la convolution
est réalisée sans padding, l’image convoluée est de taille
<span class="math notranslate nohighlight">\((n_1-2h_1)\times (n_2-2h_2)\)</span>.</p>
</div>
</div>
<div class="section" id="definition-des-couches">
<h2>Définition des couches<a class="headerlink" href="#definition-des-couches" title="Lien permanent vers ce titre">#</a></h2>
<p>Nous introduisons ici les différents types de couches utilisées dans les
réseaux convolutifs. L’assemblage de ces couches permet de construire
des architectures complexes pour la classification ou la régression,
dont certaines seront précisées dans un prochain cours.</p>
<div class="section" id="couche-de-convolution">
<h3>Couche de convolution<a class="headerlink" href="#couche-de-convolution" title="Lien permanent vers ce titre">#</a></h3>
<div class="figure align-default" id="cnn1">
<img alt="_images/cnn1.png" src="_images/cnn1.png" />
<p class="caption"><span class="caption-number">Fig. 10 </span><span class="caption-text">Illustration des calculseffectués dans une opération de convolution discrète. Le pixel (2,2) de
l’image <span class="math notranslate nohighlight">\({\mathbf Y_i^{l}}\)</span> est une combinaison linéaire des pixels <span class="math notranslate nohighlight">\((i,j)\in[\![1,3]\!]^2\)</span> de <span class="math notranslate nohighlight">\({\mathbf Y_i^{l-1}}\)</span> les coefficients de la combinaison étant portés par le filtre <span class="math notranslate nohighlight">\(\mathbf K\)</span>.</span><a class="headerlink" href="#cnn1" title="Lien permanent vers cette image">#</a></p>
</div>
<p>Soit <span class="math notranslate nohighlight">\(l\in\mathbb{N}\)</span> une couche de convolution. L’entrée de la couche
<span class="math notranslate nohighlight">\(l\)</span> est composée de <span class="math notranslate nohighlight">\(n^{(l-1)}\)</span> cartes provenant de la couche
précédente, de taille <span class="math notranslate nohighlight">\(n_1^{(l-1)} \times n_2^{(l-1)}\)</span>. Dans le cas de
la couche d’entrée du réseau (<span class="math notranslate nohighlight">\(l = 1\)</span>), l’entrée est l’image
<span class="math notranslate nohighlight">\(\mathbf{I}\)</span>. La sortie de la couche <span class="math notranslate nohighlight">\(l\)</span> est formée de <span class="math notranslate nohighlight">\(n^{(l)}\)</span> cartes
de taille <span class="math notranslate nohighlight">\(n_1^{(l)} \times n_2^{(l)}\)</span>. La <span class="math notranslate nohighlight">\(i^{\text{e}}\)</span> carte de la
couche <span class="math notranslate nohighlight">\(l\)</span>, notée <span class="math notranslate nohighlight">\(\mathbf{Y_i^{(l)}}\)</span>, se calcule comme :
$<span class="math notranslate nohighlight">\(\begin{aligned}
    \label{eq:convlayer}
    \mathbf{Y_i^{(l)}} = \mathbf{B^{(l)}_{i}} + \displaystyle\sum _{j = 1}^{n^{(l-1)}} \mathbf{K^{(l)}_{i,j}} \ast \mathbf{Y_j^{(l-1)}}
\end{aligned}\)</span><span class="math notranslate nohighlight">\( où \)</span>\mathbf{B_i^{(l)}}<span class="math notranslate nohighlight">\( est une matrice de biais et
\)</span>\mathbf{K^{(l)}_{i,j}}<span class="math notranslate nohighlight">\( est le filtre de taille
\)</span>(2h_1^{(l)} + 1) \times (2h_2^{(l)} + 1)<span class="math notranslate nohighlight">\( connectant la \)</span>j^{\text{e}}<span class="math notranslate nohighlight">\(
carte de la couche \)</span>(l-1)<span class="math notranslate nohighlight">\( à la \)</span>i^{\text{e}}<span class="math notranslate nohighlight">\( carte de la couche \)</span>l$
(voir la figure <a class="reference external" href="#F:coucheconv">[F:coucheconv]</a>{reference-type= »ref »
reference= »F:coucheconv »}).</p>
<p><span class="math notranslate nohighlight">\(n_1^{(l)}\)</span> et <span class="math notranslate nohighlight">\(n_2^{(l)}\)</span> doivent prendre en compte les effets de bords
: lors du calcul de la convolution, seuls les pixels dont la somme est
définie avec des indices positifs doivent être traités. Dans le cas où
le padding n’est pas utilisé, les cartes de sortie ont donc une taille
de <span class="math notranslate nohighlight">\(n_1^{(l)} = n_1^{(l-1)} - 2h_1^{(l)}\)</span> et
<span class="math notranslate nohighlight">\(n_2^{(l)} = n_2^{(l-1)} - 2h_2^{(l)}\)</span>.</p>
<p>Souvent, les filtres utilisés pour calculer <span class="math notranslate nohighlight">\(\mathbf{Y_i^{(l)}}\)</span> sont
les mêmes, i.e. <span class="math notranslate nohighlight">\(\mathbf{K_{i,j}^{(l)}} = \mathbf{K _{i,k}^{(l)}}\)</span> pour
<span class="math notranslate nohighlight">\(j \neq k\)</span>. De plus, la somme dans l’équation
<a class="reference external" href="#eq:convlayer">[eq:convlayer]</a>{reference-type= »eqref »
reference= »eq:convlayer »} peut être conduite sur un sous ensemble des
cartes d’entrée.</p>
<p>Il est possible de mettre en correspondance la couche de convolution et
l’opération <a class="reference external" href="#eq:convlayer">[eq:convlayer]</a>{reference-type= »eqref »
reference= »eq:convlayer »} qu’elle effectue, avec un perceptron
multicouche. Pour cela, il suffit de réécrire l’équation
<a class="reference external" href="#eq:convlayer">[eq:convlayer]</a>{reference-type= »eqref »
reference= »eq:convlayer »} : chaque carte <span class="math notranslate nohighlight">\(\mathbf{Y_i^{(l)}}\)</span> de la
couche <span class="math notranslate nohighlight">\(l\)</span> est formée de <span class="math notranslate nohighlight">\(n_1^{(l)} \cdot n_2^{(l)}\)</span> neurones organisés
dans un tableau à deux dimensions. Le neurone en position <span class="math notranslate nohighlight">\((r,s)\)</span>
calcule : $<span class="math notranslate nohighlight">\(\begin{aligned}
    \left(\mathbf{Y_i^{(l)}}\right)_{r,s} &amp;= \left(\mathbf{B_i^{(l)}}\right)_{r,s} + \displaystyle\sum _{j = 1}^{n^{(l-1)}} \left(\mathbf{K^{(l)}_{i,j} }\ast \mathbf{Y_j^{(l-1)}}\right)_{r,s}\\
    &amp;= \left(\mathbf{B_i^{(l)}}\right)_{r,s} + \displaystyle\sum _{j = 1}^{n^{(l-1)}} \displaystyle\sum _{u = - h_1^{(l)}} ^{h_1^{(l)}} \displaystyle\sum _{v = - h_2^{(l)}} ^{h_2^{(l)}} \left(\mathbf{K^{(l)}_{i,j}}\right)_{u,v} \left(\mathbf{Y_j^{(l-1)}}\right)_{r+u,s+v}
\end{aligned}\)</span>$</p>
<p>Les paramètres du réseau à entraîner (poids) peuvent alors être trouvés
dans les filtres <span class="math notranslate nohighlight">\(\mathbf{K^{(l)}_{i,j}}\)</span> et les matrices de biais
<span class="math notranslate nohighlight">\(\mathbf{B_i^{(l)}}\)</span>.</p>
<p>Comme nous le verrons dans la section
<a class="reference external" href="#subsubsec:coucheagreg">[subsubsec:coucheagreg]</a>{reference-type= »ref »
reference= »subsubsec:coucheagreg »}, un sous-échantillonnage est utilisé
pour diminuer l’influence du bruit et des distorsions dans les images.
Le sous-échantillonnage peut être également réalisé simplement avec des
paramètres de stride, en sautant un nombre fixe de pixels dans les
dimensions horizontale (saut <span class="math notranslate nohighlight">\(s_1^{(l)}\)</span>) et verticale (saut
<span class="math notranslate nohighlight">\(s_2^{(l)}\)</span>) avant d’appliquer de nouveau le filtre. La taille des
images de sortie est alors : $<span class="math notranslate nohighlight">\(\begin{aligned}
    n_1^{(l)} = \frac{n_1^{(l-1)} - 2h_1^{(l)}}{s_1^{(l)} + 1}\quad \text{ et }\quad n_2^{(l)} = \frac{n_2^{(l-1)} - 2h_2^{(l)}}{s_2^{(l)} + 1}.
\end{aligned}\)</span>$</p>
<div class="highlight-SCfigure notranslate"><div class="highlight"><pre><span></span>![image](images/Fig10-2-new)
</pre></div>
</div>
<p>Un point clé des réseaux convolutifs est d’exploiter la corrélation
spatiale des données. L’utilisation des noyaux permet d’alléger le
modèle, plutôt que d’utiliser des couches complètement connectées.</p>
</div>
<div class="section" id="couche-non-lineaire">
<h3>Couche non linéaire<a class="headerlink" href="#couche-non-lineaire" title="Lien permanent vers ce titre">#</a></h3>
<p>[]{#S:nonlinl label= »S:nonlinl »} Pour augmenter le pouvoir d’expression
des réseaux profonds, on utilise des couches non linéaires. Les entrées
d’une couche non linéaire sont <span class="math notranslate nohighlight">\(n^{(l-1)}\)</span> cartes et ses sorties
<span class="math notranslate nohighlight">\(n^{(l)} = n^{(l-1)}\)</span> cartes <span class="math notranslate nohighlight">\(\mathbf{Y_i^{(l)}}\)</span>, de taille
<span class="math notranslate nohighlight">\(n_1^{(l-1)} \times n_2^{(l-1)}\)</span> telles que <span class="math notranslate nohighlight">\(n_1^{(l)} = n_1^{(l-1)}\)</span> et
<span class="math notranslate nohighlight">\(n_2^{(l)} = n_2^{(l-1)}\)</span>, données par
<span class="math notranslate nohighlight">\(\mathbf{Y_i^{(l)}}\)</span> = <span class="math notranslate nohighlight">\(f \left(\mathbf{Y_i^{(l-1)}}\right)\)</span>, où <span class="math notranslate nohighlight">\(f\)</span> est
la fonction d’activation utilisée dans la couche <span class="math notranslate nohighlight">\(l\)</span>. Le tableau
<a class="reference external" href="#T:activation">[T:activation]</a>{reference-type= »ref »
reference= »T:activation »} propose quelques fonctions d’activation
usuelles.</p>
<div class="highlight-tabularx notranslate"><div class="highlight"><pre><span></span>\|\&gt;Y\| \&gt;Z\| \&gt;Z\| \&gt;Z\| **Nom** &amp; **Graphe** &amp; &amp; **$f&#39;$**\

Rampe &amp; &amp; $$f(x) = x$$ &amp; $f&#39;(x) = 1$\

Heaviside &amp; &amp; $$f(x) =
\begin{cases}
\;\; 0 &amp; \text{si $x \; &lt; \; 0$}  \\
\;\; 1 &amp; \text{si $x \; \geq \; 0$}
\end{cases}$$ &amp; $$f&#39;(x) =
\begin{cases}
\;\; 0 &amp; \text{si $x \; \neq \; 0$}  \\
\;\; ? &amp; \text{si $x \; = \; 0$}
\end{cases}$$\

Logistique ou sigmoı̈de &amp; &amp; $$f(x) \; = \; \frac{1}{1 + e^{-x}}$$ &amp;
$f&#39;(x) \; = \; f(x) \, \bigl(1 \, - \, f(x)\bigr)$\

Tangente hyperbolique&amp; &amp; $$\begin{aligned}
f(x) \; &amp;= \; \tanh(x) \\
&amp; = \;  \frac{2}{1 + e^{-2x}} - 1
\end{aligned}$$ &amp; $$f&#39;(x) \; = \; 1 \, - \, f^2(x)$$\

Arc Tangente&amp; &amp; $$f(x) \; = \; \tan^{-1}(x)$$ &amp;
$$f&#39;(x) \; = \; \frac{1}{x^2 + 1}$$\

ReLU&amp; &amp; $$f(x) =
\begin{cases}
\;\; 0 &amp; \text{si $x \; &lt; \; 0$}  \\
\;\; x &amp; \text{si $x \; \geq \; 0$}
\end{cases}$$ &amp; $$f&#39;(x) =
\begin{cases}
\;\; 0 &amp; \text{si $x \; \neq \; 0$}  \\
\;\; 1 &amp; \text{si $x \; = \; 0$}
\end{cases}$$\

Exponentielle Linéaire&amp; &amp; $$f(x) =
\begin{cases}
\;\; \alpha \, (e^x \, - \, 1) &amp; \text{si $x \; &lt; \; 0$}  \\
\;\; x &amp; \text{si $x \; \geq \; 0$}
\end{cases}$$ &amp; $$f&#39;(x) =
\begin{cases}
\;\; f(x) \, + \, \alpha &amp; \text{si $x \; &lt; \; 0$}  \\
\;\; 1 &amp; \text{si $x \; \geq \; 0$}
\end{cases}$$\
</pre></div>
</div>
<p>En apprentissage profond, il a été reporté que la sigmoïde et la
tangente hyperbolique avaient des performances moindres que la fonction
d’activation <em>softsign</em> : $<span class="math notranslate nohighlight">\(\begin{aligned}
    \mathbf{Y_i^{(l)}}  = \frac{1}{1+ \left|\mathbf{Y_i^{(l-1)}} \right |}.
\end{aligned}\)</span><span class="math notranslate nohighlight">\( En effet, les valeurs des pixels des cartes
\)</span>\mathbf{Y_i^{(l-1)}}<span class="math notranslate nohighlight">\( arrivant près des paliers de saturation de ces
fonctions donnent des gradients faibles, qui ont tendance à s'annuler
(problème du *gradient évanescent* ou *vanishing gradient*) lors de la
phase d'apprentissage par rétropropagation du gradient. Une autre
fonction, non saturante elle, est très largement utilisée. Il s'agit de
la fonction ReLU (Rectified Linear Unit) [&#64;Nair10] : \)</span><span class="math notranslate nohighlight">\(\begin{aligned}
    \label{eq:relu}
\mathbf{Y_i^{(l)}} = max\left (0,\mathbf{Y_i^{(l-1)}}\right )
\end{aligned}\)</span>$ Les neurones utilisant la fonction décrite dans
l’équation <a class="reference external" href="#eq:relu">[eq:relu]</a>{reference-type= »eqref »
reference= »eq:relu »} sont appelés neurones linéaires rectifiés. Glorot
et Bengio [&#64;Glorot11] ont montré que l’utilisation d’une couche ReLU en
tant que couche non linéaire permettait un entraînement efficace de
réseaux profonds sans pré-entraînement non supervisé. Plusieurs
variantes de cette fonction existent, par exemple pour assurer une
différentiabilité en 0 ou pour proposer des valeurs non nulles pour des
valeurs négatives de l’argument. La figure
<a class="reference external" href="#F:plotactiv">1.2</a>{reference-type= »ref » reference= »F:plotactiv »}.
illustre quelques unes de ces fonctions d’activation.</p>
<figure id="F:plotactiv">
<table>
<tbody>
<tr class="odd">
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr class="even">
<td style="text-align: center;"><span
class="math inline"><em>R</em><em>e</em><em>L</em><em>U</em>(<em>x</em>) = <em>m</em><em>a</em><em>x</em>(0,<em>x</em>)</span></td>
<td style="text-align: center;"><span
class="math inline">$LeakyReLU(x,\alpha) = \left \lbrace
\begin{array}{cc}
   x &amp; si\ x&gt;0  \\
   \alpha x &amp; sinon
\end{array}
\right .$</span></td>
</tr>
<tr class="odd">
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr class="even">
<td style="text-align: center;"><span class="math inline">$elu(x,\alpha)
=
\left \lbrace
\begin{array}{cc}
   x &amp; si\ x&gt;0  \\
   \alpha(e^x-1) &amp; sinon
\end{array}
\right .$</span></td>
<td style="text-align: center;"><span
class="math inline">$SeLU(x,\alpha,\lambda) =
\left \lbrace
\begin{array}{cc}
   \lambda x &amp; si\ x&gt;0  \\
   \lambda\alpha(e^x-1) &amp; sinon
\end{array}
\right .$</span></td>
</tr>
</tbody>
</table>
<figcaption>Quelques fonctions d’activation</figcaption>
</figure>
</div>
<div class="section" id="couches-de-normalisation">
<h3>Couches de normalisation<a class="headerlink" href="#couches-de-normalisation" title="Lien permanent vers ce titre">#</a></h3>
<p>La normalisation prend aujourd’hui une place de plus en plus importante,
notamment depuis les travaux de Ioffe et Szegedy [&#64;Ioffe15]. Les auteurs
suggèrent qu’un changement dans la distribution des activations d’un
réseau profond, résultant de la présentation d’un nouveau mini batch
d’exemples, ralentit le processus d’apprentissage. Pour pallier ce
problème, chaque activation du mini batch est centrée et normée
(variance unité), la moyenne et la variance étant calculées sur le mini
batch entier, indépendamment pour chaque activation. Des paramètres
d’offset <span class="math notranslate nohighlight">\(\beta\)</span> et multiplicatif <span class="math notranslate nohighlight">\(\gamma\)</span> sont alors appliqués pour
normaliser les données d’entrée
(algorithme <a class="reference external" href="#A:bn1">[A:bn1]</a>{reference-type= »ref »
reference= »A:bn1 »}).</p>
<div class="highlight-algorithm notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<p>Lorsque la descente de gradient est achevée, un post apprentissage est
appliqué dans lequel la moyenne et la variance sont calculées sur
l’ensemble d’entraînement et remplacent <span class="math notranslate nohighlight">\(\mu_\mathcal{B}\)</span> et
<span class="math notranslate nohighlight">\(\sigma^2_\mathcal{B}\)</span> (algorithme
<a class="reference external" href="#A:bn2">[A:bn2]</a>{reference-type= »ref » reference= »A:bn2 »}).</p>
<div class="highlight-algorithm notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
</div>
<div class="section" id="couche-d-agregation-et-de-sous-echantillonnage">
<h3>Couche d’agrégation et de sous-échantillonnage<a class="headerlink" href="#couche-d-agregation-et-de-sous-echantillonnage" title="Lien permanent vers ce titre">#</a></h3>
<p>[]{#subsubsec:coucheagreg label= »subsubsec:coucheagreg »}</p>
<div class="highlight-SCfigure notranslate"><div class="highlight"><pre><span></span>![image](images/fig-agregation2.pdf)
</pre></div>
</div>
<p>Le sous-échantillonnage (pooling) des cartes obtenues par les couches
précédentes a pour objectif d’assurer une robustesse au bruit et aux
distorsions.</p>
<p>La sortie d’une couche d’agrégation <span class="math notranslate nohighlight">\(l\)</span>
(figure <a class="reference external" href="#F:coucheagreg">[F:coucheagreg]</a>{reference-type= »ref »
reference= »F:coucheagreg »}) est composée de <span class="math notranslate nohighlight">\(n^{(l)} = n^{(l-1)}\)</span> cartes
de taille réduite. En général, l’agrégation est effectuée en déplaçant
dans les cartes d’entrée une fenêtre de taille <span class="math notranslate nohighlight">\(2p \times 2p\)</span> toutes les
<span class="math notranslate nohighlight">\(q\)</span> positions (il y a recouvrement si <span class="math notranslate nohighlight">\(q &lt; p\)</span> et non recouvrement
sinon), et en calculant, pour chaque position de la fenêtre, une seule
valeur, affectée à la position centrale dans la carte de sortie. On
distingue généralement deux types d’agrégation :</p>
<p>La moyenne</p>
<p>:   : on utilise un filtre <span class="math notranslate nohighlight">\(\mathbf{K_B}\)</span> de taille
<span class="math notranslate nohighlight">\((2h_1 + 1)\times (2h_2 + 1)\)</span> défini par :
$<span class="math notranslate nohighlight">\(\left(\mathbf{K_B}\right)_{r,s} = \frac{1}{(2h_1 + 1)(2h_2 + 1)}\)</span>$</p>
<p>Le maximum</p>
<p>:   : la valeur maximum dans la fenêtre est retenue.</p>
<p>Le maximum est souvent utilisé pour assurer une convergence rapide
durant la phase d’entraînement. L’agrégation avec recouvrement, elle,
semble assurer une réduction du phénomène de surapprentissage</p>
</div>
<div class="section" id="couche-completement-connectee">
<h3>Couche complètement connectée<a class="headerlink" href="#couche-completement-connectee" title="Lien permanent vers ce titre">#</a></h3>
<p>Si <span class="math notranslate nohighlight">\(l\)</span> et <span class="math notranslate nohighlight">\((l-1)\)</span> sont des couches complètement connectées, l’équation :
$<span class="math notranslate nohighlight">\(\begin{aligned}
    z_i^{(l)} = \sum _{k = 0} ^{m^{(l-1)}} w_{i,k}^{(l)} y_k^{(l-1)}\quad \text{ ou }\quad \mathbf{Z^{(l)}} = \mathbf{W^{(l)}} \mathbf{Y^{(l-1)}}
\end{aligned}\)</span><span class="math notranslate nohighlight">\( avec \)</span>\mathbf{Z^{(l)}}<span class="math notranslate nohighlight">\(, \)</span>\mathbf{W^{(l)}}<span class="math notranslate nohighlight">\( et
\)</span>\mathbf{Y^{(l-1)}}<span class="math notranslate nohighlight">\( les représentations vectorielle et matricielle des
entrées \)</span>z_i^{(l)}<span class="math notranslate nohighlight">\(, des poids \)</span>w_{i,k}^{(l)}<span class="math notranslate nohighlight">\( et des sorties
\)</span>y_k^{(l-1)}$, permet de relier ces deux couches.</p>
<p>Dans le cas contraire, la couche <span class="math notranslate nohighlight">\(l\)</span> attend <span class="math notranslate nohighlight">\(n^{(l-1)}\)</span> entrées de
taille <span class="math notranslate nohighlight">\(n_1^{(l-1)} \times n_2^{(l-1)}\)</span> et le <span class="math notranslate nohighlight">\(i^{\text{e}}\)</span> neurone de
la couche <span class="math notranslate nohighlight">\(l\)</span> calcule : $<span class="math notranslate nohighlight">\(\begin{aligned}
    y_i^{(l)} = f\left(z_i^{(l)}\right)\quad\text{ avec }\quad z_i^{(l)} = \displaystyle\sum _{j = 1}^{n^{(l-1)}} \displaystyle\sum _{r = 1} ^{n_1^{(l-1)}} \displaystyle\sum _{s = 1}^{n_2^{(l-1)}} w_{i,j,r,s}^{(l)} \left(\mathbf{ Y_j^{(l-1)}} \right)_{r,s}
\end{aligned}\)</span><span class="math notranslate nohighlight">\( où \)</span>w_{i,j,r,s}^{(l)}<span class="math notranslate nohighlight">\( est le poids connectant le
neurone en position \)</span>(r,s)<span class="math notranslate nohighlight">\( de la \)</span>j^{\text{e}}<span class="math notranslate nohighlight">\( carte de la couche
\)</span>(l - 1)<span class="math notranslate nohighlight">\( au \)</span>i^{\text{e}}<span class="math notranslate nohighlight">\( neurone de la couche \)</span>l$.</p>
<p>En pratique, les réseaux convolutifs sont utilisés pour apprendre une
hiérarchie dans les données et la (ou les) couche(s) complètement
connectée(s) est(sont) utilisée(s) en bout de réseau pour des tâches de
classification ou de régression.</p>
<p>Une couche de classification classiquement mise en œuvre utilise le
classifieur <em>softmax</em>, qui généralise la régression logistique au cas
multiclasse (<span class="math notranslate nohighlight">\(k\)</span> classes). L’ensemble d’apprentissage
<span class="math notranslate nohighlight">\({\mathcal E}_a = \left \{(\mathbf{x^{(i)}}, y^{(i)}),i \in[\![1\cdots m]\!]\right \}\)</span>
est donc tel que <span class="math notranslate nohighlight">\(y^{(i)}\in[\![1\cdots k]\!]\)</span> et le classifieur estime
la probabilité <span class="math notranslate nohighlight">\(P(y^{(i)}=j |\mathbf{x^{(i)}})\)</span> pour chaque classe
<span class="math notranslate nohighlight">\(1\leq j\leq k\)</span>. Le classifieur softmax calcule cette probabilité selon
:
$<span class="math notranslate nohighlight">\(\forall j\in[\![1\cdots k]\!]\quad P(y^{(i)}=j | \mathbf{x^{(i)}},\mathbf{W}) = \frac{e^{\mathbf{W_j^\top x^{(i)}}}}{\displaystyle\sum_{l=1}^k e^{\mathbf{W_l^\top}\mathbf{x^{(i)}}}}\)</span><span class="math notranslate nohighlight">\(
où \)</span>\mathbf{W}<span class="math notranslate nohighlight">\( est la matrice des paramètres du modèle (les poids). Ces
paramètres sont obtenus en minimisant une fonction de coût, qui peut par
exemple s'écrire :
\)</span><span class="math notranslate nohighlight">\(J(\mathbf{W}) =- \frac{1}{m}\displaystyle\sum_{i=1}^m \displaystyle\sum_{j=1}^k \mathbb{I}_{y^{(i)}=j}log\left ( \frac{e^{\mathbf{W_j^\top x^{(i)}}}}{\displaystyle\sum_{l=1}^k e^{\mathbf{W_l^\top x^{(i)}}}}\right ) + \frac{\lambda}{2}\displaystyle\sum_{i=1}^n \displaystyle\sum_{j=1}^k W_{ji}^2
\label{E:softmaxCout}\)</span><span class="math notranslate nohighlight">\( où \)</span>\lambda<span class="math notranslate nohighlight">\( est un paramètre de régularisation
contrôlant le second terme du coût qui pénalise les grandes valeurs des
poids (régularisation \)</span>\ell_2$).</p>
</div>
</div>
<div class="section" id="regularisation">
<h2>Régularisation<a class="headerlink" href="#regularisation" title="Lien permanent vers ce titre">#</a></h2>
<p>Un des enjeux principaux en apprentissage automatique est de construire
des algorithmes ayant une bonne capacité de généralisation. Les
stratégies mises en œuvre pour arriver à cette fin rentrent dans la
catégorie générale de la régularisation et de nombreuses méthodes sont
aujourd’hui proposées en ce sens. Nous faisons ici un focus sur trois
stratégies largement utilisées en apprentissage profond.</p>
<div class="section" id="regularisation-de-la-fonction-de-cout">
<h3>Régularisation de la fonction de coût<a class="headerlink" href="#regularisation-de-la-fonction-de-cout" title="Lien permanent vers ce titre">#</a></h3>
<p>L’équation <a class="reference external" href="#E:softmaxCout">[E:softmaxCout]</a>{reference-type= »ref »
reference= »E:softmaxCout »} est un exemple de régularisation de la
fonction de coût, utilisée lors de la phase d’entraînement. À la
fonction d’erreur est ajoutée une fonction des poids du réseau, qui peut
prendre de multiples formes. Les deux principales stratégies sont :</p>
<ul class="simple">
<li><p>La régularisation <span class="math notranslate nohighlight">\(\ell_2\)</span> (ou ridge regression), qui force les
poids à avoir une faible valeur absolue : un terme de régularisation
fonction de la norme <span class="math notranslate nohighlight">\(\ell_2\)</span> de la matrice des poids est ajouté (à
la manière de l’équation
<a class="reference external" href="#E:softmaxCout">[E:softmaxCout]</a>{reference-type= »ref »
reference= »E:softmaxCout »}). On parle souvent de <em>weight decay</em>.</p></li>
<li><p>La régularisation <span class="math notranslate nohighlight">\(\ell_1\)</span>, qui tend à rendre épars le réseau
profond, <em>i.e.</em> à imposer à un maximum de poids de s’annuler. Un
terme de régularisation, somme pondérée des valeurs absolues des
poids, est ajouté à la fonction objectif.</p></li>
</ul>
</div>
<div class="section" id="dropout">
<h3>Dropout<a class="headerlink" href="#dropout" title="Lien permanent vers ce titre">#</a></h3>
<p>Les techniques de dropout se rapprochent des stratégies classiques de
bagging en apprentissage automatique. L’objectif est d’entraîner un
ensemble constitué de tous les sous-réseaux qui peuvent être construits
en supprimant des neurones (hors neurones d’entrée et de sortie) du
réseau initial. Si le réseau comporte <span class="math notranslate nohighlight">\(|\mathbf{W}|\)</span> neurones cachés, il
existe ainsi <span class="math notranslate nohighlight">\(2^{|\mathbf{W}|}\)</span> modèles possibles. En pratique, les
neurones cachés se voient perturbés par un bruit binomial, qui a pour
effet de les empêcher de fonctionner en groupe et de les rendre, au
contraire, plus indépendants. Le phénomène de surapprentissage est ainsi
fortement réduit sur le réseau, qui doit décomposer les entrées en
caractéristiques pertinentes, indépendamment les unes des autres. Les
réseaux construits par dropout partagent partiellement leurs paramètres,
ce qui diminue l’empreinte mémoire de la méthode.</p>
<p>Lors de la phase de prédiction, le réseau complet est utilisé, mais les
neurones cachés sont pondérés par la fraction de bruit utilisé pendant
l’apprentissage (<em>i.e.</em> pour chaque neurone le nombre de fois où il a
été supprimé d’un sous-réseau, rapporté au nombre total de réseaux),
afin de conserver la valeur moyenne des activations des neurones
identiques à celles durant l’apprentissage.</p>
<p>Notons qu’il est également possible d’éteindre non pas un neurone, mais
un poids. La stratégie correspondante est appelée DropConnect.</p>
</div>
<div class="section" id="partage-de-parametres">
<h3>Partage de paramètres<a class="headerlink" href="#partage-de-parametres" title="Lien permanent vers ce titre">#</a></h3>
<p>La régularisation de la fonction de coût permet d’imposer aux poids
certaines contraintes (par exemple de rester faibles en amplitude pour
la régularisation <span class="math notranslate nohighlight">\(\ell_2\)</span>, ou de s’annuler pour la régularisation
<span class="math notranslate nohighlight">\(\ell_1\)</span>). Il peut également être intéressant d’imposer certains a
priori sur les poids, par exemple une dépendance entre les valeurs des
paramètres.</p>
<p>Une dépendance classique consiste à imposer que les valeurs de certains
poids soient proches les unes des autres (dans le cas par exemple où
deux modèles de classification <span class="math notranslate nohighlight">\(M_1\)</span> et <span class="math notranslate nohighlight">\(M_2\)</span>, de paramètres
<span class="math notranslate nohighlight">\(\mathbf{W_1}\)</span> et <span class="math notranslate nohighlight">\(\mathbf{W_2}\)</span>, opèrent sur des données similaires et
sur des classes identiques) et, là encore, une stratégie de pénalisation
de la fonction objectif peut être utilisée. Cependant, il est plus
courant dans ce cas d’imposer que les paramètres soient égaux (dans
l’exemple précédent imposer <span class="math notranslate nohighlight">\(\mathbf{W_1}=\mathbf{W_2}\)</span>) et d’arriver à
une stratégie dite de partage des paramètres. Dans le cas des réseaux
convolutifs utilisés en vision, cette régularisation est assez intuitive
puisque les entrées (images) possèdent de nombreuses propriétés
invariantes par transformations affines (une image de voiture reste une
image de voiture, même si l’image est translatée ou mise à l’échelle,
cf. figure <a class="reference external" href="#F:partage">[F:partage]</a>{reference-type= »ref »
reference= »F:partage »}). Le réseau exploite alors ce partage de
paramètres, en calculant une même caractéristique (un neurone et son
poids) à différentes positions dans l’image. De ce fait, le nombre de
paramètres est drastiquement réduit, ainsi que l’empreinte mémoire du
réseau appris.</p>
<div class="highlight-SCfigure notranslate"><div class="highlight"><pre><span></span>![image](images/fig-partage-param.pdf){width=&quot;0.40\\linewidth&quot;}
</pre></div>
</div>
</div>
</div>
<div class="section" id="initialisation">
<h2>Initialisation<a class="headerlink" href="#initialisation" title="Lien permanent vers ce titre">#</a></h2>
<p>Une initialisation convenable des poids est essentielle pour assurer une
convergence de la phase d’entraînement. Un choix arbitraire des poids (à
zéro, à de petites ou grandes valeurs aléatoires) peut ralentir, voire
causer de la redondance dans le réseau (problème de la symétrie).<br />
Plusieurs schémas d’initialisation ont été proposés et nous donnons dans
les deux paragraphes qui suivent d’eux d’entre eux.</p>
<div class="section" id="initialisation-prenant-en-compte-les-variations-des-neurones">
<h3>Initialisation prenant en compte les variations des neurones<a class="headerlink" href="#initialisation-prenant-en-compte-les-variations-des-neurones" title="Lien permanent vers ce titre">#</a></h3>
<p>Pour illustrer le propos, on suppose que l’entrée du réseau de neurones
est composée de <span class="math notranslate nohighlight">\(n^{(1)}\)</span> entrées
<span class="math notranslate nohighlight">\(\mathbf{x}=(x_1\ldots x_{n^{(1)}})^\top\)</span>, les <span class="math notranslate nohighlight">\(x_i\)</span> étant i.i.d,
normalisés selon une loi <span class="math notranslate nohighlight">\(\mathcal{N}(0,\sigma_x)\)</span>. Pour simplifier la
démonstration, on suppose que la couche suivante calcule un simple
potentiel post synaptique : pour un neurone de cette couche, ce
potentiel est <span class="math notranslate nohighlight">\(\mathbf{w}^\top\mathbf{x}\)</span>, avec
<span class="math notranslate nohighlight">\(\mathbf{w}\in\mathbb{R}^{n^{(1)}}\)</span> i.i.d <span class="math notranslate nohighlight">\(\mathcal{N}(0,\sigma_w)\)</span>. La
variance de ce potentiel est alors
$<span class="math notranslate nohighlight">\(Var(\mathbf{w}^\top\mathbf{x}) = \displaystyle\sum_{i=1}^{n^{(1)}} Var (w_ix_i) =  \displaystyle\sum_{i=1}^{n^{(1)}}  \left (\mathbb{E}(w_i)^2Var(x_i) + \mathbb{E}(x_i)^2Var (w_i) + Var (w_i) Var(x_i) \right )\)</span><span class="math notranslate nohighlight">\(
Les entrées et les poids sont de moyenne nulle, donc
\)</span><span class="math notranslate nohighlight">\(Var(\mathbf{w}^\top\mathbf{x}) =  \displaystyle\sum_{i=1}^{n^{(1)}}\sigma_x\sigma_w\)</span><span class="math notranslate nohighlight">\(
et puisque les \)</span>w_ix_i<span class="math notranslate nohighlight">\( sont i.i.d.
\)</span><span class="math notranslate nohighlight">\(Var(\mathbf{w}^\top\mathbf{x}) =  {n^{(1)}}\sigma_x\sigma_w\)</span><span class="math notranslate nohighlight">\( On
montre plus généralement que sur la \)</span>l^e<span class="math notranslate nohighlight">\( couche cachée, la variance de
\)</span>\mathbf{Y^{(l)}}<span class="math notranslate nohighlight">\( est
\)</span><span class="math notranslate nohighlight">\(Var(\mathbf{Y^{(l)}}) = \left ( n^{(l)} Var(w_i)\right )^l Var (x_i)\)</span><span class="math notranslate nohighlight">\(
Chaque neurone peut donc varier dans un rapport de \)</span>n^{(l)}<span class="math notranslate nohighlight">\( fois la
variation de son entrée (qui est elle même \)</span>n^{(l-1)}$ fois la variance
de son entrée…)</p>
<p>On a alors les cas de figure suivants :</p>
<ul class="simple">
<li><p>si <span class="math notranslate nohighlight">\(n^{(l)} Var(w_i)&gt;1\)</span> le gradient va tendre vers de grandes
valeurs à mesure que l’on s’enfonce dans le réseau (que <span class="math notranslate nohighlight">\(l\)</span> croît)</p></li>
<li><p>si <span class="math notranslate nohighlight">\(n^{(l)} Var(w_i)&lt;1\)</span> le gradient disparaît à mesure que l’on
s’enfonce dans le réseau</p></li>
</ul>
<p>Pour éviter ces deux problèmes, la solution est de forcer
<span class="math notranslate nohighlight">\(n^{(l)} Var(w_i)=1\)</span>, soit <span class="math notranslate nohighlight">\(Var(w_i)=1/n^{(l)}\)</span>. On initialise donc les
poids se la couche <span class="math notranslate nohighlight">\(l\)</span> selon une loi
$<span class="math notranslate nohighlight">\(\frac{1}{\sqrt{n^{(l-1)}}}\mathcal{N}(0,1)\)</span>$ Cette procédure est la
méthode d’initialisation de Xavier, ou de Glorot [&#64;Glorot10].</p>
</div>
<div class="section" id="initialisation-de-he">
<h3>Initialisation de He<a class="headerlink" href="#initialisation-de-he" title="Lien permanent vers ce titre">#</a></h3>
<p>He a montré dans [&#64;He15] que la méthode de Xavier pouvait ne pas
fonctionner correctement lorsque les traitements non linéaires étaient
effectués par la fonction ReLU. Les auteurs proposent alors de plutôt
multiplier par <span class="math notranslate nohighlight">\(\frac{\sqrt{2}}{\sqrt{n^{(l-1)}}}\)</span> pour prendre en
compte la partie négative qui ne participe pas au calcul de la variance.</p>
</div>
</div>
<div class="section" id="apprentissage">
<h2>Apprentissage<a class="headerlink" href="#apprentissage" title="Lien permanent vers ce titre">#</a></h2>
<p>La présence de nombreuses couches cachées va permettre de calculer des
caractéristiques beaucoup plus complexes et informatives des entrées.
Chaque couche calculant une transformation non linéaire de la couche
précédente, le pouvoir de représentation de ces réseaux s’en trouve
amélioré. On peut par exemple montrer qu’il existe des fonctions qu’un
réseau à <span class="math notranslate nohighlight">\(k\)</span> couches peut représenter de manière compacte (avec un
nombre de neurones cachés qui est polynomial en le nombre des entrées),
alors qu’un réseau à <span class="math notranslate nohighlight">\(k-1\)</span> couches ne peut pas le faire, à moins d’avoir
une combinatoire exponentielle sur le nombre de neurones cachés.\</p>
<div class="section" id="le-probleme-de-l-entrainement">
<h3>Le problème de l’entraînement<a class="headerlink" href="#le-probleme-de-l-entrainement" title="Lien permanent vers ce titre">#</a></h3>
<p>Si l’intérêt de ces réseaux est manifeste, la complexité de leur
utilisation vient de l’étape d’apprentissage. Jusqu’à récemment,
l’algorithme utilisé était classique et consistait en une initialisation
aléatoire des poids du réseau, suivie d’un entraînement sur un ensemble
d’apprentissage, en minimisant une fonction objectif. Cependant, dans le
cas des réseaux profonds, cette approche peut ne pas être adaptée :</p>
<ul class="simple">
<li><p>Les données étiquetées doivent être en nombre suffisant pour
permettre un entraînement efficace, d’autant plus que le réseau est
complexe. Dans le cas contraire, un surapprentissage peut notamment
être induit.</p></li>
<li><p>Sur un tel réseau, l’apprentissage se résume à l’optimisation d’une
fonction fortement non convexe, qui amène presque sûrement à des
minima locaux lorsque des algorithmes classiques sont utilisés.</p></li>
<li><p>Dans l’étape de rétropropagation, les gradients diminuent rapidement
à mesure que le nombre de couches cachées augmente. La dérivée de la
fonction objectif par rapport à <span class="math notranslate nohighlight">\(\mathbf{W}\)</span> devient alors très
faible à mesure que le calcul se rétropropage vers la couche
d’entrée. Les poids des premières couches changent donc très
lentement et le réseau n’est plus en capacité d’apprendre. Ce
problème est connu sous le nom de problème de gradient évanescent
(vanishing gradient).</p></li>
</ul>
<p>L’algorithme principalement utilisé pour l’apprentissage des réseaux
convolutifs reste la rétropropagation du gradient. Le choix de la
fonction objectif, de sa régularisation, de la méthode d’optimisation
(descente de gradient, méthodes à taux d’apprentissage adaptatifs telles
qu’AdaGrad, RMSProp ou Adam) et des paramètres associés, ou des
techniques de présentation des exemples (batchs, minibatchs) sont autant
de facteurs importants permettant aux modèles non seulement de converger
vers un optimum local satisfaisant, mais également de proposer un modèle
final ayant une bonne capacité de généralisation.</p>
<p>Aujourd’hui, de nombreux réseaux, déjà entraînés, sont mis à
disposition. En effet, ces entraînements nécessitent de grandes bases
d’apprentissage (type ImageNet) et une puissance de calcul assez élevée
(GPU obligatoire(s)). Pour le traitement de problèmes précis, des
méthodes existent, qui partent de ces réseaux préentraînés et les
modifient localement pour, par exemple, apprendre de nouvelles classes
d’images non encore vues par le réseau. L’idée sous-jacente est que les
premières couches capturent des caractéristiques bas niveau et que la
sémantique vient avec les couches profondes. Ainsi, dans un problème de
classification, où les classes n’ont pas été apprises, on peut supposer
qu’en conservant les premières couches on extraira des caractéristiques
communes des images (bords, colorimétrie,…) et qu’en changeant les
dernières couches (information sémantique et haut niveau et étage de
classification), c’est-à-dire en réapprenant les connexions, on
spécifiera le nouveau réseau pour la nouvelle tâche de classification.
Cette approche rentre dans le cadre des méthodes de <em>transfer learning</em>
[&#64;Pan10] et de <em>fine tuning</em>, cas particulier d’adaptation de domaine :</p>
<ul class="simple">
<li><p>Les méthodes de transfert prennent un réseau déjà entraîné, enlèvent
la dernière couche complètement connectée et traitent le réseau
restant comme un extracteur de caractéristiques. Un nouveau
classifieur, la dernière couche, est alors entraîné sur le nouveau
problème.</p></li>
<li><p>Les méthodes de fine tuning ré-entraînent le classifieur du réseau
et remettent à jour les poids du réseau pré-entraîné par
rétropropagation.</p></li>
</ul>
<p>Plusieurs facteurs influent sur le choix de la méthode à utiliser : la
taille des données d’apprentissage du nouveau problème et la
ressemblance du nouveau jeu de données avec celui qui a servi à
entraîner le réseau initial :</p>
<ul class="simple">
<li><p>Pour un jeu de données similaire de petite taille, on utilise du
transfer learning, avec un classifieur utilisé sur les
caractéristiques calculées sur les dernières couches du réseau
initial.</p></li>
<li><p>Pour un jeu de données de petite taille et un problème différent, on
utilise du transfer learning, avec un classifieur utilisé sur les
caractéristiques calculées sur les premières couches du réseau
initial.</p></li>
<li><p>Pour un jeu de données, similaire ou non, de grande taille, on
utilise le fine tuning.</p></li>
</ul>
<p>Notons qu’il est toujours possible d’augmenter la taille du jeu de
données par des technique d’augmentation de données.</p>
<p>Dans le cas où un réseau ad hoc doit être construit et où une base
d’apprentissage suffisante est disponible, l’entraînement par
optimisation reste possible. Il existe en particulier des techniques
d’apprentissage couche à couche, lorsque les couches successives
calculent des fonctions d’activation des couches précédentes (empilement
d’autoencodeurs par exemple).</p>
</div>
<div class="section" id="apprentissage-glouton-par-couche">
<h3>Apprentissage glouton par couche<a class="headerlink" href="#apprentissage-glouton-par-couche" title="Lien permanent vers ce titre">#</a></h3>
<p>L’idée est d’entraîner les couches une à une, d’abord dans un réseau à
une couche cachée, puis à deux couches cachées… À chaque étape <span class="math notranslate nohighlight">\(k\)</span>,
la couche <span class="math notranslate nohighlight">\(k\)</span> est ajoutée et a pour entrée la couche <span class="math notranslate nohighlight">\(k-1\)</span> précédemment
entraînée. L’entraînement peut être supervisé, mais le plus souvent il
est non supervisé. Les poids issus de cet entraînement servent
d’initialisation pour le réseau final.<br />
En comparaison des points précédents, cette approche est bien plus
pertinente :</p>
<ul class="simple">
<li><p>Les données non étiquetées sont très faciles à obtenir.</p></li>
<li><p>L’initialisation des poids sur des données non étiquetées est plus
performante qu’une initialisation aléatoire. Empiriquement, une
méthode type descente de gradient permet d’aboutir à un meilleur
minimum local (les données non étiquetées fournissent en effet des
informations a priori déjà importantes sur les données).</p></li>
</ul>
</div>
<div class="section" id="visualisation-du-mecanisme-des-reseaux-convolutifs">
<h3>Visualisation du mécanisme des réseaux convolutifs<a class="headerlink" href="#visualisation-du-mecanisme-des-reseaux-convolutifs" title="Lien permanent vers ce titre">#</a></h3>
<p>Le mécanisme interne des réseaux convolutifs est mal compris et
l’analyse des raisons qui font que leur puissance de prédiction est
importante n’est pas aisée. S’il est toujours possible de rétroprojeter
les activations depuis la première couche de convolution, les couches
d’agrégation et de rectification empêchent de comprendre le
fonctionnement des couches suivantes, ce qui peut être gênant dans la
construction et l’amélioration de ces réseaux.</p>
<p>Les méthodes de visualisation du fonctionnement des réseaux convolutifs
peuvent être rangées en trois catégories, décrites dans les paragraphes
suivants.</p>
</div>
<div class="section" id="methodes-de-visualisation-de-base">
<h3>Méthodes de visualisation de base<a class="headerlink" href="#methodes-de-visualisation-de-base" title="Lien permanent vers ce titre">#</a></h3>
<p>Les méthodes les plus simples consistent à visualiser les activations
lors du passage d’une image dans le réseau. Pour des activations type
ReLU, ces activations sont ininterprétables au début de l’entraînement,
mais à mesure que ce dernier progresse, les cartes d’activation
<span class="math notranslate nohighlight">\(\mathbf{Y_i^{(l)}}\)</span> deviennent localisées et éparses.</p>
<p>Il est également possible de visualiser les filtres des différentes
couches de convolution (figure
<a class="reference external" href="#F:viewfilters">1.3</a>{reference-type= »ref » reference= »F:viewfilters »}).
Les filtres des premières couches agissent comme des détecteurs de bords
et coins et, à mesure que l’on s’enfonce dans le réseau, les filtres
capturent des concepts haut niveau comme des objets ou encore des
visages.</p>
<p>Citons encore d’autres méthodes qui proposent de visualiser les
dernières couches (les couches complètement connectées) de grande
dimension (par exemple 4096 pour AlexNet) via une méthode de réduction
de dimension.</p>
<figure id="F:viewfilters">
<img src="images/viewfilters" />
<figcaption>Visualisation des filtres de la première couche d’AlexNet (à
gauche, 64 filtres 11<span class="math inline">×</span>11) et de
ResNet-18 (à droite, 64 filtres 7<span
class="math inline">×</span>7).</figcaption>
</figure>
</div>
<div class="section" id="methodes-fondees-sur-les-activations">
<h3>Méthodes fondées sur les activations<a class="headerlink" href="#methodes-fondees-sur-les-activations" title="Lien permanent vers ce titre">#</a></h3>
<p>Plusieurs stratégies peuvent être adoptées pour sonder le fonctionnement
d’un réseau convolutif, en utilisant les informations portées par les
cartes <span class="math notranslate nohighlight">\(\mathbf{Y_i^{(l)}}\)</span>, parmi lesquelles :</p>
<ul>
<li><p>Utiliser des couches de convolution transposée (improprement
appelées parfois couches de déconvolution), ajoutées à chaque couche
de convolution du réseau. Étant données les cartes d’entrée de la
couche <span class="math notranslate nohighlight">\(l\)</span>, les cartes de sortie <span class="math notranslate nohighlight">\(\mathbf{Y_i^{(l)}}\)</span> sont envoyées
dans la couche de convolution transposée correspondante au niveau
<span class="math notranslate nohighlight">\(l\)</span>. Cette dernière reconstruit les <span class="math notranslate nohighlight">\(\mathbf{Y_i^{(l-1)}}\)</span> qui ont
permis le calcul des activations de la couche <span class="math notranslate nohighlight">\(l\)</span>. Le processus est
alors itéré jusqu’à atteindre la couche d’entrée <span class="math notranslate nohighlight">\(l = 1\)</span>, les
activations de la couche <span class="math notranslate nohighlight">\(l\)</span> étant alors rétroprojetées dans le plan
image [&#64;ZE13]. La présence de couches d’agrégation et de
rectification rend ce processus non inversible (par exemple, une
couche d’agrégation maximum nécessite de connaître à quelles
positions de l’image <span class="math notranslate nohighlight">\(\mathbf{Y_i^{(l)}}\)</span> sont situés les maxima
retenus).</p></li>
<li><p>Faire passer un grand nombre d’images dans le réseau et, pour un
neurone particulier, conserver celle qui a le plus activé ce
neurone. Il est alors possible de visualiser les images pour
comprendre ce à quoi le neurone s’intéresse dans son champ réceptif
(figure <a class="reference external" href="#F:champreceptif">1.4</a>{reference-type= »ref »
reference= »F:champreceptif »}).</p>
<figure id="F:champreceptif">
<img src="images/champreceptif" />
<figcaption>Champ récéptif de quelques neurones de la dernière couche
d’agrégation du réseau AlexNet, superposées aux images ayant le plus
fortement activé ces neurones. Le champ est encadré en blanc, et la
valeur d’activation correspondante est reportée en haut. On voit par
exemple que certains neurones sont très sensibles aux textes, d’autres
aux réflexions spéculaires, ou encore aux hauts du corps (source : <span
class="citation" data-cites="Girshick14"></span>).</figcaption>
</figure>
</li>
<li><p>Cacher (par un rectangle noir par exemple) différentes parties de
l’image d’entrée qui est d’une certaine classe (disons un chien) et
observer la sortie du réseau (la probabilité de la classe de l’image
d’entrée). En représentant les valeurs de probabilité de la classe
d’intérêt comme une fonction de la position du rectangle occultant,
il est possible de voir si le réseau s’intéresse effectivement aux
parties de l’image spécifiques de la classe, ou à des autres zones
(le fond par exemple) (figure
<a class="reference external" href="#F:occlusion">1.5</a>{reference-type= »ref » reference= »F:occlusion »}).</p>
<figure id="F:occlusion">
<img src="images/occlusion" />
<figcaption>Occlusion d’une image (à gauche). Le rectangle noir est
déplacé dans l’image et pour chaque position la probabilité de la classe
de l’image (ici un loulou de Poméranie) est enregistrée. Ces
probabilités sont ensuite représentées sous forme d’une carte 2D (à
droite). La probabilité de la classe s’effondre lorsque le rectangle
couvre une partie de la face du chien. Cela suggère que cette face est
grandement responsable de la forte probabilité de classement de l’image
comme un loulou. A l’inverse, l’occlusion du fond n’altère pas la forte
valeur de probabilité de la classe (source : <span class="citation"
data-cites="ZE13"></span>).</figcaption>
</figure>
</li>
</ul>
</div>
<div class="section" id="methodes-fondees-sur-le-gradient">
<h3>Méthodes fondées sur le gradient<a class="headerlink" href="#methodes-fondees-sur-le-gradient" title="Lien permanent vers ce titre">#</a></h3>
<p>Pour comprendre quelle(s) partie(s) de l’image est (sont) utilisée(s)
par le réseau pour effectuer une prédiction, il est possible de calculer
des cartes de saillance (saliency maps). L’idée est relativement simple
: calculer le gradient de la classe de sortie par rapport à l’image
d’entrée. Cela indique à quel point une petite variation dans l’image
induit un changement de prédiction. En visualisant les gradients, on
observe alors par exemple leurs fortes valeurs, indiquant qu’une petite
variation du pixel correspondant augmente la valeur de sortie.<br />
Il est également possible d’utiliser le gradient par rapport à la
dernière couche de convolution (approche Grad-CAM), ce qui permet de
récupérer des informations de localisation spatiale des régions
importantes pour la prédiction (figure
<a class="reference external" href="#F:grad">1.6</a>{reference-type= »ref » reference= »F:grad »}, droite).<br />
Plus généralement, en choisissant un neurone intermédiaire du réseau
(d’une couche de convolution), la méthode de rétropropagation guidée
calcule le gradient de sa valeur par rapport aux pixels de l’image
d’entrée, ce qui permet de souligner les parties de l’image auxquelles
ce neurone répond (figure <a class="reference external" href="#F:grad">1.6</a>{reference-type= »ref »
reference= »F:grad »}, milieu).</p>
<figure id="F:grad">
<img src="images/gradCam" />
<figcaption>Approches par gradient de visualisation du fonctionnement
d’un réseau convolutif. Comparaison de la méthode de rétropropagation
guidée et de Grad-CAM.(source : <span class="citation"
data-cites="Selvaraju17"></span>).</figcaption>
</figure>
<p>Ces gradients peuvent également être utilisés dans la méthode de montée
de gradient (gradient Ascent), dont l’objectif est de générer une image
qui active de manière maximale un neurone donné du réseau. Le principe
est d’itérativement passer l’image d’entrée <span class="math notranslate nohighlight">\(\mathbf{I}\)</span> dans le réseau
pour obtenir les valeurs des cartes <span class="math notranslate nohighlight">\(\mathbf{Y_i^{(l)}}\)</span>, de
rétropropager pour obtenir le gradient d’un neurone par rapport aux
pixels de <span class="math notranslate nohighlight">\(\mathbf{I}\)</span> et d’opérer une petite modification de ces
pixels. Outre son aspect informatif sur la structure interne du réseau
étudié (visualisation des cartes <span class="math notranslate nohighlight">\(\mathbf{Y_i^{(l)}}\)</span> intermédiaires),
cette méthode produit des images parfois très artistiques (figure
<a class="reference external" href="#F:gradAscent">1.7</a>{reference-type= »ref » reference= »F:gradAscent »}).</p>
<figure id="F:gradAscent">
<img src="images/gradAscent" />
<figcaption>Visualisation des 4 premières couches de convolution d’un
réseau convolutif par montée de gradient (source : <span
class="citation" data-cites="Yosinski15"></span>).</figcaption>
</figure>
</div>
</div>
<div class="section" id="quelques-applications-subsubsec-applis">
<h2>Quelques applications {#subsubsec:applis}<a class="headerlink" href="#quelques-applications-subsubsec-applis" title="Lien permanent vers ce titre">#</a></h2>
<p>Depuis leur introduction en reconnaissance de caractères manuscrits
[&#64;LeCun90], les réseaux convolutifs n’ont cessé de trouver des champs
applicatifs, commerciaux et parfois ludiques. Parmi ces applications,
nous en citons ici quelques-unes.</p>
<div class="section" id="la-classification-d-images">
<h3>La classification d’images<a class="headerlink" href="#la-classification-d-images" title="Lien permanent vers ce titre">#</a></h3>
<p><br />
Premier domaine d’application des réseaux convolutifs, la classification
d’images consiste à affecter une image à une classe, apprise par le
réseau sur un grand nombre d’exemples. Depuis l’avènement d’ImageNet, et
la mise en place de la compétition ILSVRC, les résultats obtenus ne
cessent de s’améliorer et sont depuis quelques années la référence dans
ce domaine (dépassant même les performances humaines en 2015).</p>
</div>
<div class="section" id="l-annotation-de-scenes">
<h3>L’annotation de scènes<a class="headerlink" href="#l-annotation-de-scenes" title="Lien permanent vers ce titre">#</a></h3>
<p><br />
Des réseaux convolutifs ont été utilisés pour annoter des scènes 2D ou
2D+t, <em>i.e.</em> assigner à chaque pixel un label identifiant l’objet auquel
il appartient. De nombreux réseaux ont été développés à cet effet
(R-CNN, Fast R-CNN, Mast R-CNN par exemple)
(figure <a class="reference external" href="#F:annotation">[F:annotation]</a>{reference-type= »ref »
reference= »F:annotation »}).</p>
</div>
<div class="section" id="la-reconnaissance-d-actions">
<h3>La reconnaissance d’actions<a class="headerlink" href="#la-reconnaissance-d-actions" title="Lien permanent vers ce titre">#</a></h3>
<p><br />
Le développement de champs réceptifs 3D dans les réseaux convolutifs a
permis d’extraire dans ces réseaux des caractéristiques invariantes à la
translation. L’intégration de techniques de régularisation adaptée (type
partage de paramètres) a rendu possible l’optimisation de ces réseaux
pour la reconnaissance d’actions dans des scènes dynamiques.</p>
</div>
<div class="section" id="l-analyse-de-documents">
<h3>L’analyse de documents<a class="headerlink" href="#l-analyse-de-documents" title="Lien permanent vers ce titre">#</a></h3>
<p><br />
L’analyse de documents à des fins de reconnaissance de caractères, de
classification de documents ou encore d’annotation sémantique a
largement bénéficié de l’apport des réseaux convolutifs.</p>
</div>
<div class="section" id="l-augmentation-de-donnees">
<h3>L’augmentation de données<a class="headerlink" href="#l-augmentation-de-donnees" title="Lien permanent vers ce titre">#</a></h3>
<p><br />
De nombreux réseaux ont été proposés pour ajouter à des données nD des
informations manquantes (colorisation d’images,
figure <a class="reference external" href="#F:color">[F:color]</a>{reference-type= »ref »
reference= »F:color »}, inpainting, restauration d’images,
superrésolution), ou pour proposer des versions différentes des données
initiales en fonction d’une contrainte de style extérieure (transfert de
style, figure <a class="reference external" href="#F:transfer">[F:transfer]</a>{reference-type= »ref »
reference= »F:transfer »}).</p>
<figure>
<figcaption>Quelques applications des réseaux convolutifs.</figcaption>
</figure>
<p>Les réseaux convolutifs sont des réseaux spécialisés pour traiter des
données dont la topologie se conforme à une structure de grille
n-dimensionnelle. Dans le cas de données 1D séquentielles, d’autres
réseaux performants ont été développés : les réseaux récurrents.</p>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="PMC.html" title="précédent page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">précédent</p>
            <p class="prev-next-title">Perceptrons multicouches</p>
        </div>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By Vincent BARRA<br/>
  
      &copy; Copyright 2022.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>