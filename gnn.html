
<!DOCTYPE html>


<html lang="fr" data-content_root="./" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Graph Neural Networks &#8212; Apprentissage profond</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.5.1/css/all.min.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.1/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.1/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.1/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="_static/styles/sphinx-book-theme.css?v=384b581d" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="_static/proof.css?v=ca93fcec" />
    <link rel="stylesheet" type="text/css" href="_static/design-style.1e8bd061cd6da7fc9cf755528e8ffc24.min.css?v=0a3b3ea7" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=8d27b9dea8ad943066ae" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=8d27b9dea8ad943066ae" />
  <script src="_static/vendor/fontawesome/6.5.1/js/all.min.js?digest=8d27b9dea8ad943066ae"></script>

    <script src="_static/documentation_options.js?v=72dce1d2"></script>
    <script src="_static/doctools.js?v=9a2dae69"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="_static/copybutton.js?v=f281be69"></script>
    <script src="_static/scripts/sphinx-book-theme.js?v=efea14e4"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js?v=4a39c7ea"></script>
    <script src="_static/translations.js?v=bf059b8c"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js?v=36754332"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'gnn';</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Recherche" href="search.html" />
    <link rel="prev" title="Transformers" href="transformers.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="fr"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a id="pst-skip-link" class="skip-link" href="#main-content">Passer au contenu principal</a>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Haut de page
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <header class="bd-header navbar navbar-expand-lg bd-navbar">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  

<a class="navbar-brand logo" href="intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="_static/logo.png" class="logo__image only-light" alt="Apprentissage profond - Home"/>
    <script>document.write(`<img src="_static/logo.png" class="logo__image only-dark" alt="Apprentissage profond - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn navbar-btn search-button-field search-button__button" title="Recherche" aria-label="Recherche" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Recherche</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Introduction</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="NN.html">Introduction aux réseaux de neurones</a></li>
<li class="toctree-l1"><a class="reference internal" href="PMC.html">Perceptrons multicouches</a></li>
<li class="toctree-l1"><a class="reference internal" href="cnn.html">Réseaux convolutifs</a></li>
<li class="toctree-l1"><a class="reference internal" href="ae.html">Auto-encodeurs</a></li>
<li class="toctree-l1"><a class="reference internal" href="rnn.html">Réseaux récurrents</a></li>
<li class="toctree-l1"><a class="reference internal" href="transferLearning.html">Utilisation de réseaux existants</a></li>
<li class="toctree-l1"><a class="reference internal" href="transformers.html">Transformers</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Graph Neural Networks</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">



<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Mode plein écran"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="clair/sombre" aria-label="clair/sombre" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Recherche" aria-label="Recherche" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Graph Neural Networks</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contenu </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#representation-d-un-graphe">Représentation d’un graphe</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#definition-matricielle-d-un-graphe">Définition matricielle d’un graphe</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#proprietes">Propriétés</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction-aux-gnn">Introduction aux GNN</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#utilisations">Utilisations</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#en-relation-avec-la-structure-de-graphe">En relation avec la structure de graphe</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#en-relation-avec-les-sommets">En relation avec les sommets</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#en-relation-avec-les-arcs-ou-aretes">En relation avec les arcs ou arêtes</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#gcn">GCN</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#equivariance-et-invariance">Equivariance et invariance</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#partage-des-parametres">Partage des paramètres</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#exemple">Exemple</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#application-en-classification">Application en classification</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#modeles-inductifs-et-transductifs">Modèles inductifs et transductifs</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#exemple-en-classification-de-sommets">Exemple en classification de sommets</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#echantillonnage-du-voisinage">Échantillonnage du voisinage</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#partitionnement-du-graphe">Partitionnement du graphe</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#couches-d-un-gnn">Couches d’un GNN</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#amelioration-de-la-diagonale">Amélioration de la diagonale</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#connexions-residuelles">Connexions résiduelles</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#agregation-moyenne">Agrégation moyenne</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#normalisation-de-kipf">Normalisation de Kipf</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#agregation-par-max-pooling">Agrégation par max pooling</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#agregation-par-attention">Agrégation par attention</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#prise-en-compte-de-l-information-des-arcs-ou-aretes-s-e">Prise en compte de l’information des arcs ou arêtes {#S:E}</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#partie-pratique">Partie pratique</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="graph-neural-networks">
<h1>Graph Neural Networks<a class="headerlink" href="#graph-neural-networks" title="Lien vers cette rubrique">#</a></h1>
<p>Comme leur nom l’indique, les Graph Neural Networks sont des réseaux de
neurones qui traitent les graphes. Ce traitement pose trois
problématiques :</p>
<ul class="simple">
<li><p>leur topologie est variable, et il est difficile de concevoir des
réseaux qui soient à la fois suffisamment expressifs et capables de
gérer cette variation.</p></li>
<li><p>ils peuvent être de taille conséquente : un graphe représentant les
connexions entre les utilisateurs d’un réseau social peut avoir
plusieurs millions de sommets.</p></li>
<li><p>il se peut qu’il n’y ait à disposition pour le problème à traiter
qu’un seul graphe, de sorte que le protocole habituel d’entraînement
avec de nombreux exemples de données et de test avec de nouvelles
données n’est pas toujours possible.</p></li>
</ul>
<section id="representation-d-un-graphe">
<h2>Représentation d’un graphe<a class="headerlink" href="#representation-d-un-graphe" title="Lien vers cette rubrique">#</a></h2>
<section id="definition-matricielle-d-un-graphe">
<h3>Définition matricielle d’un graphe<a class="headerlink" href="#definition-matricielle-d-un-graphe" title="Lien vers cette rubrique">#</a></h3>
<p>Soit <span class="math notranslate nohighlight">\(G=(V,E)\)</span> un graphe à <span class="math notranslate nohighlight">\(N=|V|\)</span> sommets et <span class="math notranslate nohighlight">\(M=|E|\)</span> arcs (ou arêtes).
Chaque sommet et chaque arc (ou arête) peut porter une information
vectorielle (graphe pondéré). On choisit alors de représenter <span class="math notranslate nohighlight">\(G\)</span> par
trois matrices <span class="math notranslate nohighlight">\(\mathbf A,\mathbf X\)</span> et <span class="math notranslate nohighlight">\(\mathbf E\)</span> représentant
respectivement la struture de <span class="math notranslate nohighlight">\(G\)</span>, les sommets et arcs (ou arêtes):</p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(\mathbf A\)</span> est la matrice d’adjacence (pour les arêtes) ou
d’incidence (pour les arcs).</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbf X\)</span> est une matrice de taille <span class="math notranslate nohighlight">\(d\times N\)</span>, la <span class="math notranslate nohighlight">\(i\)</span>-eme
colonne de <span class="math notranslate nohighlight">\(\mathbf X\)</span> donnant les <span class="math notranslate nohighlight">\(d\)</span> informations portées par le
sommet <span class="math notranslate nohighlight">\(i\in[\![1,N]\!]\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbf E\)</span> est une matrice de taille <span class="math notranslate nohighlight">\(d_e\times M\)</span>, la <span class="math notranslate nohighlight">\(i\)</span>-eme
colonne de <span class="math notranslate nohighlight">\(\mathbf E\)</span> donnant les <span class="math notranslate nohighlight">\(d_e\)</span> informations portées par
l’arc (ou l’arête) <span class="math notranslate nohighlight">\(i\in[\![1,N]\!]\)</span>.</p></li>
</ol>
<p>Dans un premier temps, on considérera uniquement le cas où <span class="math notranslate nohighlight">\(\mathbf X\)</span>
existe (seuls les sommets sont pondérés). On reviendra sur le cas de
<span class="math notranslate nohighlight">\(\mathbf E\)</span> plus tard.</p>
</section>
<section id="proprietes">
<h3>Propriétés<a class="headerlink" href="#proprietes" title="Lien vers cette rubrique">#</a></h3>
<p>La matrice d’adjacence peut être utilisée pour trouver les voisins d’un
sommet. Supposons que le <span class="math notranslate nohighlight">\(i\)</span>-eme sommet soit encodé sous la forme d’un
vecteur colonne <span class="math notranslate nohighlight">\(\mathbf X_{\bullet,i}\)</span> avec une seule entrée non nulle
à la position <span class="math notranslate nohighlight">\(i\)</span>, fixée à un. En prémultipliant <span class="math notranslate nohighlight">\(\mathbf X_{\bullet,i}\)</span>
par la matrice <span class="math notranslate nohighlight">\(\mathbf A\)</span>, on calcule un vecteur avec des uns aux
positions des voisins. En répétant cette procédure <span class="math notranslate nohighlight">\(n\)</span> fois, on accède
aux voisins du sommet <span class="math notranslate nohighlight">\(i\)</span> accessibles en <span class="math notranslate nohighlight">\(n\)</span> étapes. Ainsi, le
coefficient (l,c) de <span class="math notranslate nohighlight">\(\mathbf A^n\)</span> contient le nombre de chaînes uniques
de longueur <span class="math notranslate nohighlight">\(n\)</span> du sommet <span class="math notranslate nohighlight">\(l\)</span> au sommet <span class="math notranslate nohighlight">\(c\)</span>. A noter que ce n’est pas le
nombre de chemins uniques puisqu’il inclut les chemins qui visitent le
même sommet plus d’une fois. Les sommets étant numérotés arbitrairement
dans <span class="math notranslate nohighlight">\(G\)</span>, il est de plus possible de changer l’indexation de ces
sommets, sans changer la struture de <span class="math notranslate nohighlight">\(G\)</span>, à l’aide d’une matrice de
permutation <span class="math notranslate nohighlight">\(\mathbf P\)</span>. Les informations portées par les sommets sont
alors résumées dans la matrice <span class="math notranslate nohighlight">\(\mathbf X\mathbf P\)</span>, et la nouvelle
matrice d’adjacence est donnée par <span class="math notranslate nohighlight">\(\mathbf P^T\mathbf A \mathbf P\)</span>.</p>
</section>
</section>
<section id="introduction-aux-gnn">
<h2>Introduction aux GNN<a class="headerlink" href="#introduction-aux-gnn" title="Lien vers cette rubrique">#</a></h2>
<p>Un GNN est un modèle qui prend les représentations des sommets
<span class="math notranslate nohighlight">\(\mathbf X\)</span> et la matrice d’adjacence <span class="math notranslate nohighlight">\(\mathbf A\)</span> comme entrées et les
fait passer par une série de <span class="math notranslate nohighlight">\(K\)</span> couches. Les représentations des
sommets sont mises à jour à chaque couche pour créer des représentations
intermédiaires cachées <span class="math notranslate nohighlight">\(\mathbf H_k\)</span> avant de calculer les
représentations de sortie <span class="math notranslate nohighlight">\(\mathbf H_K\)</span>, dont les colonnes comprennent
des informations sur les sommets correspondants et leur contexte dans le
<span class="math notranslate nohighlight">\(G\)</span>.</p>
<p>Les GNN peuvent être utilisés en exploitant la structure de <span class="math notranslate nohighlight">\(G\)</span>, les
sommets et/ou les arcs ou arêtes.</p>
<section id="utilisations">
<h3>Utilisations<a class="headerlink" href="#utilisations" title="Lien vers cette rubrique">#</a></h3>
<section id="en-relation-avec-la-structure-de-graphe">
<h4>En relation avec la structure de graphe<a class="headerlink" href="#en-relation-avec-la-structure-de-graphe" title="Lien vers cette rubrique">#</a></h4>
<p>Le réseau attribue une étiquette ou estime une ou plusieurs valeurs à
partir de l’ensemble du graphe, en exploitant à la fois la structure et
la représentation des sommets. Par exemple, on peut vouloir prédire la
température à laquelle un gaz (molécules représentées sous la forme d’un
graphe) devient liquide (régression) ou si une molécule est toxique ou
non (classification). Les représentations des sommets de sortie sont
combinées (par exemple, en calculant la moyenne) et le vecteur résultant
est mis en correspondance avec un vecteur de taille fixe par le biais
d’une transformation linéaire ou d’un réseau de neurones. Pour la
régression, le décalage entre le résultat et les valeurs de vérité
terrain est calculé à l’aide de la fonction de perte moindres carrés.
Pour la classification binaire, la sortie passe par une fonction
sigmoïde et la perte est calculée à l’aide de l’entropie croisée binaire</p>
<div class="math notranslate nohighlight">
\[P(y=1\mid \mathbf X,\mathbf A) = \sigma(\beta_K+\mathbf w_K\mathbf H_K\mathbf 1/N)\]</div>
<p>où <span class="math notranslate nohighlight">\(\beta_K\)</span> et <span class="math notranslate nohighlight">\(\mathbf w_K^T\in\mathbb{R}^D\)</span> sont des paramètres à
apprendre. Multiplier à droite par le vecteur colonne <span class="math notranslate nohighlight">\(\mathbf 1\)</span> somme
toutes les représentations, et diviser par <span class="math notranslate nohighlight">\(N\)</span> calcule la moyenne. La
technique résultante est dite <em>mean pooling</em>.</p>
</section>
<section id="en-relation-avec-les-sommets">
<h4>En relation avec les sommets<a class="headerlink" href="#en-relation-avec-les-sommets" title="Lien vers cette rubrique">#</a></h4>
<p>Le réseau attribue une étiquette (classification) ou une ou plusieurs
valeurs (régression) à chaque sommet du graphe, en utilisant à la fois
la structure du graphe et les représentations des sommets. Par exemple,
dans un graphe construit à partir d’un nuage de points 3D d’un avion,
l’objectif peut être de classer les sommets selon qu’ils appartiennent
aux ailes ou au fuselage (classification). En régression, on peut par
exemple vouloir prédire le nombre de messages qu’un abonné d’un réseau
social recevra. Les fonctions de perte sont définies de la même manière
que pour les tâches au niveau du graphe, sauf que cette opération est
effectuée indépendamment pour chaque sommet i :</p>
<div class="math notranslate nohighlight">
\[P(y^{(i)}=1\mid \mathbf X,\mathbf A) = \sigma(\beta_K+\mathbf w_K\mathbf h^{(i)}_K)\]</div>
</section>
<section id="en-relation-avec-les-arcs-ou-aretes">
<h4>En relation avec les arcs ou arêtes<a class="headerlink" href="#en-relation-avec-les-arcs-ou-aretes" title="Lien vers cette rubrique">#</a></h4>
<p>Le réseau prédit s’il doit y avoir ou non un arc (ou arête) entre les
sommets <span class="math notranslate nohighlight">\(i\)</span> et <span class="math notranslate nohighlight">\(j\)</span>. Par exemple, dans le cadre d’un réseau social, le
réseau peut prédire si deux personnes se connaissent et s’apprécient et
suggérer qu’elles se connectent si c’est le cas. C’est une tâche de
classification binaire pour laquelle la représentation des deux sommets
doit être convertie en un nombre unique représentant la probabilité de
présence de l’arc (ou arête). L’une des possibilités consiste à prendre
le produit scalaire des représentations des sommets et à faire passer le
résultat par une fonction sigmoïde pour calculer la probabilité.</p>
</section>
</section>
<section id="gcn">
<h3>GCN<a class="headerlink" href="#gcn" title="Lien vers cette rubrique">#</a></h3>
<p>Il existe de nombreux types de GNN, on se restreint ici aux réseaux
convolutifs de graphes, ou GCN. Ces modèles sont convolutifs en ce sens
qu’ils mettent à jour chaque sommet en agrégeant les informations
provenant des sommets voisins. En tant que tels, ils induisent un biais
inductif relationnel (une tendance à donner la priorité aux informations
provenant des voisins). On suppose de plus que les convolutions
s’opèrent dans le domaine spatial (utilisant la structure de <span class="math notranslate nohighlight">\(G\)</span>),
plutôt que dans l’espace de Fourier (méthodes basées spectre). Chaque
couche du GCN est une fonction <span class="math notranslate nohighlight">\(\mathbf F_{\boldsymbol \phi}\)</span>, de paramètres
<span class="math notranslate nohighlight">\({\boldsymbol \phi}\)</span>, qui prend en entrée les représentations des sommets
<span class="math notranslate nohighlight">\(\mathbf X\)</span> et la matrice d’adjacence <span class="math notranslate nohighlight">\(\mathbf A\)</span> de <span class="math notranslate nohighlight">\(G\)</span> et produit de
nouvelles représentations des sommets :</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\mathbf H_1 &amp;=&amp; \mathbf F_{\boldsymbol \phi_O}(\mathbf X,\mathbf A)\\ 
\mathbf H_2 &amp;=&amp; \mathbf F_{\boldsymbol \phi_1}(\mathbf H_1,\mathbf A)\\ 
\cdots &amp;&amp;\\ 
\mathbf H_K &amp;=&amp; \mathbf F_{\boldsymbol \phi_{K-1}}(\mathbf H_{K-1},\mathbf A)
\end{aligned}\end{split}\]</div>
<p>les <span class="math notranslate nohighlight">\(\boldsymbol \phi_{k}\)</span> étant les paramètres du réseau entre
la couche <span class="math notranslate nohighlight">\(k\)</span> et la couche <span class="math notranslate nohighlight">\(k+1\)</span></p>
<section id="equivariance-et-invariance">
<h4>Equivariance et invariance<a class="headerlink" href="#equivariance-et-invariance" title="Lien vers cette rubrique">#</a></h4>
<p>L’indexation des sommets dans le graphe étant arbitraire, il est
indispensable que tout modèle respecte cette propriété. Chaque couche
doit donc être équivariante<a class="footnote-reference brackets" href="#id3" id="id1" role="doc-noteref"><span class="fn-bracket">[</span>1<span class="fn-bracket">]</span></a> par rapport aux permutations des indices
des sommets, soit pour toute permutation <span class="math notranslate nohighlight">\(\mathbf P\)</span> et tout <span class="math notranslate nohighlight">\(k\)</span>:
$<span class="math notranslate nohighlight">\(\mathbf{H_{k+1}}\mathbf P = F_{\boldsymbol \phi_{k}}(\mathbf H_k\mathbf P, \mathbf P^T\mathbf A\mathbf P)\)</span>$</p>
<p>Pour les tâches de classification des sommets et de prédiction des arcs
ou arêtes, les résultats doivent également être équivariants pour les
permutations des indices des sommets. Toutefois, pour les tâches en
relation avec le graphe, la couche finale agrège les informations
provenant de l’ensemble du graphe, de sorte que le résultat est
invariant par rapport à l’ordre des sommets.</p>
</section>
<section id="partage-des-parametres">
<h4>Partage des paramètres<a class="headerlink" href="#partage-des-parametres" title="Lien vers cette rubrique">#</a></h4>
<p>Dans les réseaux convolutifs, des couches convolutives sont utilisées,
qui traitent chaque position de l’image de manière identique. Cela
permet de réduire le nombre de paramètres et d’introduire un biais
inductif qui force le modèle à traiter chaque partie de l’image de la
même manière. Le même argument peut être avancé pour les sommets d’un
graphe. On pourrait apprendre un modèle avec des paramètres distincts
associés à chaque sommet. Cependant, le réseau doit maintenant apprendre
indépendamment la signification des connexions dans le graphe à chaque
position, et l’apprentissage nécessiterait de nombreux graphes ayant la
même topologie. Il est plus judicieux de construire un modèle qui
utilise les mêmes paramètres à chaque sommet, réduisant ainsi le nombre
de paramètres et partageant ce que le réseau apprend à chaque sommet sur
l’ensemble du graphe.</p>
<p>On peut modéliser une convolution (qui met à jour une variable en
prenant une somme pondérée des informations provenant de ses voisins)
comme le fait que chaque voisin envoie un message à la variable
d’intérêt, qui agrège ces messages pour former la mise à jour. Dans le
cas des images, les voisins sont les pixels d’une région carrée de
taille fixe autour de la position actuelle, de sorte que les relations
spatiales à chaque position sont les mêmes. Dans un graphe, chaque
sommet peut avoir un nombre différent de voisins et il n’existe a priori
pas de relation privilégiée entre sommets : il n’y a aucune raison de
pondérer favorablement (ou pas) les informations provenant d’un sommet
particulier.</p>
</section>
<section id="exemple">
<h4>Exemple<a class="headerlink" href="#exemple" title="Lien vers cette rubrique">#</a></h4>
<p>La (<a class="reference internal" href="#id2"><span class="std std-numref">Fig. 58</span></a>) présente un exemple simple de GCN.</p>
<figure class="align-default" id="id2">
<img alt="_images/gcn.png" src="_images/gcn.png" />
<figcaption>
<p><span class="caption-number">Fig. 58 </span><span class="caption-text">Exemple de GCN</span><a class="headerlink" href="#id2" title="Lien vers cette image">#</a></p>
</figcaption>
</figure>
<ul>
<li><p>à gauche le graphe <span class="math notranslate nohighlight">\(G\)</span> initial, les colonnes de <span class="math notranslate nohighlight">\(\mathbf X\)</span> étant
reportées à côté des sommets correspondants.</p></li>
<li><p>au milieu, chaque sommet de la première couche cachée est mis à jour
en :</p>
<ol class="arabic simple">
<li><p>agrégeant les sommets voisins d’un sommet <span class="math notranslate nohighlight">\(i\)</span> en un unique
vecteur :
<span class="math notranslate nohighlight">\(\mathbf f_1(i) = \displaystyle\sum_{j\textrm{ voisin de }i} \mathbf h_1(j)\)</span></p></li>
<li><p>appliquant pour tout <span class="math notranslate nohighlight">\(i\)</span> une transformation linéaire <span class="math notranslate nohighlight">\(\mathbf L\)</span>
au sommet initial <span class="math notranslate nohighlight">\(\mathbf x^{i}\)</span> et aux sommets agrégés et en
ajoutant un biais <span class="math notranslate nohighlight">\(\boldsymbol\beta_0\)</span> :
<span class="math notranslate nohighlight">\(\boldsymbol \beta_0 + \mathbf L\mathbf x^{i} +  \mathbf L \mathbf f_1(i)\)</span></p></li>
<li><p>appliquant une fonction non linéaire <span class="math notranslate nohighlight">\(g\)</span> au résultat précédent :
<span class="math notranslate nohighlight">\(\mathbf h_1^{i} =g(\boldsymbol \beta_0 + \mathbf L \mathbf x^{i} + \mathbf L f_1(i))\)</span></p></li>
</ol>
</li>
<li><p>à droite, le processus répété pour toute couche <span class="math notranslate nohighlight">\(k\)</span> :</p>
<div class="math notranslate nohighlight">
\[(\forall i)\; \; \mathbf h_{k+1}^{i} = g \left (\boldsymbol \beta_k+\mathbf L_k \mathbf h_k^{i} + \mathbf L_k\left (\displaystyle\sum_{j\textrm{ voisin de }i} \mathbf h_k(j)\right ) \right )\]</div>
</li>
</ul>
<p>On peut écrire ce processus de manière matricielle : Si
<span class="math notranslate nohighlight">\(\mathbf H_k\in\mathcal{M}_{D,N}(\mathbb{R})\)</span> est la matrice dont les
colonnes sont les représentations des sommets, alors</p>
<div class="math notranslate nohighlight">
\[\mathbf H_{k+1} = g\left (\boldsymbol\beta_k \mathbf 1^T + \mathbf L_k\mathbf H_k+\mathbf L_k\mathbf H_k \mathbf A\right ) = g\left (\boldsymbol\beta_k \mathbf 1^T + \mathbf L_k\mathbf H_k(\mathbf A+\mathbf I)\right )\]</div>
<p>où <span class="math notranslate nohighlight">\(g\)</span> est appliquée point à point sur les éléments de la matrice
argument. On remarque que la couche <span class="math notranslate nohighlight">\(k+1\)</span> est bien équivariante à la
permutation de la numérotation des sommets, utilise la structure du
graphe (<span class="math notranslate nohighlight">\(\mathbf A\)</span>) pour produire un biais inductif et partage les
paramètres sur tout le graphe.</p>
</section>
</section>
<section id="application-en-classification">
<h3>Application en classification<a class="headerlink" href="#application-en-classification" title="Lien vers cette rubrique">#</a></h3>
<p>Pour l’exemple, on s’intéresse à un problème de classification binaire.
On modélise une molécule comme un graphe, sont les sommets sont les
atomes. La matrice <span class="math notranslate nohighlight">\(\mathbf A\)</span> donne les liaisons entre les atomes, et
la matrice <span class="math notranslate nohighlight">\(\mathbf X\)</span> donne le nom de l’atome : si la table périodique
des éléments comporte <span class="math notranslate nohighlight">\(D\)</span> atomes, le sommet (l’atome) <span class="math notranslate nohighlight">\(i\)</span> est un vecteur
de <span class="math notranslate nohighlight">\(\{0,1\}^D\)</span>, où la seule composante qui vaille 1 est celle qui
identifie le type de l’atome. On s’intéresse alors de savoir si une
molécule donnée est toxique (<span class="math notranslate nohighlight">\(y=1\)</span>) ou pas (<span class="math notranslate nohighlight">\(y=0\)</span>).</p>
<p>Les équations du réseau sont alors :</p>
<div class="math notranslate nohighlight">
\[\forall k\in[\![0,K-1]\!]\; \mathbf H_{k+1} = g\left (\boldsymbol\beta_k \mathbf 1^T + \mathbf L_k\mathbf H_k(\mathbf A+\mathbf I)\right )\]</div>
<p>et</p>
<div class="math notranslate nohighlight">
\[f(\mathbf X,\mathbf A,\boldsymbol\phi) = P(y=1\mid \mathbf X,\mathbf A) = \sigma(\boldsymbol \beta_K+\mathbf w_K\mathbf H_K\mathbf 1/N)\]</div>
<p>où <span class="math notranslate nohighlight">\(\boldsymbol \phi=(\boldsymbol \beta_k,\mathbf L_k)_{k\in[\![0,K]\!]}\)</span> sont les
paramètres du réseau à apprendre et <span class="math notranslate nohighlight">\(\sigma\)</span> la fonction sigmoïde.</p>
<p>Étant donnés <span class="math notranslate nohighlight">\(n\)</span> exemples d’entraînement
<span class="math notranslate nohighlight">\((\mathbf X_i,\mathbf A_i,y_i)_{i\in[\![1,n]\!]}\)</span>, <span class="math notranslate nohighlight">\(\boldsymbol \phi\)</span> peut être
classiquement appris par minimisation de l’entropie croisée binaire sur
des batchs d’exemples. Si dans les MLP et les CNN, les entrées sont de
taille identique (et donc les exemples sont concaténés en un tenseur de
dimension supérieure pour un entraînement efficace par GPU ou TPU), les
graphes de la base d’entraînement ont très probablement un nombre de
sommets <span class="math notranslate nohighlight">\(N\)</span> et une dimension de l’espace de représentation <span class="math notranslate nohighlight">\(D\)</span>
différents, ce qui rend cette concaténation impossible. Une astuce
simple permet cependant de traiter l’ensemble du batch en parallèle. Les
graphes du batch sont traités comme des composantes disjointes d’un seul
grand graphe. Le réseau peut alors être exécuté comme une instance
unique des équations de réseau. La mise en commun des moyennes est
effectuée uniquement sur les graphes individuels afin d’obtenir une
représentation unique par graphe qui peut être introduite dans la
fonction de perte.</p>
</section>
<section id="modeles-inductifs-et-transductifs">
<h3>Modèles inductifs et transductifs<a class="headerlink" href="#modeles-inductifs-et-transductifs" title="Lien vers cette rubrique">#</a></h3>
<p>Jusqu’à présent, tous les modèles présentés dans ce cours ont été
inductifs : on exploite un ensemble de données étiquetées pour apprendre
la relation entre les entrées et les sorties. On l’applique ensuite à de
nouvelles données de test. En d’autres termes, on apprend la règle qui
associe les entrées aux sorties, puis on l’applique ailleurs.</p>
<p>En revanche, un modèle transductif (ou apprentissage semi-supervisé)
prend en compte les données étiquetées et non étiquetées en même temps.
Il ne produit pas de règle, mais simplement une étiquette pour les
sorties inconnues. Il présente l’avantage de pouvoir utiliser des
modèles sur des données non étiquetées pour prendre ses décisions, mais
nécessite un nouvel entraînement du modèle lorsque des données non
étiquetées supplémentaires sont ajoutées.</p>
<p>Les deux types de problèmes sont couramment rencontrés pour les graphes.
Parfois, on dispose de nombreux graphes étiquetés et on apprend une
correspondance entre le graphe et les étiquettes. D’autres fois, il
arrive qu’il n’y ait à disposition qu’un seul graphe de très grande
dimension et dans ce cas, les données d’apprentissage et de test sont
nécessairement connectées.</p>
<p>Les utilisations des GNN en relation avec la structure des graphes ne se
produisent que dans le cadre inductif où il existe des graphes
d’apprentissage et de test. Toutefois, les utilisations en relation avec
les sommets et les tâches de prédiction des arcs ou arêtes peuvent se
produire dans l’un ou l’autre cadre. Dans le cas transductif, la
fonction de perte minimise le décalage entre la sortie du modèle et la
vérité lorsqu’elle est connue. Les nouvelles prédictions sont calculées
en exécutant la passe avant et en récupérant les résultats lorsque la
vérité est inconnue.</p>
</section>
<section id="exemple-en-classification-de-sommets">
<h3>Exemple en classification de sommets<a class="headerlink" href="#exemple-en-classification-de-sommets" title="Lien vers cette rubrique">#</a></h3>
<p>On s’intéresse ici à un problème de classification binaire des sommet
dans un cadre transductif. Le graphe considéré comporte des millions de
sommets, certains ayant des étiquettes binaires <span class="math notranslate nohighlight">\(y_i\)</span>. L’objectif est
alors d’étiqueter les sommets non étiquetés restants. Le réseau est le
même que dans l’exemple <a class="reference internal" href="#S:excl"><span class="xref myst">1.2.3</span></a>{reference-type= »ref »
reference= »S:excl »} avec une couche finale différente qui produit un
vecteur de sortie de taille <span class="math notranslate nohighlight">\(1\times N\)</span> :</p>
<div class="math notranslate nohighlight">
\[f(\mathbf X,\mathbf A,\boldsymbol\phi) =  \sigma(\boldsymbol \beta_K\mathbf 1^T+\mathbf w_K\mathbf H_K)\]</div>
<p>la fonction <span class="math notranslate nohighlight">\(\sigma\)</span> agissant point à point. On trouve <span class="math notranslate nohighlight">\(\boldsymbol \phi\)</span> par
minimisation de l’entropie croisée binaire, mais seulement à partir des
valeurs des sommets pour lesquels les étiquettes <span class="math notranslate nohighlight">\(y_i\)</span> sont connues.</p>
<p>L’entraînement de ce réseau pose deux problèmes. Tout d’abord, il est
difficile d’entraîner un réseau de cette taille, ne serait-ce que parce
qu’il faut stocker les représentations des sommets à chaque couche du
réseau dans la passe avant. Cela implique à la fois le stockage et le
traitement d’une structure plusieurs fois plus grande que le graphe
entier. De plus, n’ayant qu’un seul graphe à disposition, la descente de
gradient (ou tout autre algorithme d’optimisation) sur batch est
impossible, puisq’un seul objet constitue la base d’entraînement.</p>
<p>Pour répondre à ce second problème, on choisit un sous-ensemble
aléatoire de sommets étiquetés à chaque étape de l’entraînement. Chaque
sommet dépend de ses voisins dans la couche précédente. Ces derniers
dépendent à leur tour de leurs voisins de la couche précédente, de sorte
que chaque sommet possède l’équivalent d’un champ réceptif comme dans
les CNN. La taille du champ réceptif est appelée voisinage à <span class="math notranslate nohighlight">\(k\)</span> sauts.
On peut donc effectuer une étape de descente de gradient en utilisant le
graphe qui forme l’union des voisinages de <span class="math notranslate nohighlight">\(k\)</span>-sauts des sommets du
batch. Les entrées restantes ne contribuent pas. S’il y a de nombreuses
couches et que le graphe est fortement connecté, chaque sommet d’entrée
peut se trouver dans le champ réceptif de chaque sortie, ce qui ne
réduit pas du tout la taille du graphe : c’est le problème de
l’expansion du graphe. Deux approches s’attaquent à ce problème :
l’échantillonnage du voisinage et le partitionnement du graphe.</p>
<section id="echantillonnage-du-voisinage">
<h4>Échantillonnage du voisinage<a class="headerlink" href="#echantillonnage-du-voisinage" title="Lien vers cette rubrique">#</a></h4>
<p>Le graphe complet est échantillonné, ce qui réduit les connexions à
chaque couche du réseau. Par exemple, on peut commencer par les sommets
du batch et échantillonner aléatoirement un nombre fixe de leurs voisins
dans la couche précédente. Ensuite, on échantillonne au hasard un nombre
fixe de leurs voisins dans la couche précédente, et ainsi de suite. La
taille du graphe augmente toujours à chaque couche, mais de manière
beaucoup plus contrôlée. Cette opération est renouvelée pour chaque
batch, de sorte que les voisins contributeurs diffèrent même si le même
batch est tiré deux fois. Cette techique rappelle celle du dropout et
ajoute une certaine régularisation.</p>
</section>
<section id="partitionnement-du-graphe">
<h4>Partitionnement du graphe<a class="headerlink" href="#partitionnement-du-graphe" title="Lien vers cette rubrique">#</a></h4>
<p>On peut également partitionner le graphe original en sous-ensembles de
sommet disjoints, et construire des graphes plus petits qui ne sont pas
connectés les uns aux autres avant le traitement. Il existe des
algorithmes standards pour choisir ces sous-ensembles afin de maximiser
le nombre de liens internes. Ces petits graphes peuvent chacun être
traités comme des batchs, ou un sous-ensemble aléatoire d’entre eux peut
être combiné pour former un batch (en rétablissant toutes les arêtes
entre eux à partir du graphe d’origine). Utilisant l’une de ces deux
approches, il est alors possible d’entraîner les paramètres du réseau de
la même manière que pour le cadre inductif, en divisant les sommets
étiquetés en ensembles d’entrainement, de test et de validation comme
souhaité. Pour effectuer l’inférence, on calcule les prédictions pour
les sommets inconnus sur la base de leur voisinage de <span class="math notranslate nohighlight">\(k\)</span>-sauts.
Contrairement à l’entraînement, il n’est pas nécessaire de stocker les
représentations intermédiaires, ce qui rend l’utilisation de la mémoire
beaucoup plus efficiente.</p>
</section>
</section>
<section id="couches-d-un-gnn">
<h3>Couches d’un GNN<a class="headerlink" href="#couches-d-un-gnn" title="Lien vers cette rubrique">#</a></h3>
<p>les sections précédentes combinaient les sommets adjacents par
sommation, en multipliant <span class="math notranslate nohighlight">\(\mathbf H\)</span> par <span class="math notranslate nohighlight">\(\mathbf A+\mathbf I\)</span>. Dans la
suite de ce paragraphe, on présente des alternatives à cette approche.</p>
<section id="amelioration-de-la-diagonale">
<h4>Amélioration de la diagonale<a class="headerlink" href="#amelioration-de-la-diagonale" title="Lien vers cette rubrique">#</a></h4>
<p>La mise à jour proposée jusqu’à lors
$<span class="math notranslate nohighlight">\(\forall k\in[\![0,K-1]\!]\; \mathbf H_{k+1} = g\left (\boldsymbol\beta_k \mathbf 1^T + \mathbf L_k\mathbf H_k(\mathbf A+\mathbf I)\right )\)</span>$</p>
<p>peut être modifiée en</p>
<div class="math notranslate nohighlight">
\[\forall k\in[\![0,K-1]\!]\; \mathbf H_{k+1} = g\left (\boldsymbol\beta_k \mathbf 1^T + \mathbf L_k\mathbf H_k(\mathbf A+(1+\epsilon_k)\mathbf I)\right )\]</div>
<p>où <span class="math notranslate nohighlight">\(\epsilon_k\)</span> est appris, ou en</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\forall k\in[\![0,K-1]\!]\; \mathbf H_{k+1} &amp;=&amp; g\left (\boldsymbol\beta_k \mathbf 1^T + \mathbf L_k\mathbf H_k\mathbf A+\boldsymbol \psi_k \mathbf H_k\right )\\
&amp;=&amp;g\left (\boldsymbol\beta_k \mathbf 1^T + \begin{pmatrix}\mathbf L_k &amp;\boldsymbol \psi_k\end{pmatrix} \begin{pmatrix}\mathbf H_k\mathbf A\\ \mathbf H_k\end{pmatrix}\right )\\ 
&amp;=&amp;g\left (\boldsymbol\beta_k \mathbf 1^T + \mathbf L'_k \begin{pmatrix}\mathbf H_k\mathbf A\\ \mathbf H_k\end{pmatrix}\right )
\end{aligned}\end{split}\]</div>
<p>où
<span class="math notranslate nohighlight">\(\mathbf L'_k=\begin{pmatrix}\mathbf L_k &amp;\boldsymbol \psi_k\end{pmatrix}\)</span>
permet d’appliquer une transformation linéaire différente au sommet
courant.</p>
</section>
<section id="connexions-residuelles">
<h4>Connexions résiduelles<a class="headerlink" href="#connexions-residuelles" title="Lien vers cette rubrique">#</a></h4>
<p>Avec les connexions résiduelles, la représentation agrégée des voisins
est transformée et passe par la fonction d’activation avant d’être
additionnée ou concaténée avec le sommet actuel :</p>
<div class="math notranslate nohighlight">
\[\begin{split}\mathbf H_{k+1} = \begin{pmatrix}g\left (\boldsymbol\beta_k \mathbf 1^T + \mathbf L_k\mathbf H_k\mathbf A\right ) \\ \mathbf H_{k}   \end{pmatrix}\end{split}\]</div>
</section>
<section id="agregation-moyenne">
<h4>Agrégation moyenne<a class="headerlink" href="#agregation-moyenne" title="Lien vers cette rubrique">#</a></h4>
<p>Les méthodes précédentes regroupent les voisins en additionnant les
représentation des sommets. Cependant, il est possible de combiner
différemment ces représentations. Parfois, il est préférable de prendre
la moyenne des voisins plutôt que la somme. Cette méthode peut s’avérer
plus performante si les informations de représentation sont plus
importantes et les informations structurelles moins, car la part de
contribution du voisinage ne dépend pas du nombre de voisins :</p>
<div class="math notranslate nohighlight">
\[\mathbf f(i) = \frac{1}{|\mathcal{V}_i|}\displaystyle\sum_{j\in\mathcal{V}_i} \mathbf h(j)\]</div>
<p>où <span class="math notranslate nohighlight">\(\mathcal{V}_i\)</span> désigne l’ensemble des voisins du sommet <span class="math notranslate nohighlight">\(i\)</span>. En
notation matricielle, si <span class="math notranslate nohighlight">\(\mathbf D\)</span> est la matrice diagonale des degrés
alors</p>
<div class="math notranslate nohighlight">
\[\forall k\in[\![0,K-1]\!]\; \mathbf H_{k+1} = g\left (\boldsymbol\beta_k \mathbf 1^T + \mathbf L_k\mathbf H_k(\mathbf A\mathbf D^{-1} + I) \right )\]</div>
</section>
<section id="normalisation-de-kipf">
<h4>Normalisation de Kipf<a class="headerlink" href="#normalisation-de-kipf" title="Lien vers cette rubrique">#</a></h4>
<p>Ici</p>
<div class="math notranslate nohighlight">
\[\mathbf f(i) = \displaystyle\sum_{j\in\mathcal{V}_i} \frac{\mathbf h(j)}{\sqrt{|\mathcal{V}_i||\mathcal{V}_j|}}\]</div>
<p>l’information provenant des sommets ayant un grand nombre de voisins
devant être revue à la baisse (il existe un grand nombre d’arcs qui
fournissent moins d’information unique). En notation matricielle, cette
normalisation s’écrit</p>
<div class="math notranslate nohighlight">
\[\forall k\in[\![0,K-1]\!]\; \mathbf H_{k+1} = g\left (\boldsymbol\beta_k \mathbf 1^T + \mathbf L_k\mathbf H_k(\mathbf D^{-\frac12}\mathbf A\mathbf D^{-\frac12} + I) \right )\]</div>
</section>
<section id="agregation-par-max-pooling">
<h4>Agrégation par max pooling<a class="headerlink" href="#agregation-par-max-pooling" title="Lien vers cette rubrique">#</a></h4>
<p>Comme dans le cas des CNN, on peut envisager d’agréger par le max, qui
s’effectue alors composante par composante.</p>
<div class="math notranslate nohighlight">
\[\mathbf f(i) = \displaystyle\max_{j\in\mathcal{V}_i} \mathbf h(j)\]</div>
</section>
<section id="agregation-par-attention">
<h4>Agrégation par attention<a class="headerlink" href="#agregation-par-attention" title="Lien vers cette rubrique">#</a></h4>
<p>Les méthodes d’agrégation examinées jusqu’à présent pondèrent la
contribution des voisins de manière égale ou d’une manière qui dépend de
la topologie du graphe. Inversement, dans les couches d’attention de
graphe, les poids dépendent des données aux sommets. Une transformation
linéaire est appliquée aux représentations des sommets</p>
<div class="math notranslate nohighlight">
\[\forall k\in[\![0,K-1]\!]\; \mathbf H'_{k} = \boldsymbol\beta_k \mathbf 1^T + \mathbf L_k\mathbf H_k\]</div>
<p>La similarité <span class="math notranslate nohighlight">\(s_{ij}\)</span> entre les représentations transformées
<span class="math notranslate nohighlight">\(\mathbf h'_i, \mathbf h'_j\)</span> des sommets <span class="math notranslate nohighlight">\(i\)</span> et <span class="math notranslate nohighlight">\(j\)</span> est calculée en
concaténant les paires, en effectuant un produit scalaire avec un
vecteur colonne <span class="math notranslate nohighlight">\(\boldsymbol \phi_k\)</span> de paramètres appris et en appliquant une
fonction d’activation</p>
<div class="math notranslate nohighlight">
\[\begin{split}s_{ij} = g\left (\boldsymbol \phi_k^T \begin{pmatrix}\mathbf h'_i\\ \mathbf h'_j \end{pmatrix}\right )\end{split}\]</div>
<p>Les similarités sont stockées dans une matrice <span class="math notranslate nohighlight">\(\mathbf S\)</span>. Comme pour
les mécanismes d’attention, les poids doivent être positifs et de somme
1, mais pour un sommet donné, seuls lui et ses voisins doivent
contribuer. On effectue donc l’opération</p>
<div class="math notranslate nohighlight">
\[\forall k\in[\![0,K-1]\!]\; \mathbf H_{k+1} = g\left (\mathbf H'_{k}.\textrm{Softmax}(\mathbf S,\mathbf A+\mathbf I) \right )\]</div>
<p>la fonction <span class="math notranslate nohighlight">\(\textrm{Softmax}(\bullet,\bullet)\)</span> calcule les valeurs
d’attention en appliquant l’opération softmax séparément à chaque
colonne de son premier argument <span class="math notranslate nohighlight">\(\mathbf S\)</span>, mais seulement après avoir
fixé à <span class="math notranslate nohighlight">\(-\infty\)</span> les valeurs pour lesquelles le deuxième argument
<span class="math notranslate nohighlight">\(\mathbf A+\mathbf I\)</span> est égal à zéro, de sorte qu’elles ne contribuent
pas. Cela garantit que l’attention accordée aux sommets non voisins est
nulle.</p>
</section>
</section>
<section id="prise-en-compte-de-l-information-des-arcs-ou-aretes-s-e">
<h3>Prise en compte de l’information des arcs ou arêtes {#S:E}<a class="headerlink" href="#prise-en-compte-de-l-information-des-arcs-ou-aretes-s-e" title="Lien vers cette rubrique">#</a></h3>
<p>Les paragraphes précédents ont abordé le traitement des représentrations
des sommets. Ceux-ci évoluent au fur et à mesure qu’ils sont transmis
dans le réseau, de sorte qu’à la fin ils représentent à la fois le
sommet et son contexte dans le graphe. On considère maintenant le cas où
l’information est associée aux arêtes du graphe.</p>
<p>Il est facile d’adapter le mécanisme de représentation précédent pour
traiter la représentation des arêtes à l’aide du graphe des arêtes (ou
graphe adjoint). Il s’agit d’un graphe complémentaire, dans lequel
chaque arête du graphe original devient un sommet, et chaque paire
d’arêtes ayant un sommet commun dans le graphe original crée une arête
dans le nouveau graphe. En général, un graphe peut être reconstruit à
partir de son graphe d’arêtes, de sorte qu’il est possible de passer
d’une représentation à l’autre.</p>
<p>Une fois le graphe d’arêtes construit, on utilise les mêmes techniques,
en agrégeant les informations de chaque nouveau sommet à partir de ses
voisins et en les combinant avec la représentation actuelle. Lorsque les
représentations des sommets et d’arêtes sont tous deux présents, on peut
passer d’un graphe à l’autre. Il existe donc quatre mises à jour
possibles (les sommets mettent à jour les sommets, les sommets mettent à
jour les arêtes, les arêtes mettent à jour les sommets et les arêtes
mettent à jour les arêtes), qui peuvent être alternées à volonté ou,
moyennant des modifications mineures, les sommets peuvent être mis à
jour simultanément à partir des sommets et des arêtes.</p>
</section>
</section>
<section id="partie-pratique">
<h2>Partie pratique<a class="headerlink" href="#partie-pratique" title="Lien vers cette rubrique">#</a></h2>
<p>Pour bien comprendre la structure d’un GNN, on propose de ne pas
utiliser de librairie dédiée (type
<a class="reference external" href="https://graphneural.network/">Spectral</a>,
<a class="reference external" href="https://stellargraph.readthedocs.io/en/stable/README.html">StellarGraph</a>
ou encore <a class="reference external" href="https://github.com/deepmind/graph_nets">GraphNets</a>), mais
plutôt de l’implémenter directement à partir de Tensorflow et Keras.</p>
<p># Classification de sommets par GNN</p>
<p>On propose ici de construire et apprendre un GNN pour prédire le sujet
d’un article à partir de son contenu et de ses citations. Nous
utiliserons pour cela le [jeu de données
Cora](<a class="reference external" href="https://relational.fit.cvut.cz/dataset/CORA">https://relational.fit.cvut.cz/dataset/CORA</a>)</p>
<p>## Jeu de données</p>
<p>Le jeu de données Cora comprend 2 708 articles scientifiques de machine
learning étiquetés avec l’un des 7 thèmes suivants : Neural_Networks,
Probabilistic_Methods, Genetic_Algorithms, Theory ,Case_Based,
Reinforcement_Learning et Rule_Learning.</p>
<p>Les articles sont reliés par une arc indiquant quel article cite quel
autre. Il existe 5 429 citations dans la base.</p>
<p>Chaque article possède un vecteur de mots binaire de taille 1433,
indiquant la présence d’un mot correspondant.</p>
<p>En pratique, le jeu de données comporte deux fichiers - “cora.cites” qui
gère les citations. Les deux colonnes donne l’article cité
(“cited_paper_id”) et l’article qui cite (“citing_paper_id”). -
“cora.content” qui gère le contenu des papiers, et qui contient 1435
colonnes : “paper_id”, “subject”, et 1433 descripteurs binaires.</p>
<hr class="footnotes docutils" />
<aside class="footnote-list brackets">
<aside class="footnote brackets" id="id3" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id1">1</a><span class="fn-bracket">]</span></span>
<p>Une fonction <span class="math notranslate nohighlight">\(f\)</span> est équivariante pour une transformation <span class="math notranslate nohighlight">\(t\)</span> si
pour tout <span class="math notranslate nohighlight">\(x, f(t(x)) = t(f(x))\)</span></p>
</aside>
</aside>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="transformers.html"
       title="page précédente">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">précédent</p>
        <p class="prev-next-title">Transformers</p>
      </div>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contenu
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#representation-d-un-graphe">Représentation d’un graphe</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#definition-matricielle-d-un-graphe">Définition matricielle d’un graphe</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#proprietes">Propriétés</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction-aux-gnn">Introduction aux GNN</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#utilisations">Utilisations</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#en-relation-avec-la-structure-de-graphe">En relation avec la structure de graphe</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#en-relation-avec-les-sommets">En relation avec les sommets</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#en-relation-avec-les-arcs-ou-aretes">En relation avec les arcs ou arêtes</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#gcn">GCN</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#equivariance-et-invariance">Equivariance et invariance</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#partage-des-parametres">Partage des paramètres</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#exemple">Exemple</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#application-en-classification">Application en classification</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#modeles-inductifs-et-transductifs">Modèles inductifs et transductifs</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#exemple-en-classification-de-sommets">Exemple en classification de sommets</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#echantillonnage-du-voisinage">Échantillonnage du voisinage</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#partitionnement-du-graphe">Partitionnement du graphe</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#couches-d-un-gnn">Couches d’un GNN</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#amelioration-de-la-diagonale">Amélioration de la diagonale</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#connexions-residuelles">Connexions résiduelles</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#agregation-moyenne">Agrégation moyenne</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#normalisation-de-kipf">Normalisation de Kipf</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#agregation-par-max-pooling">Agrégation par max pooling</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#agregation-par-attention">Agrégation par attention</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#prise-en-compte-de-l-information-des-arcs-ou-aretes-s-e">Prise en compte de l’information des arcs ou arêtes {#S:E}</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#partie-pratique">Partie pratique</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
Par Vincent BARRA
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=8d27b9dea8ad943066ae"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=8d27b9dea8ad943066ae"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>