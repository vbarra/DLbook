---
jupytext:
  formats: md:myst
  text_representation:
    extension: .md
    format_name: myst
kernelspec:
  display_name: Python 3
  language: python
  name: python3
---
# Mod√®les de diffusion

## Introduction

Un mod√®le de diffusion (de d√©bruitage) transforme du bruit √† partir d'une distribution simple en un √©chantillon de donn√©es. Lorsque les donn√©es sont des images, le mod√®le se compose de deux processus ({numref}`diffusion`): 
- un processus de diffusion vers l'avant $q$, choisi, qui ajoute progressivement du bruit gaussien √† une image, jusqu'√† aboutir √† du bruit pur
- un processus de diffusion inverse de d√©bruitage $p_{\boldsymbol\theta}$ , mod√©lis√© par un r√©seau de neurones, entra√Æn√© √† d√©bruiter progressivement une image √† partir d'un bruit pur, jusqu'√† obtenir une image r√©elle.

Les processus sont temporels, index√©s par le temps $t\in[\![0,T]\!]$. A $t=0$, on  √©chantillonne une image r√©elle $\boldsymbol ùê±_0$ de la distribution de donn√©es. Le processus $q$ √©chantillonne un bruit provenant d'une distribution gaussienne √† chaque pas de temps $t$ ,  ajout√© √† l'image du pas de temps pr√©c√©dent. Si $T$ est suffisamment et que les processus d'ajout de bruit est correct, on obtient une distribution gaussienne isotrope √† $t=T$.

```{prf:remark}
:class: dropdown
 Une distribution gaussienne isotrope $\mathcal N(\boldsymbol \mu,\boldsymbol \Sigma)$ est telle que $\boldsymbol \Sigma = \sigma^2\boldsymbol I$.
 ```


```{figure} ./images/diffusion.png
:name: diffusion
Illustration du mod√®le de diffusion (source : [{cite:p}`Ho20`](https://proceedings.neurips.cc/paper_files/paper/2020/file/4c5bcfec8584af0d967f1ab10179ca4b-Paper.pdf))
```


## Mod√®le de diffusion

Soit $q(\mathbf{x}_0)$ la distribution des donn√©es r√©elles. On √©chantillonne (\mathbf{x}_0 \sim q(\mathbf{x}_0)$ et on d√©finit un processus de diffusion avant $q(\mathbf{x}_t | \mathbf{x}_{t-1})$ qui ajoute un bruit gaussien √† chaque pas de temps $t\in[\![1,T]\!]$, selon une mise √† jour de la variance connue  $0 < \beta_1 < \beta_2 < ... < \beta_T < 1$ : 

$$
q(\mathbf{x}_t | \mathbf{x}_{t-1}) = \mathcal{N}(\mathbf{x}_t; \sqrt{1 - \beta_t} \mathbf{x}_{t-1}, \beta_t \mathbf{I})
$$

Chaque donn√©e $\boldsymbol x_t$ est ainsi tir√©e selon une distribution conditionnelle gaussienne  $\mathbf{\mu}_t = \sqrt{1 - \beta_t} \mathbf{x}_{t-1}$ et  $\sigma^2_t = \beta_t$,ce qui peut √™tre r√©alis√© en √©chantillonnant selon  $\mathbf{\epsilon} \sim \mathcal{N}(\mathbf{0}, \mathbf{I})$ et en posant  $\mathbf{x}_t = \sqrt{1 - \beta_t} \mathbf{x}_{t-1} +  \sqrt{\beta_t} \mathbf{\epsilon}$. 

En d√©finissant les $\beta_t$ correctement, $\mathbf{x}_T$ est un bruit gaussien.

Si on connaissait la distribution conditionnelle  $p(\mathbf{x}_{t-1} | \mathbf{x}_t)$, alors on pourrait calculer le processus inverse en tirant $\mathbf{x}_T$ selon une distribution gaussienne isotrope, et en "d√©bruitant" progressivement pour aboutir en $t=0$ √† une r√©alisation $\mathbf{x}_0$$ de la distribution des donn√©es. Cependant, $p(\mathbf{x}_{t-1} | \mathbf{x}_t)$ n'est pas accessible et on utiliser un r√©seau de neurones $p_{\boldsymbol \theta} (\mathbf{x}_{t-1} | \mathbf{x}_t)$ pour approcher cette distribution conditionnelle, o√π $\boldsymbol \theta$ est l'ensemble des param√®tres du r√©seau.

SI on suppose que le processus inverse est gaussien, alors on peut √©crire 

$$ p_{\boldsymbol \theta} (\mathbf{x}_{t-1} | \mathbf{x}_t) = \mathcal{N}(\mathbf{x}_{t-1}; \mu_\theta(\mathbf{x}_{t},t), \Sigma_\theta (\mathbf{x}_{t},t))$$
