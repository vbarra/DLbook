---
jupytext:
  formats: md:myst
  text_representation:
    extension: .md
    format_name: myst
kernelspec:
  display_name: Python 3
  language: python
  name: python3
---
# Mod√®les de diffusion

## Introduction

Un mod√®le de diffusion (de d√©bruitage) transforme du bruit √† partir d'une distribution simple en un √©chantillon de donn√©es. Lorsque les donn√©es sont des images, le mod√®le se compose de deux processus ({numref}`diffusion`): 
- un processus de diffusion vers l'avant $q$, choisi, qui ajoute progressivement du bruit gaussien √† une image, jusqu'√† aboutir √† du bruit pur
- un processus de diffusion inverse de d√©bruitage $p_{\boldsymbol\theta}$ , mod√©lis√© par un r√©seau de neurones, entra√Æn√© √† d√©bruiter progressivement une image √† partir d'un bruit pur, jusqu'√† obtenir une image r√©elle.

Les processus sont temporels, index√©s par le temps $t\in[\![0,T]\!]$. A $t=0$, on  √©chantillonne une image r√©elle $\boldsymbol ùê±_0$ de la distribution de donn√©es. Le processus $q$ √©chantillonne un bruit provenant d'une distribution gaussienne √† chaque pas de temps $t$ ,  ajout√© √† l'image du pas de temps pr√©c√©dent. Si $T$ est suffisamment et que les processus d'ajout de bruit est correct, on obtient une distribution gaussienne isotrope √† $t=T$.

```{prf:remark}
:class: dropdown
 Une distribution gaussienne isotrope $\mathcal N(\boldsymbol \mu,\boldsymbol \Sigma)$ est telle que $\boldsymbol \Sigma = \sigma^2\boldsymbol I$.
 ```


```{figure} ./images/diffusion.png
:name: diffusion
Illustration du mod√®le de diffusion (source : [{cite:p}`Ho20`](https://proceedings.neurips.cc/paper_files/paper/2020/file/4c5bcfec8584af0d967f1ab10179ca4b-Paper.pdf))
```


## Mod√®le de diffusion

### Mod√®le
Soit $q(\mathbf{x}_0)$ la distribution des donn√©es r√©elles. On √©chantillonne $\mathbf{x}_0 \sim q(\mathbf{x}_0)$ et on d√©finit un processus de diffusion avant $q(\mathbf{x}_t | \mathbf{x}_{t-1})$ qui ajoute un bruit gaussien √† chaque pas de temps $t\in[\![1,T]\!]$, selon une mise √† jour de la variance connue  $0 < \beta_1 < \beta_2 < ... < \beta_T < 1$ : 

$$
q(\mathbf{x}_t | \mathbf{x}_{t-1}) = \mathcal{N}(\mathbf{x}_t; \sqrt{1 - \beta_t} \mathbf{x}_{t-1}, \beta_t \mathbf{I})
$$

Chaque donn√©e $\boldsymbol x_t$ est ainsi tir√©e selon une distribution conditionnelle gaussienne  $\mathbf{\mu}_t = \sqrt{1 - \beta_t} \mathbf{x}_{t-1}$ et  $\sigma^2_t = \beta_t$,ce qui peut √™tre r√©alis√© en √©chantillonnant selon  $\mathbf{\varepsilon} \sim \mathcal{N}(\mathbf{0}, \mathbf{I})$ et en posant  $\mathbf{x}_t = \sqrt{1 - \beta_t} \mathbf{x}_{t-1} +  \sqrt{\beta_t} \mathbf{\varepsilon}$. 

En d√©finissant les $\beta_t$ correctement, $\mathbf{x}_T$ est un bruit gaussien.

Si on connaissait la distribution conditionnelle  $p(\mathbf{x}_{t-1} | \mathbf{x}_t)$, alors on pourrait calculer le processus inverse en tirant $\mathbf{x}_T$ selon une distribution gaussienne isotrope, et en "d√©bruitant" progressivement pour aboutir en $t=0$ √† une r√©alisation $\mathbf{x}_0$ de la distribution des donn√©es. Cependant, $p(\mathbf{x}_{t-1} | \mathbf{x}_t)$ n'est pas accessible et on utilise un r√©seau de neurones $p_{\boldsymbol \theta} (\mathbf{x}_{t-1} | \mathbf{x}_t)$ pour approcher cette distribution conditionnelle, o√π $\boldsymbol \theta$ est l'ensemble des param√®tres du r√©seau.

Si on suppose que le processus inverse est gaussien, alors on peut √©crire 

$$ p_{\boldsymbol \theta} (\mathbf{x}_{t-1} | \mathbf{x}_t) = \mathcal{N}(\mathbf{x}_{t-1}; \mu_\theta(\mathbf{x}_{t},t), \Sigma_\theta (\mathbf{x}_{t},t))$$

Le r√©seau doit donc apprendre la moyenne et la variance, qui dependent du temps. Dans l'impl√©mentation initiale {cite:p}`Ho20`, les auteurs relaxent la contrainte de la variance ($\Sigma_\theta ( \mathbf{x}_t, t) = \sigma^2_t \mathbf{I}$), et apprennent uniquement la moyenne. Dans les impl√©mentations suivantes (par exemple [celle-ci](https://openreview.net/pdf?id=-NEXDKk8gZ)), la contrainte a √©t√© prise en compte.

Dans la suite, on suppose seulement apprendre la moyenne.

### Apprentissage


La combinaison de $q$ et $p_{\boldsymbol\theta}$ peut-√™tre vue comme un autoencodeur variationnel. Ainsi, ELBO peut √™tre utilis√©e pour minimiser la log-vraisemblance n√©gative par rapport √† $\mathbf{x}_0$ . ELBO est la somme de fonctions de pertes calcul√©es √† chaque pas de temps 

$\ell  = \displaystyle\sum_{t=0}^T \ell_t$. 

Par construction de $q$ et $p_{\boldsymbol\theta}$, les $\ell_t,t\in[\![1,T]\!]$ sont les divergences de Kullback-Leibler entre deux distributions gaussiennes, ce qui peut √™tre √©crit comme une perte $\ell_2$ calcul√©e sur les moyennes de ces gaussiennes.

Par construction de $q$, puisque la somme de gaussiennes est √©galemment gaussienne, on peut √©chantillonner $\mathbf{x}_t$ pour tout $t$ conditionnellement √† $\mathbf{x}_0$ : ainsi 

$$q(\mathbf{x}_t | \mathbf{x}_0) = \cal{N}(\mathbf{x}_t; \sqrt{\bar{\alpha}_t} \mathbf{x}_0, (1- \bar{\alpha}_t) \mathbf{I})$$

avec  $\alpha_t := 1 - \beta_t$ et  $\bar{\alpha_t} := \displaystyle\prod_{s=1}^{t} \alpha_s$. 

Les $\bar{\alpha}_t$ sont des fonctions des  $\beta_t$, permettant de mettre √† jour la variance. Ces derniers √©tant connus, les $\bar{\alpha}_t$ le sont aussi et peuvent √™tre pr√©calcul√©s. 
Ainsi, pendant l'entra√Ænement, on tire al√©atoirement $t$ et on optimise $\ell_t$

Il est √©galement possible de reparam√©triser la moyenne pour que le r√©seau de neurones apprenne le bruit ajout√© (via un r√©seau $\mathbf{\varepsilon}_{\boldsymbol \theta}(\mathbf{x}_t, t)$ pour un niveau de bruit $t$ dans les divergences de Kullback-Leibler d√©finissant les $\ell_t$. Le r√©seau $p_{\boldsymbol \theta}$ pr√©dit donc le bruit plut√¥t que la moyenne, qui peut ensuite √™tre calcul√©e par 

$$ \mathbf{\mu}_\theta(\mathbf{x}_t, t) = \frac{1}{\sqrt{\alpha_t}} \left(  \mathbf{x}_t - \frac{\beta_t}{\sqrt{1- \bar{\alpha}_t}} \mathbf{\varepsilon}_\theta(\mathbf{x}_t, t) \right)$$

La fonction objectif finale $\ell_t$, √† $t$ choisi al√©atoirement et √©tant donn√© $\mathbf{\varepsilon} \sim \mathcal{N}(\mathbf{0}, \mathbf{I})$ : 

$$ \| \mathbf{\varepsilon} - \mathbf{\varepsilon}_\theta(\mathbf{x}_t, t) \|^2 = \| \mathbf{\varepsilon} - \mathbf{\varepsilon}_\theta( \sqrt{\bar{\alpha}_t} \mathbf{x}_0 + \sqrt{(1- \bar{\alpha}_t)  } \mathbf{\varepsilon}, t) \|^2.$$

avec $\mathbf{x}_0$ la donn√©e initiale. $\mathbf{\varepsilon}$ est le bruit √©chantillonn√© au temps $t$ et $\mathbf{\varepsilon}_\theta (\mathbf{x}_t, t)$ est le r√©seau de neurones, √©quip√© d'une fonction de perte quadratique entre le bruit r√©el et le bruit gaussien pr√©dit.


```{prf:algorithm} Algorithme d'apprentissage d'un mod√®le de diffusion 
1. Tant que (non convergence)
    1. $\boldsymbol x_0\sim q(\boldsymbol x_0)$
    2. $t\sim Uniform(1,T)$
    3. $\varepsilon\sim\mathcal N(\boldsymbol 0,\boldsymbol I)$
    4. Calculer $\nabla_{\boldsymbol\theta}\|\mathbf\varepsilon - \mathbf{\varepsilon}_\theta(\sqrt{\bar{\alpha}_t} \mathbf{x}_0 + \sqrt{(1- \bar{\alpha}_t)  } \mathbf{\varepsilon}, t) \|^2$
    5. Mettre √† jour $\boldsymbol\theta$ par un pas de descente de gradient
```
Plut√¥t qu'un seul exemple $x_0$, le r√©seau est classiquement entra√Æn√© sur un batch.