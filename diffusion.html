
<!DOCTYPE html>


<html lang="fr" data-content_root="./" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Modèles de diffusion &#8212; Apprentissage profond</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.5.1/css/all.min.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.1/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.1/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.1/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="_static/styles/sphinx-book-theme.css?v=384b581d" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="_static/proof.css?v=ca93fcec" />
    <link rel="stylesheet" type="text/css" href="_static/design-style.1e8bd061cd6da7fc9cf755528e8ffc24.min.css?v=0a3b3ea7" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=8d27b9dea8ad943066ae" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=8d27b9dea8ad943066ae" />
  <script src="_static/vendor/fontawesome/6.5.1/js/all.min.js?digest=8d27b9dea8ad943066ae"></script>

    <script src="_static/documentation_options.js?v=72dce1d2"></script>
    <script src="_static/doctools.js?v=9a2dae69"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="_static/copybutton.js?v=f281be69"></script>
    <script src="_static/scripts/sphinx-book-theme.js?v=efea14e4"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js?v=4a39c7ea"></script>
    <script src="_static/translations.js?v=bf059b8c"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js?v=36754332"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'diffusion';</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Recherche" href="search.html" />
    <link rel="prev" title="Réseaux antagonistes générateurs" href="gan.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="fr"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a id="pst-skip-link" class="skip-link" href="#main-content">Passer au contenu principal</a>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Haut de page
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <header class="bd-header navbar navbar-expand-lg bd-navbar">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  

<a class="navbar-brand logo" href="intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="_static/logo.png" class="logo__image only-light" alt="Apprentissage profond - Home"/>
    <script>document.write(`<img src="_static/logo.png" class="logo__image only-dark" alt="Apprentissage profond - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn navbar-btn search-button-field search-button__button" title="Recherche" aria-label="Recherche" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Recherche</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Introduction</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="NN.html">Introduction aux réseaux de neurones</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Modèles classiques</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="perceptron.html">Perceptron</a></li>
<li class="toctree-l1"><a class="reference internal" href="PMC.html">Perceptrons multicouches</a></li>
<li class="toctree-l1"><a class="reference internal" href="cnn.html">Réseaux convolutifs</a></li>
<li class="toctree-l1"><a class="reference internal" href="ae.html">Auto-encodeurs</a></li>
<li class="toctree-l1"><a class="reference internal" href="rnn.html">Réseaux récurrents</a></li>
<li class="toctree-l1"><a class="reference internal" href="transferLearning.html">Utilisation de réseaux existants</a></li>
<li class="toctree-l1"><a class="reference internal" href="transformers.html">Transformers</a></li>
<li class="toctree-l1"><a class="reference internal" href="gnn.html">Graph Neural Networks</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Modèles génératifs</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="vae.html">Autoencodeurs variationnels</a></li>
<li class="toctree-l1"><a class="reference internal" href="gan.html">Réseaux antagonistes générateurs</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Modèles de diffusion</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">



<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Mode plein écran"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="clair/sombre" aria-label="clair/sombre" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Recherche" aria-label="Recherche" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Modèles de diffusion</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contenu </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction">Introduction</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#modele-de-diffusion">Modèle de diffusion</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#modele">Modèle</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#apprentissage">Apprentissage</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#reseau-de-neurones">Réseau de neurones</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#implementation">Implémentation</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="modeles-de-diffusion">
<h1>Modèles de diffusion<a class="headerlink" href="#modeles-de-diffusion" title="Lien vers cette rubrique">#</a></h1>
<section id="introduction">
<h2>Introduction<a class="headerlink" href="#introduction" title="Lien vers cette rubrique">#</a></h2>
<p>Un modèle de diffusion (de débruitage) transforme du bruit à partir d’une distribution simple en un échantillon de données. Le modèle se compose de deux processus (<a class="reference internal" href="#diffusion"><span class="std std-numref">Fig. 70</span></a>) :</p>
<ul class="simple">
<li><p>un processus de diffusion vers l’avant <span class="math notranslate nohighlight">\(q\)</span>, choisi, qui ajoute progressivement du bruit gaussien à une donnée, jusqu’à aboutir à du bruit pur</p></li>
<li><p>un processus de diffusion inverse de débruitage <span class="math notranslate nohighlight">\(p_{\boldsymbol\theta}\)</span> , modélisé par un réseau de neurones, entraîné à débruiter progressivement une donnée à partir d’un bruit pur, jusqu’à obtenir une réalisation d’une donnée réelle.</p></li>
</ul>
<p>Les processus sont temporels, indexés par le temps <span class="math notranslate nohighlight">\(t\in[\![0,T]\!]\)</span>. A <span class="math notranslate nohighlight">\(t=0\)</span>, on  échantillonne une donnée réelle <span class="math notranslate nohighlight">\(\boldsymbol 𝐱_0\)</span> de la distribution de données. Le processus <span class="math notranslate nohighlight">\(q\)</span> échantillonne un bruit provenant d’une distribution gaussienne à chaque pas de temps <span class="math notranslate nohighlight">\(t\)</span> ,  ajouté à la donnée du pas de temps précédent. Si <span class="math notranslate nohighlight">\(T\)</span> est suffisamment et que les processus d’ajout de bruit est correct, on obtient une distribution gaussienne isotrope à <span class="math notranslate nohighlight">\(t=T\)</span>.</p>
<div class="proof remark dropdown admonition" id="remark-0">
<p class="admonition-title"><span class="caption-number">Remark 3 </span></p>
<section class="remark-content" id="proof-content">
<p>Une distribution gaussienne isotrope <span class="math notranslate nohighlight">\(\mathcal N(\boldsymbol \mu,\boldsymbol \Sigma)\)</span> est telle que <span class="math notranslate nohighlight">\(\boldsymbol \Sigma = \sigma^2\boldsymbol I\)</span>.</p>
</section>
</div><figure class="align-default" id="diffusion">
<img alt="_images/diffusion.png" src="_images/diffusion.png" />
<figcaption>
<p><span class="caption-number">Fig. 70 </span><span class="caption-text">Illustration du modèle de diffusion (source : <a class="reference external" href="https://proceedings.neurips.cc/paper_files/paper/2020/file/4c5bcfec8584af0d967f1ab10179ca4b-Paper.pdf"><span id="id1">[<a class="reference internal" href="#id53" title="Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, 6840–6851. Curran Associates, Inc., 2020.">1</a>]</span></a>)</span><a class="headerlink" href="#diffusion" title="Lien vers cette image">#</a></p>
</figcaption>
</figure>
</section>
<section id="modele-de-diffusion">
<h2>Modèle de diffusion<a class="headerlink" href="#modele-de-diffusion" title="Lien vers cette rubrique">#</a></h2>
<section id="modele">
<h3>Modèle<a class="headerlink" href="#modele" title="Lien vers cette rubrique">#</a></h3>
<p>Soit <span class="math notranslate nohighlight">\(q(\mathbf{x}_0)\)</span> la distribution des données réelles. On échantillonne <span class="math notranslate nohighlight">\(\mathbf{x}_0 \sim q(\mathbf{x}_0)\)</span> et on définit un processus de diffusion avant <span class="math notranslate nohighlight">\(q(\mathbf{x}_t | \mathbf{x}_{t-1})\)</span> qui ajoute un bruit gaussien à chaque pas de temps <span class="math notranslate nohighlight">\(t\in[\![1,T]\!]\)</span>, selon une mise à jour de la variance connue  <span class="math notranslate nohighlight">\(0 &lt; \beta_1 &lt; \beta_2 &lt; ... &lt; \beta_T &lt; 1\)</span> :</p>
<div class="math notranslate nohighlight">
\[
q(\mathbf{x}_t | \mathbf{x}_{t-1}) = \mathcal{N}(\sqrt{1 - \beta_t} \mathbf{x}_{t-1}, \beta_t \mathbf{I})
\]</div>
<p>Chaque donnée <span class="math notranslate nohighlight">\(\boldsymbol x_t\)</span> est ainsi tirée selon une distribution conditionnelle gaussienne  <span class="math notranslate nohighlight">\(\mathbf{\mu}_t = \sqrt{1 - \beta_t} \mathbf{x}_{t-1}\)</span> et  <span class="math notranslate nohighlight">\(\sigma^2_t = \beta_t\)</span>,ce qui peut être réalisé en échantillonnant selon  <span class="math notranslate nohighlight">\(\mathbf{\varepsilon} \sim \mathcal{N}(\mathbf{0}, \mathbf{I})\)</span> et en posant  <span class="math notranslate nohighlight">\(\mathbf{x}_t = \sqrt{1 - \beta_t} \mathbf{x}_{t-1} +  \sqrt{\beta_t} \mathbf{\varepsilon}\)</span>.</p>
<p>En définissant les <span class="math notranslate nohighlight">\(\beta_t\)</span> correctement, <span class="math notranslate nohighlight">\(\mathbf{x}_T\)</span> est un bruit gaussien.</p>
<p>Originellement, la croissance des <span class="math notranslate nohighlight">\(\beta_t\)</span> a été supposée linéaire, de <span class="math notranslate nohighlight">\(\beta_1 = 10^{−4}\)</span> à <span class="math notranslate nohighlight">\(\beta_T = 0.02\)</span> <span id="id2">[<a class="reference internal" href="#id53" title="Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, 6840–6851. Curran Associates, Inc., 2020.">1</a>]</span>. Cependant, des auteurs ont montré qu’une mise à jour des coefficients <span class="math notranslate nohighlight">\(\beta_t\)</span> à l’aide d’un cosinus était plus efficace :</p>
<div class="math notranslate nohighlight">
\[\beta_t = 1-\frac{\bar{\alpha_t}}{\bar{\alpha_t}-1}\textrm{ avec } \bar{\alpha_t} = \frac{f(t)}{f(0)}\textrm{où } f(t) = cos\left (\frac{\pi}{2}\frac{t/T+s}{1+s}\right ), s \textrm{ offset}\]</div>
<p>Si on connaissait la distribution conditionnelle  <span class="math notranslate nohighlight">\(p(\mathbf{x}_{t-1} | \mathbf{x}_t)\)</span>, alors on pourrait calculer le processus inverse en tirant <span class="math notranslate nohighlight">\(\mathbf{x}_T\)</span> selon une distribution gaussienne isotrope, et en « débruitant » progressivement pour aboutir en <span class="math notranslate nohighlight">\(t=0\)</span> à une réalisation <span class="math notranslate nohighlight">\(\mathbf{x}_0\)</span> de la distribution des données. Cependant, <span class="math notranslate nohighlight">\(p(\mathbf{x}_{t-1} | \mathbf{x}_t)\)</span> n’est pas accessible et on utilise un réseau de neurones <span class="math notranslate nohighlight">\(p_{\boldsymbol \theta} (\mathbf{x}_{t-1} | \mathbf{x}_t)\)</span> pour approcher cette distribution conditionnelle, où <span class="math notranslate nohighlight">\(\boldsymbol \theta\)</span> est l’ensemble des paramètres du réseau.</p>
<p>Si on suppose que le processus inverse est gaussien, alors on peut écrire</p>
<div class="math notranslate nohighlight">
\[ p_{\boldsymbol \theta} (\mathbf{x}_{t-1} | \mathbf{x}_t) = \mathcal{N}(\mu_\theta(\mathbf{x}_{t},t), \Sigma_\theta (\mathbf{x}_{t},t))\]</div>
<p>Le réseau doit donc apprendre la moyenne et la variance, qui dependent du temps. Dans l’implémentation initiale <span id="id3">[<a class="reference internal" href="#id53" title="Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, 6840–6851. Curran Associates, Inc., 2020.">1</a>]</span>, les auteurs relaxent la contrainte de la variance (<span class="math notranslate nohighlight">\(\Sigma_\theta ( \mathbf{x}_t, t) = \sigma^2_t \mathbf{I}\)</span>), et apprennent uniquement la moyenne. Dans les implémentations suivantes (par exemple <a class="reference external" href="https://openreview.net/pdf?id=-NEXDKk8gZ">celle-ci</a>), la contrainte a été prise en compte.</p>
<p>Dans la suite, on suppose seulement apprendre la moyenne.</p>
</section>
<section id="apprentissage">
<h3>Apprentissage<a class="headerlink" href="#apprentissage" title="Lien vers cette rubrique">#</a></h3>
<p>La combinaison de <span class="math notranslate nohighlight">\(q\)</span> et <span class="math notranslate nohighlight">\(p_{\boldsymbol\theta}\)</span> peut-être vue comme un autoencodeur variationnel. Ainsi, ELBO peut être utilisée pour minimiser la log-vraisemblance négative par rapport à <span class="math notranslate nohighlight">\(\mathbf{x}_0\)</span> . ELBO est la somme de fonctions de pertes calculées à chaque pas de temps</p>
<p><span class="math notranslate nohighlight">\(\ell  = \displaystyle\sum_{t=0}^T \ell_t\)</span>.</p>
<p>Par construction de <span class="math notranslate nohighlight">\(q\)</span> et <span class="math notranslate nohighlight">\(p_{\boldsymbol\theta}\)</span>, les <span class="math notranslate nohighlight">\(\ell_t,t\in[\![1,T]\!]\)</span> sont les divergences de Kullback-Leibler entre deux distributions gaussiennes, ce qui peut être écrit comme une perte <span class="math notranslate nohighlight">\(\ell_2\)</span> calculée sur les moyennes de ces gaussiennes.</p>
<p>Par construction de <span class="math notranslate nohighlight">\(q\)</span>, puisque la somme de gaussiennes est égalemment gaussienne, on peut échantillonner <span class="math notranslate nohighlight">\(\mathbf{x}_t\)</span> pour tout <span class="math notranslate nohighlight">\(t\)</span> conditionnellement à <span class="math notranslate nohighlight">\(\mathbf{x}_0\)</span> : ainsi</p>
<div class="math notranslate nohighlight">
\[q(\mathbf{x}_t | \mathbf{x}_0) = \cal{N}(\sqrt{\bar{\alpha}_t} \mathbf{x}_0, (1- \bar{\alpha}_t) \mathbf{I})\]</div>
<p>avec  <span class="math notranslate nohighlight">\(\alpha_t = 1 - \beta_t\)</span> et  <span class="math notranslate nohighlight">\(\bar{\alpha_t} = \displaystyle\prod_{s=1}^{t} \alpha_s\)</span>.</p>
<p>Les <span class="math notranslate nohighlight">\(\bar{\alpha}_t\)</span> sont des fonctions des  <span class="math notranslate nohighlight">\(\beta_t\)</span>, permettant de mettre à jour la variance. Ces derniers étant connus, les <span class="math notranslate nohighlight">\(\bar{\alpha}_t\)</span> le sont aussi et peuvent être précalculés.
Ainsi, pendant l’entraînement, on tire aléatoirement <span class="math notranslate nohighlight">\(t\)</span> et on optimise <span class="math notranslate nohighlight">\(\ell_t\)</span></p>
<p>Il est également possible de reparamétriser la moyenne pour que le réseau de neurones apprenne le bruit ajouté via un réseau <span class="math notranslate nohighlight">\(\mathbf{\varepsilon}_{\boldsymbol \theta}(\mathbf{x}_t, t)\)</span> pour un niveau de bruit <span class="math notranslate nohighlight">\(t\)</span> dans les divergences de Kullback-Leibler définissant les <span class="math notranslate nohighlight">\(\ell_t\)</span>. Le réseau <span class="math notranslate nohighlight">\(p_{\boldsymbol \theta}\)</span> prédit donc le bruit plutôt que la moyenne, qui peut ensuite être calculée par</p>
<div class="math notranslate nohighlight">
\[ \mathbf{\mu}_\theta(\mathbf{x}_t, t) = \frac{1}{\sqrt{\alpha_t}} \left(  \mathbf{x}_t - \frac{\beta_t}{\sqrt{1- \bar{\alpha}_t}} \mathbf{\varepsilon}_\theta(\mathbf{x}_t, t) \right)\]</div>
<p>La fonction objectif finale <span class="math notranslate nohighlight">\(\ell_t\)</span>, à <span class="math notranslate nohighlight">\(t\)</span> choisi aléatoirement et étant donné <span class="math notranslate nohighlight">\(\mathbf{\varepsilon} \sim \mathcal{N}(\mathbf{0}, \mathbf{I})\)</span> est alors :</p>
<div class="math notranslate nohighlight">
\[ \| \mathbf{\varepsilon} - \mathbf{\varepsilon}_\theta(\mathbf{x}_t, t) \|^2 = \| \mathbf{\varepsilon} - \mathbf{\varepsilon}_\theta( \sqrt{\bar{\alpha}_t} \mathbf{x}_0 + \sqrt{(1- \bar{\alpha}_t)  } \mathbf{\varepsilon}, t) \|^2.\]</div>
<p>avec <span class="math notranslate nohighlight">\(\mathbf{x}_0\)</span> la donnée initiale. <span class="math notranslate nohighlight">\(\mathbf{\varepsilon}\)</span> est le bruit échantillonné au temps <span class="math notranslate nohighlight">\(t\)</span> et <span class="math notranslate nohighlight">\(\mathbf{\varepsilon}_\theta (\mathbf{x}_t, t)\)</span> est le réseau de neurones, équipé d’une fonction de perte quadratique entre le bruit réel et le bruit gaussien prédit.</p>
<div class="proof algorithm admonition" id="algorithm-1">
<p class="admonition-title"><span class="caption-number">Algorithm 8 </span> (Algorithme d’apprentissage d’un modèle de diffusion)</p>
<section class="algorithm-content" id="proof-content">
<ol class="arabic simple">
<li><p>Tant que (non convergence)</p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(\boldsymbol x_0\sim q(\boldsymbol x_0)\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(t\sim Uniform(1,T)\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\varepsilon\sim\mathcal N(\boldsymbol 0,\boldsymbol I)\)</span></p></li>
<li><p>Calculer <span class="math notranslate nohighlight">\(\nabla_{\boldsymbol\theta}\|\mathbf\varepsilon - \mathbf{\varepsilon}_\theta(\sqrt{\bar{\alpha}_t} \mathbf{x}_0 + \sqrt{(1- \bar{\alpha}_t)  } \mathbf{\varepsilon}, t) \|^2\)</span></p></li>
<li><p>Mettre à jour <span class="math notranslate nohighlight">\(\boldsymbol\theta\)</span> par un pas de descente de gradient</p></li>
</ol>
</li>
</ol>
</section>
</div><p>Plutôt qu’un seul exemple <span class="math notranslate nohighlight">\(x_0\)</span>, le réseau est classiquement entraîné sur un batch.</p>
</section>
<section id="reseau-de-neurones">
<h3>Réseau de neurones<a class="headerlink" href="#reseau-de-neurones" title="Lien vers cette rubrique">#</a></h3>
<p>Le réseau de neurones <span class="math notranslate nohighlight">\(\mathbf{\varepsilon}_\theta(\mathbf{x}_t, t)\)</span> utilisé dans <a class="reference external" href="https://proceedings.neurips.cc/paper_files/paper/2020/file/4c5bcfec8584af0d967f1ab10179ca4b-Paper.pdf"><span id="id4">[<a class="reference internal" href="#id54" title="Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: convolutional networks for biomedical image segmentation. CoRR, 2015.">2</a>]</span></a> traite des images et les auteurs utilisent le réseau <a class="reference external" href="https://arxiv.org/abs/1505.04597">U-net</a> (<a class="reference internal" href="#unet"><span class="std std-numref">Fig. 71</span></a>)</p>
<figure class="align-default" id="unet">
<img alt="_images/unet.png" src="_images/unet.png" />
<figcaption>
<p><span class="caption-number">Fig. 71 </span><span class="caption-text">Architecture du modèle U-net (source : <a class="reference external" href="https://proceedings.neurips.cc/paper_files/paper/2020/file/4c5bcfec8584af0d967f1ab10179ca4b-Paper.pdf"><span id="id5">[<a class="reference internal" href="#id54" title="Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: convolutional networks for biomedical image segmentation. CoRR, 2015.">2</a>]</span></a>)</span><a class="headerlink" href="#unet" title="Lien vers cette image">#</a></p>
</figcaption>
</figure>
<p>U-Net est une architecture pour la segmentation sémantique. Il se compose d’un chemin de contraction et d’un chemin d’expansion. Le chemin de contraction suit l’architecture typique d’un réseau convolutif. Il consiste en l’application répétée de deux convolutions 3<span class="math notranslate nohighlight">\(\times\)</span>3 , chacune suivie d’une activation ReLU et d’une opération d’agrégation max 2<span class="math notranslate nohighlight">\(\times\)</span>2 avec un stride égal à 2 pour le sous-échantillonnage. À chaque étape de sous-échantillonnage, le nombre de canaux de caractéristiques est doublé. Chaque étape du chemin d’expansion consiste en un suréchantillonnage de la carte de caractéristiques suivi d’une convolution 2<span class="math notranslate nohighlight">\(\times\)</span>2  qui divise par deux le nombre de canaux de caractéristiques, une concaténation avec la carte de caractéristiques recadrée correspondante du chemin de contraction, et deux convolutions 3<span class="math notranslate nohighlight">\(\times\)</span>3, chacune suivie d’une ReLU. Le recadrage est nécessaire en raison de la perte de pixels de bordure dans chaque convolution. Sur la dernière couche, une convolution 1<span class="math notranslate nohighlight">\(\times\)</span>1 est utilisée pour faire correspondre chaque vecteur de caractéristiques à 64 composantes au nombre de classes souhaité. Au total, le réseau comporte 23 couches de convolution.</p>
</section>
</section>
<section id="implementation">
<h2>Implémentation<a class="headerlink" href="#implementation" title="Lien vers cette rubrique">#</a></h2>
<p>De nombreuses implémentations du modèle DDPM utilisant U-net sont disponibles (voir par exemple <a class="reference external" href="https://paperswithcode.com/paper/denoising-diffusion-probabilistic-models">ce lien</a>). On propose ici l’implémentation d’un modèle plus simple (pour le réseau <span class="math notranslate nohighlight">\(\mathbf{\varepsilon}_\theta(\mathbf{x}_t, t)\)</span>), permettant la génération d’un nuage de points de forme donnée. Le réseau sera ici un simple perceptron multicouches.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_swiss_roll</span>
</pre></div>
</div>
<p>On génère une forme (ici un swiss roll)</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">X</span><span class="p">,</span><span class="n">_</span> <span class="o">=</span> <span class="n">make_swiss_roll</span><span class="p">(</span><span class="mi">1000</span><span class="p">,</span><span class="n">noise</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:,[</span><span class="mi">0</span><span class="p">,</span><span class="mi">2</span><span class="p">]]</span><span class="o">/</span><span class="mf">10.0</span>

<span class="n">data</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">T</span>

<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="o">*</span><span class="n">data</span><span class="p">);</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;off&#39;</span><span class="p">)</span>

<span class="n">dataset</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">X</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
</pre></div>
</div>
<figure class="align-default" id="curve">
<img alt="_images/curve.png" src="_images/curve.png" />
<figcaption>
<p><span class="caption-number">Fig. 72 </span><span class="caption-text">Forme du nuage de points <span class="math notranslate nohighlight">\(x_0\)</span></span><a class="headerlink" href="#curve" title="Lien vers cette image">#</a></p>
</figcaption>
</figure>
<p>On précalcule ensuite les constantes du modèle. Les <span class="math notranslate nohighlight">\(\beta_t\)</span> évoluent de manière linéaire.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">T</span> <span class="o">=</span> <span class="mi">150</span>

<span class="n">betas</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">6</span><span class="p">,</span><span class="mi">6</span><span class="p">,</span><span class="n">T</span><span class="p">)</span>
<span class="n">betas</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">betas</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="mf">0.5e-2</span> <span class="o">-</span> <span class="mf">1e-5</span><span class="p">)</span><span class="o">+</span><span class="mf">1e-5</span>

<span class="n">alphas</span> <span class="o">=</span> <span class="mi">1</span><span class="o">-</span><span class="n">betas</span>
<span class="n">alphas_prod</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cumprod</span><span class="p">(</span><span class="n">alphas</span><span class="p">,</span><span class="mi">0</span><span class="p">)</span>
<span class="n">alphas_prod_p</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">])</span><span class="o">.</span><span class="n">float</span><span class="p">(),</span><span class="n">alphas_prod</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]],</span><span class="mi">0</span><span class="p">)</span>
<span class="n">alphas_bar_sqrt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">alphas_prod</span><span class="p">)</span>
<span class="n">one_minus_alphas_bar_log</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">alphas_prod</span><span class="p">)</span>
<span class="n">one_minus_alphas_bar_sqrt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">alphas_prod</span><span class="p">)</span>
</pre></div>
</div>
<p>On calcule <span class="math notranslate nohighlight">\(x_t\)</span> à un temps quelconque donné.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">q_x</span><span class="p">(</span><span class="n">x_0</span><span class="p">,</span><span class="n">t</span><span class="p">):</span>
    <span class="n">noise</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn_like</span><span class="p">(</span><span class="n">x_0</span><span class="p">)</span>
    <span class="n">alphas_t</span> <span class="o">=</span> <span class="n">alphas_bar_sqrt</span><span class="p">[</span><span class="n">t</span><span class="p">]</span>
    <span class="n">alphas_1_m_t</span> <span class="o">=</span> <span class="n">one_minus_alphas_bar_sqrt</span><span class="p">[</span><span class="n">t</span><span class="p">]</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">alphas_t</span> <span class="o">*</span> <span class="n">x_0</span> <span class="o">+</span> <span class="n">alphas_1_m_t</span> <span class="o">*</span> <span class="n">noise</span><span class="p">)</span>

</pre></div>
</div>
<p>Le processus avant <span class="math notranslate nohighlight">\(q\)</span> est illustré sur la (<a class="reference internal" href="#q"><span class="std std-numref">Fig. 73</span></a>), à partir du code suivant :</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">nb_courbes</span> <span class="o">=</span> <span class="mi">20</span>
<span class="n">fig</span><span class="p">,</span><span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">5</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">nb_courbes</span><span class="p">):</span>
    <span class="n">j</span> <span class="o">=</span> <span class="n">i</span><span class="o">//</span><span class="mi">5</span>
    <span class="n">k</span> <span class="o">=</span> <span class="n">i</span><span class="o">%</span><span class="mi">5</span>
    <span class="n">n</span> <span class="o">=</span> <span class="n">i</span><span class="o">*</span><span class="n">T</span><span class="o">//</span><span class="n">nb_courbes</span>
    <span class="n">q_i</span> <span class="o">=</span> <span class="n">q_x</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">n</span><span class="p">]))</span>
    <span class="n">axs</span><span class="p">[</span><span class="n">j</span><span class="p">,</span><span class="n">k</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">q_i</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span><span class="n">q_i</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span><span class="n">s</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
    <span class="n">axs</span><span class="p">[</span><span class="n">j</span><span class="p">,</span><span class="n">k</span><span class="p">]</span><span class="o">.</span><span class="n">set_axis_off</span><span class="p">()</span>
    <span class="n">axs</span><span class="p">[</span><span class="n">j</span><span class="p">,</span><span class="n">k</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;$q(\mathbf</span><span class="si">{x}</span><span class="s1">_{&#39;</span><span class="o">+</span><span class="nb">str</span><span class="p">(</span><span class="n">n</span><span class="p">)</span><span class="o">+</span><span class="s1">&#39;})$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
<figure class="align-default" id="q">
<img alt="_images/q.png" src="_images/q.png" />
<figcaption>
<p><span class="caption-number">Fig. 73 </span><span class="caption-text">Application du processus avant <span class="math notranslate nohighlight">\(q\)</span> au cours du temps</span><a class="headerlink" href="#q" title="Lien vers cette image">#</a></p>
</figcaption>
</figure>
<p>On construit ensuite le processus de diffusion. Le réseau utilisé est un perceptron multicouches à trois couches cachées de <code class="docutils literal notranslate"><span class="pre">n_hidden</span></code>neurones et à activations ReLU.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">n_hidden</span> <span class="o">=</span> <span class="mi">64</span>

<span class="k">class</span> <span class="nc">DDPM</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">n_steps</span><span class="p">,</span><span class="n">n_hidden</span><span class="o">=</span><span class="mi">64</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">DDPM</span><span class="p">,</span><span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">linears</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">(</span>
            <span class="p">[</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="n">n_hidden</span><span class="p">),</span><span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">n_hidden</span><span class="p">,</span><span class="n">n_hidden</span><span class="p">),</span><span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">n_hidden</span><span class="p">,</span><span class="n">n_hidden</span><span class="p">),</span><span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">n_hidden</span><span class="p">,</span><span class="mi">2</span><span class="p">),</span>
            <span class="p">]</span>
        <span class="p">)</span>
        <span class="c1"># Gestion des pas de temps</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">step_embeddings</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">(</span>
            <span class="p">[</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">n_steps</span><span class="p">,</span><span class="n">n_hidden</span><span class="p">),</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">n_steps</span><span class="p">,</span><span class="n">n_hidden</span><span class="p">),</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">n_steps</span><span class="p">,</span><span class="n">n_hidden</span><span class="p">),</span>
            <span class="p">]</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">x</span><span class="p">,</span><span class="n">t</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">idx</span><span class="p">,</span><span class="n">embedding_layer</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">step_embeddings</span><span class="p">):</span>
            <span class="n">t_embedding</span> <span class="o">=</span> <span class="n">embedding_layer</span><span class="p">(</span><span class="n">t</span><span class="p">)</span>
            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linears</span><span class="p">[</span><span class="mi">2</span><span class="o">*</span><span class="n">idx</span><span class="p">](</span><span class="n">x</span><span class="p">)</span>
            <span class="n">x</span> <span class="o">+=</span> <span class="n">t_embedding</span>
            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linears</span><span class="p">[</span><span class="mi">2</span><span class="o">*</span><span class="n">idx</span><span class="o">+</span><span class="mi">1</span><span class="p">](</span><span class="n">x</span><span class="p">)</span>
            
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linears</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">](</span><span class="n">x</span><span class="p">)</span>
    
        <span class="k">return</span> <span class="n">x</span>
</pre></div>
</div>
<p>On écrit ensuite la fonction de perte du réseau</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">diffusion_loss</span><span class="p">(</span><span class="n">model</span><span class="p">,</span><span class="n">x_0</span><span class="p">,</span><span class="n">alphas_bar_sqrt</span><span class="p">,</span><span class="n">one_minus_alphas_bar_sqrt</span><span class="p">,</span><span class="n">n_steps</span><span class="p">):</span>
    <span class="n">batch_size</span> <span class="o">=</span> <span class="n">x_0</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    
    <span class="c1"># Généreration de temps  aléatoires pour un échantillon de taille batch_size</span>
    <span class="n">t</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">n_steps</span><span class="p">,</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">batch_size</span><span class="o">//</span><span class="mi">2</span><span class="p">,))</span>
    <span class="n">t</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">t</span><span class="p">,</span><span class="n">n_steps</span><span class="o">-</span><span class="mi">1</span><span class="o">-</span><span class="n">t</span><span class="p">],</span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">t</span> <span class="o">=</span> <span class="n">t</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    
    <span class="c1"># Coefficient de x_0</span>
    <span class="n">a</span> <span class="o">=</span> <span class="n">alphas_bar_sqrt</span><span class="p">[</span><span class="n">t</span><span class="p">]</span>
    
    <span class="c1"># Génération, d&#39;un bruit aléatoire</span>
    <span class="n">e</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn_like</span><span class="p">(</span><span class="n">x_0</span><span class="p">)</span>

    <span class="c1"># coefficient du bruit</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">one_minus_alphas_bar_sqrt</span><span class="p">[</span><span class="n">t</span><span class="p">]</span>
    
    <span class="c1">#réalisation à l&#39;instant t</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">x_0</span><span class="o">*</span><span class="n">a</span><span class="o">+</span><span class="n">e</span><span class="o">*</span><span class="n">b</span>
    
    <span class="c1"># Prédiction du bruit à l&#39;instant t</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">t</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>
    
    <span class="c1"># Erreur quadratique moyenne</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">e</span> <span class="o">-</span> <span class="n">output</span><span class="p">)</span><span class="o">.</span><span class="n">square</span><span class="p">()</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
</pre></div>
</div>
<p>On écrit ensuite la fonction d’échantillonnage du processus inverse</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># On récupère x[T], x[T-1], x[T-2]|... à partir de x[T]. . x[0]</span>
<span class="k">def</span> <span class="nf">p_sample_loop</span><span class="p">(</span><span class="n">model</span><span class="p">,</span><span class="n">shape</span><span class="p">,</span><span class="n">n_steps</span><span class="p">,</span><span class="n">betas</span><span class="p">,</span><span class="n">one_minus_alphas_bar_sqrt</span><span class="p">):</span>
    <span class="n">cur_x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span>
    <span class="n">x_seq</span> <span class="o">=</span> <span class="p">[</span><span class="n">cur_x</span><span class="p">]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">reversed</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">n_steps</span><span class="p">)):</span>
        <span class="n">cur_x</span> <span class="o">=</span> <span class="n">p_sample</span><span class="p">(</span><span class="n">model</span><span class="p">,</span><span class="n">cur_x</span><span class="p">,</span><span class="n">i</span><span class="p">,</span><span class="n">betas</span><span class="p">,</span><span class="n">one_minus_alphas_bar_sqrt</span><span class="p">)</span>
        <span class="n">x_seq</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">cur_x</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">x_seq</span>

<span class="c1"># On échantillonne la valeur reconstruite à l&#39;instant t à partir de x[T]</span>
<span class="k">def</span> <span class="nf">p_sample</span><span class="p">(</span><span class="n">model</span><span class="p">,</span><span class="n">x</span><span class="p">,</span><span class="n">t</span><span class="p">,</span><span class="n">betas</span><span class="p">,</span><span class="n">one_minus_alphas_bar_sqrt</span><span class="p">):</span>
    <span class="n">t</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">t</span><span class="p">])</span>
    
    <span class="n">coeff</span> <span class="o">=</span> <span class="n">betas</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="o">/</span> <span class="n">one_minus_alphas_bar_sqrt</span><span class="p">[</span><span class="n">t</span><span class="p">]</span>
    <span class="n">eps_theta</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">t</span><span class="p">)</span>
    <span class="n">mean</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">betas</span><span class="p">[</span><span class="n">t</span><span class="p">])</span><span class="o">.</span><span class="n">sqrt</span><span class="p">())</span><span class="o">*</span><span class="p">(</span><span class="n">x</span><span class="o">-</span><span class="p">(</span><span class="n">coeff</span><span class="o">*</span><span class="n">eps_theta</span><span class="p">))</span>
    
    <span class="n">z</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn_like</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">sigma_t</span> <span class="o">=</span> <span class="n">betas</span><span class="p">[</span><span class="n">t</span><span class="p">]</span><span class="o">.</span><span class="n">sqrt</span><span class="p">()</span>
    <span class="n">sample</span> <span class="o">=</span> <span class="n">mean</span> <span class="o">+</span> <span class="n">sigma_t</span> <span class="o">*</span> <span class="n">z</span>
    
    <span class="k">return</span> <span class="p">(</span><span class="n">sample</span><span class="p">)</span>
</pre></div>
</div>
<p>Et on entraîne enfin le modèle</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">batch_size</span> <span class="o">=</span> <span class="mi">128</span>
<span class="n">dataloader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span><span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span><span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">nb_epochs</span> <span class="o">=</span> <span class="mi">4000</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">DDPM</span><span class="p">(</span><span class="n">T</span><span class="p">,</span><span class="n">n_hidden</span><span class="p">)</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span><span class="n">lr</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">)</span>

<span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">nb_epochs</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">idx</span><span class="p">,</span><span class="n">batch_x</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">dataloader</span><span class="p">):</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">diffusion_loss</span><span class="p">(</span><span class="n">model</span><span class="p">,</span><span class="n">batch_x</span><span class="p">,</span><span class="n">alphas_bar_sqrt</span><span class="p">,</span><span class="n">one_minus_alphas_bar_sqrt</span><span class="p">,</span><span class="n">T</span><span class="p">)</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">clip_grad_norm_</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span><span class="mf">1.</span><span class="p">)</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">t</span><span class="o">%</span><span class="mi">100</span><span class="o">==</span><span class="mi">0</span><span class="p">:</span>
            <span class="n">x_seq</span> <span class="o">=</span> <span class="n">p_sample_loop</span><span class="p">(</span><span class="n">model</span><span class="p">,</span><span class="n">dataset</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span><span class="n">T</span><span class="p">,</span><span class="n">betas</span><span class="p">,</span><span class="n">one_minus_alphas_bar_sqrt</span><span class="p">)</span>
</pre></div>
</div>
<p>Les figures suivantes présentent l’action du  processus inverse au début (<a class="reference internal" href="#q0"><span class="std std-numref">Fig. 74</span></a>), au milieu (<a class="reference internal" href="#q1"><span class="std std-numref">Fig. 75</span></a>) et à la fin (<a class="reference internal" href="#q2"><span class="std std-numref">Fig. 76</span></a>) de l’entraînement.</p>
<figure class="align-default" id="q0">
<img alt="_images/curve_150_1.png" src="_images/curve_150_1.png" />
<figcaption>
<p><span class="caption-number">Fig. 74 </span><span class="caption-text">Processus inverse au début de l’entraînement</span><a class="headerlink" href="#q0" title="Lien vers cette image">#</a></p>
</figcaption>
</figure>
<figure class="align-default" id="q1">
<img alt="_images/curve_150_2.png" src="_images/curve_150_2.png" />
<figcaption>
<p><span class="caption-number">Fig. 75 </span><span class="caption-text">Processus inverse au milieu de l’entraînement</span><a class="headerlink" href="#q1" title="Lien vers cette image">#</a></p>
</figcaption>
</figure>
<figure class="align-default" id="q2">
<img alt="_images/curve_150_3.png" src="_images/curve_150_3.png" />
<figcaption>
<p><span class="caption-number">Fig. 76 </span><span class="caption-text">Processus inverse à la fin de l’entraînement</span><a class="headerlink" href="#q2" title="Lien vers cette image">#</a></p>
</figcaption>
</figure>
<p>On peut alors partir d’un bruit gaussien 2D et appliquer le processus inverse pour générer une réalisation des données d’entrée.</p>
<p><img alt="" src="_images/reversediffusion.gif" /></p>
<p>On peut également partir d’autres nuages de points</p>
<table class="table">
<thead>
<tr class="row-odd"><th class="head"><p><img alt="" src="_images/reversediffusioncircles.gif" /></p></th>
<th class="head"><p><img alt="" src="_images/reversediffusionmoons.gif" /></p></th>
</tr>
</thead>
</table>
<div class="docutils container" id="id6">
<div role="list" class="citation-list">
<div class="citation" id="id53" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>1<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id1">1</a>,<a role="doc-backlink" href="#id2">2</a>,<a role="doc-backlink" href="#id3">3</a>)</span>
<p>Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, <em>Advances in Neural Information Processing Systems</em>, volume 33, 6840–6851. Curran Associates, Inc., 2020.</p>
</div>
<div class="citation" id="id54" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>2<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id4">1</a>,<a role="doc-backlink" href="#id5">2</a>)</span>
<p>Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: convolutional networks for biomedical image segmentation. <em>CoRR</em>, 2015.</p>
</div>
</div>
</div>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="gan.html"
       title="page précédente">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">précédent</p>
        <p class="prev-next-title">Réseaux antagonistes générateurs</p>
      </div>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contenu
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction">Introduction</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#modele-de-diffusion">Modèle de diffusion</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#modele">Modèle</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#apprentissage">Apprentissage</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#reseau-de-neurones">Réseau de neurones</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#implementation">Implémentation</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
Par Vincent BARRA
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=8d27b9dea8ad943066ae"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=8d27b9dea8ad943066ae"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>