
<!DOCTYPE html>


<html lang="fr" data-content_root="./" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Mod√®les de diffusion &#8212; Apprentissage profond</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.5.1/css/all.min.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.1/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.1/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.1/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="_static/styles/sphinx-book-theme.css?v=384b581d" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="_static/proof.css?v=ca93fcec" />
    <link rel="stylesheet" type="text/css" href="_static/design-style.1e8bd061cd6da7fc9cf755528e8ffc24.min.css?v=0a3b3ea7" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=8d27b9dea8ad943066ae" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=8d27b9dea8ad943066ae" />
  <script src="_static/vendor/fontawesome/6.5.1/js/all.min.js?digest=8d27b9dea8ad943066ae"></script>

    <script src="_static/documentation_options.js?v=72dce1d2"></script>
    <script src="_static/doctools.js?v=9a2dae69"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="_static/copybutton.js?v=f281be69"></script>
    <script src="_static/scripts/sphinx-book-theme.js?v=efea14e4"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js?v=4a39c7ea"></script>
    <script src="_static/translations.js?v=bf059b8c"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js?v=36754332"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'diffusion';</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Recherche" href="search.html" />
    <link rel="prev" title="R√©seaux antagonistes g√©n√©rateurs" href="gan.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="fr"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a id="pst-skip-link" class="skip-link" href="#main-content">Passer au contenu principal</a>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Haut de page
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <header class="bd-header navbar navbar-expand-lg bd-navbar">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  

<a class="navbar-brand logo" href="intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="_static/logo.png" class="logo__image only-light" alt="Apprentissage profond - Home"/>
    <script>document.write(`<img src="_static/logo.png" class="logo__image only-dark" alt="Apprentissage profond - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn navbar-btn search-button-field search-button__button" title="Recherche" aria-label="Recherche" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Recherche</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Introduction</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="NN.html">Introduction aux r√©seaux de neurones</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Mod√®les classiques</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="perceptron.html">Perceptron</a></li>
<li class="toctree-l1"><a class="reference internal" href="PMC.html">Perceptrons multicouches</a></li>
<li class="toctree-l1"><a class="reference internal" href="cnn.html">R√©seaux convolutifs</a></li>
<li class="toctree-l1"><a class="reference internal" href="ae.html">Auto-encodeurs</a></li>
<li class="toctree-l1"><a class="reference internal" href="rnn.html">R√©seaux r√©currents</a></li>
<li class="toctree-l1"><a class="reference internal" href="transferLearning.html">Utilisation de r√©seaux existants</a></li>
<li class="toctree-l1"><a class="reference internal" href="transformers.html">Transformers</a></li>
<li class="toctree-l1"><a class="reference internal" href="gnn.html">Graph Neural Networks</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Mod√®les g√©n√©ratifs</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="vae.html">Autoencodeurs variationnels</a></li>
<li class="toctree-l1"><a class="reference internal" href="gan.html">R√©seaux antagonistes g√©n√©rateurs</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Mod√®les de diffusion</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">



<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Mode plein √©cran"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="clair/sombre" aria-label="clair/sombre" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Recherche" aria-label="Recherche" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Mod√®les de diffusion</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contenu </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction">Introduction</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#modele-de-diffusion">Mod√®le de diffusion</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#modele">Mod√®le</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#apprentissage">Apprentissage</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#reseau-de-neurones">R√©seau de neurones</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#implementation">Impl√©mentation</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="modeles-de-diffusion">
<h1>Mod√®les de diffusion<a class="headerlink" href="#modeles-de-diffusion" title="Lien vers cette rubrique">#</a></h1>
<section id="introduction">
<h2>Introduction<a class="headerlink" href="#introduction" title="Lien vers cette rubrique">#</a></h2>
<p>Un mod√®le de diffusion (de d√©bruitage) transforme du bruit √† partir d‚Äôune distribution simple en un √©chantillon de donn√©es. Le mod√®le se compose de deux processus (<a class="reference internal" href="#diffusion"><span class="std std-numref">Fig. 70</span></a>) :</p>
<ul class="simple">
<li><p>un processus de diffusion vers l‚Äôavant <span class="math notranslate nohighlight">\(q\)</span>, choisi, qui ajoute progressivement du bruit gaussien √† une donn√©e, jusqu‚Äô√† aboutir √† du bruit pur</p></li>
<li><p>un processus de diffusion inverse de d√©bruitage <span class="math notranslate nohighlight">\(p_{\boldsymbol\theta}\)</span> , mod√©lis√© par un r√©seau de neurones, entra√Æn√© √† d√©bruiter progressivement une donn√©e √† partir d‚Äôun bruit pur, jusqu‚Äô√† obtenir une r√©alisation d‚Äôune donn√©e r√©elle.</p></li>
</ul>
<p>Les processus sont temporels, index√©s par le temps <span class="math notranslate nohighlight">\(t\in[\![0,T]\!]\)</span>. A <span class="math notranslate nohighlight">\(t=0\)</span>, on  √©chantillonne une donn√©e r√©elle <span class="math notranslate nohighlight">\(\boldsymbol ùê±_0\)</span> de la distribution de donn√©es. Le processus <span class="math notranslate nohighlight">\(q\)</span> √©chantillonne un bruit provenant d‚Äôune distribution gaussienne √† chaque pas de temps <span class="math notranslate nohighlight">\(t\)</span> ,  ajout√© √† la donn√©e du pas de temps pr√©c√©dent. Si <span class="math notranslate nohighlight">\(T\)</span> est suffisamment et que les processus d‚Äôajout de bruit est correct, on obtient une distribution gaussienne isotrope √† <span class="math notranslate nohighlight">\(t=T\)</span>.</p>
<div class="proof remark dropdown admonition" id="remark-0">
<p class="admonition-title"><span class="caption-number">Remark 3 </span></p>
<section class="remark-content" id="proof-content">
<p>Une distribution gaussienne isotrope <span class="math notranslate nohighlight">\(\mathcal N(\boldsymbol \mu,\boldsymbol \Sigma)\)</span> est telle que <span class="math notranslate nohighlight">\(\boldsymbol \Sigma = \sigma^2\boldsymbol I\)</span>.</p>
</section>
</div><figure class="align-default" id="diffusion">
<img alt="_images/diffusion.png" src="_images/diffusion.png" />
<figcaption>
<p><span class="caption-number">Fig. 70 </span><span class="caption-text">Illustration du mod√®le de diffusion (source : <a class="reference external" href="https://proceedings.neurips.cc/paper_files/paper/2020/file/4c5bcfec8584af0d967f1ab10179ca4b-Paper.pdf"><span id="id1">[<a class="reference internal" href="#id53" title="Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, 6840‚Äì6851. Curran Associates, Inc., 2020.">1</a>]</span></a>)</span><a class="headerlink" href="#diffusion" title="Lien vers cette image">#</a></p>
</figcaption>
</figure>
</section>
<section id="modele-de-diffusion">
<h2>Mod√®le de diffusion<a class="headerlink" href="#modele-de-diffusion" title="Lien vers cette rubrique">#</a></h2>
<section id="modele">
<h3>Mod√®le<a class="headerlink" href="#modele" title="Lien vers cette rubrique">#</a></h3>
<p>Soit <span class="math notranslate nohighlight">\(q(\mathbf{x}_0)\)</span> la distribution des donn√©es r√©elles. On √©chantillonne <span class="math notranslate nohighlight">\(\mathbf{x}_0 \sim q(\mathbf{x}_0)\)</span> et on d√©finit un processus de diffusion avant <span class="math notranslate nohighlight">\(q(\mathbf{x}_t | \mathbf{x}_{t-1})\)</span> qui ajoute un bruit gaussien √† chaque pas de temps <span class="math notranslate nohighlight">\(t\in[\![1,T]\!]\)</span>, selon une mise √† jour de la variance connue  <span class="math notranslate nohighlight">\(0 &lt; \beta_1 &lt; \beta_2 &lt; ... &lt; \beta_T &lt; 1\)</span> :</p>
<div class="math notranslate nohighlight">
\[
q(\mathbf{x}_t | \mathbf{x}_{t-1}) = \mathcal{N}(\sqrt{1 - \beta_t} \mathbf{x}_{t-1}, \beta_t \mathbf{I})
\]</div>
<p>Chaque donn√©e <span class="math notranslate nohighlight">\(\boldsymbol x_t\)</span> est ainsi tir√©e selon une distribution conditionnelle gaussienne  <span class="math notranslate nohighlight">\(\mathbf{\mu}_t = \sqrt{1 - \beta_t} \mathbf{x}_{t-1}\)</span> et  <span class="math notranslate nohighlight">\(\sigma^2_t = \beta_t\)</span>,ce qui peut √™tre r√©alis√© en √©chantillonnant selon  <span class="math notranslate nohighlight">\(\mathbf{\varepsilon} \sim \mathcal{N}(\mathbf{0}, \mathbf{I})\)</span> et en posant  <span class="math notranslate nohighlight">\(\mathbf{x}_t = \sqrt{1 - \beta_t} \mathbf{x}_{t-1} +  \sqrt{\beta_t} \mathbf{\varepsilon}\)</span>.</p>
<p>En d√©finissant les <span class="math notranslate nohighlight">\(\beta_t\)</span> correctement, <span class="math notranslate nohighlight">\(\mathbf{x}_T\)</span> est un bruit gaussien.</p>
<p>Originellement, la croissance des <span class="math notranslate nohighlight">\(\beta_t\)</span> a √©t√© suppos√©e lin√©aire, de <span class="math notranslate nohighlight">\(\beta_1 = 10^{‚àí4}\)</span> √† <span class="math notranslate nohighlight">\(\beta_T = 0.02\)</span> <span id="id2">[<a class="reference internal" href="#id53" title="Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, 6840‚Äì6851. Curran Associates, Inc., 2020.">1</a>]</span>. Cependant, des auteurs ont montr√© qu‚Äôune mise √† jour des coefficients <span class="math notranslate nohighlight">\(\beta_t\)</span> √† l‚Äôaide d‚Äôun cosinus √©tait plus efficace :</p>
<div class="math notranslate nohighlight">
\[\beta_t = 1-\frac{\bar{\alpha_t}}{\bar{\alpha_t}-1}\textrm{ avec } \bar{\alpha_t} = \frac{f(t)}{f(0)}\textrm{o√π } f(t) = cos\left (\frac{\pi}{2}\frac{t/T+s}{1+s}\right ), s \textrm{ offset}\]</div>
<p>Si on connaissait la distribution conditionnelle  <span class="math notranslate nohighlight">\(p(\mathbf{x}_{t-1} | \mathbf{x}_t)\)</span>, alors on pourrait calculer le processus inverse en tirant <span class="math notranslate nohighlight">\(\mathbf{x}_T\)</span> selon une distribution gaussienne isotrope, et en ¬´¬†d√©bruitant¬†¬ª progressivement pour aboutir en <span class="math notranslate nohighlight">\(t=0\)</span> √† une r√©alisation <span class="math notranslate nohighlight">\(\mathbf{x}_0\)</span> de la distribution des donn√©es. Cependant, <span class="math notranslate nohighlight">\(p(\mathbf{x}_{t-1} | \mathbf{x}_t)\)</span> n‚Äôest pas accessible et on utilise un r√©seau de neurones <span class="math notranslate nohighlight">\(p_{\boldsymbol \theta} (\mathbf{x}_{t-1} | \mathbf{x}_t)\)</span> pour approcher cette distribution conditionnelle, o√π <span class="math notranslate nohighlight">\(\boldsymbol \theta\)</span> est l‚Äôensemble des param√®tres du r√©seau.</p>
<p>Si on suppose que le processus inverse est gaussien, alors on peut √©crire</p>
<div class="math notranslate nohighlight">
\[ p_{\boldsymbol \theta} (\mathbf{x}_{t-1} | \mathbf{x}_t) = \mathcal{N}(\mu_\theta(\mathbf{x}_{t},t), \Sigma_\theta (\mathbf{x}_{t},t))\]</div>
<p>Le r√©seau doit donc apprendre la moyenne et la variance, qui dependent du temps. Dans l‚Äôimpl√©mentation initiale <span id="id3">[<a class="reference internal" href="#id53" title="Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, 6840‚Äì6851. Curran Associates, Inc., 2020.">1</a>]</span>, les auteurs relaxent la contrainte de la variance (<span class="math notranslate nohighlight">\(\Sigma_\theta ( \mathbf{x}_t, t) = \sigma^2_t \mathbf{I}\)</span>), et apprennent uniquement la moyenne. Dans les impl√©mentations suivantes (par exemple <a class="reference external" href="https://openreview.net/pdf?id=-NEXDKk8gZ">celle-ci</a>), la contrainte a √©t√© prise en compte.</p>
<p>Dans la suite, on suppose seulement apprendre la moyenne.</p>
</section>
<section id="apprentissage">
<h3>Apprentissage<a class="headerlink" href="#apprentissage" title="Lien vers cette rubrique">#</a></h3>
<p>La combinaison de <span class="math notranslate nohighlight">\(q\)</span> et <span class="math notranslate nohighlight">\(p_{\boldsymbol\theta}\)</span> peut-√™tre vue comme un autoencodeur variationnel. Ainsi, ELBO peut √™tre utilis√©e pour minimiser la log-vraisemblance n√©gative par rapport √† <span class="math notranslate nohighlight">\(\mathbf{x}_0\)</span> . ELBO est la somme de fonctions de pertes calcul√©es √† chaque pas de temps</p>
<p><span class="math notranslate nohighlight">\(\ell  = \displaystyle\sum_{t=0}^T \ell_t\)</span>.</p>
<p>Par construction de <span class="math notranslate nohighlight">\(q\)</span> et <span class="math notranslate nohighlight">\(p_{\boldsymbol\theta}\)</span>, les <span class="math notranslate nohighlight">\(\ell_t,t\in[\![1,T]\!]\)</span> sont les divergences de Kullback-Leibler entre deux distributions gaussiennes, ce qui peut √™tre √©crit comme une perte <span class="math notranslate nohighlight">\(\ell_2\)</span> calcul√©e sur les moyennes de ces gaussiennes.</p>
<p>Par construction de <span class="math notranslate nohighlight">\(q\)</span>, puisque la somme de gaussiennes est √©galemment gaussienne, on peut √©chantillonner <span class="math notranslate nohighlight">\(\mathbf{x}_t\)</span> pour tout <span class="math notranslate nohighlight">\(t\)</span> conditionnellement √† <span class="math notranslate nohighlight">\(\mathbf{x}_0\)</span> : ainsi</p>
<div class="math notranslate nohighlight">
\[q(\mathbf{x}_t | \mathbf{x}_0) = \cal{N}(\sqrt{\bar{\alpha}_t} \mathbf{x}_0, (1- \bar{\alpha}_t) \mathbf{I})\]</div>
<p>avec  <span class="math notranslate nohighlight">\(\alpha_t = 1 - \beta_t\)</span> et  <span class="math notranslate nohighlight">\(\bar{\alpha_t} = \displaystyle\prod_{s=1}^{t} \alpha_s\)</span>.</p>
<p>Les <span class="math notranslate nohighlight">\(\bar{\alpha}_t\)</span> sont des fonctions des  <span class="math notranslate nohighlight">\(\beta_t\)</span>, permettant de mettre √† jour la variance. Ces derniers √©tant connus, les <span class="math notranslate nohighlight">\(\bar{\alpha}_t\)</span> le sont aussi et peuvent √™tre pr√©calcul√©s.
Ainsi, pendant l‚Äôentra√Ænement, on tire al√©atoirement <span class="math notranslate nohighlight">\(t\)</span> et on optimise <span class="math notranslate nohighlight">\(\ell_t\)</span></p>
<p>Il est √©galement possible de reparam√©triser la moyenne pour que le r√©seau de neurones apprenne le bruit ajout√© via un r√©seau <span class="math notranslate nohighlight">\(\mathbf{\varepsilon}_{\boldsymbol \theta}(\mathbf{x}_t, t)\)</span> pour un niveau de bruit <span class="math notranslate nohighlight">\(t\)</span> dans les divergences de Kullback-Leibler d√©finissant les <span class="math notranslate nohighlight">\(\ell_t\)</span>. Le r√©seau <span class="math notranslate nohighlight">\(p_{\boldsymbol \theta}\)</span> pr√©dit donc le bruit plut√¥t que la moyenne, qui peut ensuite √™tre calcul√©e par</p>
<div class="math notranslate nohighlight">
\[ \mathbf{\mu}_\theta(\mathbf{x}_t, t) = \frac{1}{\sqrt{\alpha_t}} \left(  \mathbf{x}_t - \frac{\beta_t}{\sqrt{1- \bar{\alpha}_t}} \mathbf{\varepsilon}_\theta(\mathbf{x}_t, t) \right)\]</div>
<p>La fonction objectif finale <span class="math notranslate nohighlight">\(\ell_t\)</span>, √† <span class="math notranslate nohighlight">\(t\)</span> choisi al√©atoirement et √©tant donn√© <span class="math notranslate nohighlight">\(\mathbf{\varepsilon} \sim \mathcal{N}(\mathbf{0}, \mathbf{I})\)</span> est alors :</p>
<div class="math notranslate nohighlight">
\[ \| \mathbf{\varepsilon} - \mathbf{\varepsilon}_\theta(\mathbf{x}_t, t) \|^2 = \| \mathbf{\varepsilon} - \mathbf{\varepsilon}_\theta( \sqrt{\bar{\alpha}_t} \mathbf{x}_0 + \sqrt{(1- \bar{\alpha}_t)  } \mathbf{\varepsilon}, t) \|^2.\]</div>
<p>avec <span class="math notranslate nohighlight">\(\mathbf{x}_0\)</span> la donn√©e initiale. <span class="math notranslate nohighlight">\(\mathbf{\varepsilon}\)</span> est le bruit √©chantillonn√© au temps <span class="math notranslate nohighlight">\(t\)</span> et <span class="math notranslate nohighlight">\(\mathbf{\varepsilon}_\theta (\mathbf{x}_t, t)\)</span> est le r√©seau de neurones, √©quip√© d‚Äôune fonction de perte quadratique entre le bruit r√©el et le bruit gaussien pr√©dit.</p>
<div class="proof algorithm admonition" id="algorithm-1">
<p class="admonition-title"><span class="caption-number">Algorithm 8 </span> (Algorithme d‚Äôapprentissage d‚Äôun mod√®le de diffusion)</p>
<section class="algorithm-content" id="proof-content">
<ol class="arabic simple">
<li><p>Tant que (non convergence)</p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(\boldsymbol x_0\sim q(\boldsymbol x_0)\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(t\sim Uniform(1,T)\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\varepsilon\sim\mathcal N(\boldsymbol 0,\boldsymbol I)\)</span></p></li>
<li><p>Calculer <span class="math notranslate nohighlight">\(\nabla_{\boldsymbol\theta}\|\mathbf\varepsilon - \mathbf{\varepsilon}_\theta(\sqrt{\bar{\alpha}_t} \mathbf{x}_0 + \sqrt{(1- \bar{\alpha}_t)  } \mathbf{\varepsilon}, t) \|^2\)</span></p></li>
<li><p>Mettre √† jour <span class="math notranslate nohighlight">\(\boldsymbol\theta\)</span> par un pas de descente de gradient</p></li>
</ol>
</li>
</ol>
</section>
</div><p>Plut√¥t qu‚Äôun seul exemple <span class="math notranslate nohighlight">\(x_0\)</span>, le r√©seau est classiquement entra√Æn√© sur un batch.</p>
</section>
<section id="reseau-de-neurones">
<h3>R√©seau de neurones<a class="headerlink" href="#reseau-de-neurones" title="Lien vers cette rubrique">#</a></h3>
<p>Le r√©seau de neurones <span class="math notranslate nohighlight">\(\mathbf{\varepsilon}_\theta(\mathbf{x}_t, t)\)</span> utilis√© dans <a class="reference external" href="https://proceedings.neurips.cc/paper_files/paper/2020/file/4c5bcfec8584af0d967f1ab10179ca4b-Paper.pdf"><span id="id4">[<a class="reference internal" href="#id54" title="Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: convolutional networks for biomedical image segmentation. CoRR, 2015.">2</a>]</span></a> traite des images et les auteurs utilisent le r√©seau <a class="reference external" href="https://arxiv.org/abs/1505.04597">U-net</a> (<a class="reference internal" href="#unet"><span class="std std-numref">Fig. 71</span></a>)</p>
<figure class="align-default" id="unet">
<img alt="_images/unet.png" src="_images/unet.png" />
<figcaption>
<p><span class="caption-number">Fig. 71 </span><span class="caption-text">Architecture du mod√®le U-net (source : <a class="reference external" href="https://proceedings.neurips.cc/paper_files/paper/2020/file/4c5bcfec8584af0d967f1ab10179ca4b-Paper.pdf"><span id="id5">[<a class="reference internal" href="#id54" title="Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: convolutional networks for biomedical image segmentation. CoRR, 2015.">2</a>]</span></a>)</span><a class="headerlink" href="#unet" title="Lien vers cette image">#</a></p>
</figcaption>
</figure>
<p>U-Net est une architecture pour la segmentation s√©mantique. Il se compose d‚Äôun chemin de contraction et d‚Äôun chemin d‚Äôexpansion. Le chemin de contraction suit l‚Äôarchitecture typique d‚Äôun r√©seau convolutif. Il consiste en l‚Äôapplication r√©p√©t√©e de deux convolutions 3<span class="math notranslate nohighlight">\(\times\)</span>3 , chacune suivie d‚Äôune activation ReLU et d‚Äôune op√©ration d‚Äôagr√©gation max 2<span class="math notranslate nohighlight">\(\times\)</span>2 avec un stride √©gal √† 2 pour le sous-√©chantillonnage. √Ä chaque √©tape de sous-√©chantillonnage, le nombre de canaux de caract√©ristiques est doubl√©. Chaque √©tape du chemin d‚Äôexpansion consiste en un sur√©chantillonnage de la carte de caract√©ristiques suivi d‚Äôune convolution 2<span class="math notranslate nohighlight">\(\times\)</span>2  qui divise par deux le nombre de canaux de caract√©ristiques, une concat√©nation avec la carte de caract√©ristiques recadr√©e correspondante du chemin de contraction, et deux convolutions 3<span class="math notranslate nohighlight">\(\times\)</span>3, chacune suivie d‚Äôune ReLU. Le recadrage est n√©cessaire en raison de la perte de pixels de bordure dans chaque convolution. Sur la derni√®re couche, une convolution 1<span class="math notranslate nohighlight">\(\times\)</span>1 est utilis√©e pour faire correspondre chaque vecteur de caract√©ristiques √† 64 composantes au nombre de classes souhait√©. Au total, le r√©seau comporte 23 couches de convolution.</p>
</section>
</section>
<section id="implementation">
<h2>Impl√©mentation<a class="headerlink" href="#implementation" title="Lien vers cette rubrique">#</a></h2>
<p>De nombreuses impl√©mentations du mod√®le DDPM utilisant U-net sont disponibles (voir par exemple <a class="reference external" href="https://paperswithcode.com/paper/denoising-diffusion-probabilistic-models">ce lien</a>). On propose ici l‚Äôimpl√©mentation d‚Äôun mod√®le plus simple (pour le r√©seau <span class="math notranslate nohighlight">\(\mathbf{\varepsilon}_\theta(\mathbf{x}_t, t)\)</span>), permettant la g√©n√©ration d‚Äôun nuage de points de forme donn√©e. Le r√©seau sera ici un simple perceptron multicouches.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_swiss_roll</span>
</pre></div>
</div>
<p>On g√©n√®re une forme (ici un swiss roll)</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">X</span><span class="p">,</span><span class="n">_</span> <span class="o">=</span> <span class="n">make_swiss_roll</span><span class="p">(</span><span class="mi">1000</span><span class="p">,</span><span class="n">noise</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:,[</span><span class="mi">0</span><span class="p">,</span><span class="mi">2</span><span class="p">]]</span><span class="o">/</span><span class="mf">10.0</span>

<span class="n">data</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">T</span>

<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="o">*</span><span class="n">data</span><span class="p">);</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;off&#39;</span><span class="p">)</span>

<span class="n">dataset</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">X</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
</pre></div>
</div>
<figure class="align-default" id="curve">
<img alt="_images/curve.png" src="_images/curve.png" />
<figcaption>
<p><span class="caption-number">Fig. 72 </span><span class="caption-text">Forme du nuage de points <span class="math notranslate nohighlight">\(x_0\)</span></span><a class="headerlink" href="#curve" title="Lien vers cette image">#</a></p>
</figcaption>
</figure>
<p>On pr√©calcule ensuite les constantes du mod√®le. Les <span class="math notranslate nohighlight">\(\beta_t\)</span> √©voluent de mani√®re lin√©aire.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">T</span> <span class="o">=</span> <span class="mi">150</span>

<span class="n">betas</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">6</span><span class="p">,</span><span class="mi">6</span><span class="p">,</span><span class="n">T</span><span class="p">)</span>
<span class="n">betas</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">betas</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="mf">0.5e-2</span> <span class="o">-</span> <span class="mf">1e-5</span><span class="p">)</span><span class="o">+</span><span class="mf">1e-5</span>

<span class="n">alphas</span> <span class="o">=</span> <span class="mi">1</span><span class="o">-</span><span class="n">betas</span>
<span class="n">alphas_prod</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cumprod</span><span class="p">(</span><span class="n">alphas</span><span class="p">,</span><span class="mi">0</span><span class="p">)</span>
<span class="n">alphas_prod_p</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">])</span><span class="o">.</span><span class="n">float</span><span class="p">(),</span><span class="n">alphas_prod</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]],</span><span class="mi">0</span><span class="p">)</span>
<span class="n">alphas_bar_sqrt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">alphas_prod</span><span class="p">)</span>
<span class="n">one_minus_alphas_bar_log</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">alphas_prod</span><span class="p">)</span>
<span class="n">one_minus_alphas_bar_sqrt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">alphas_prod</span><span class="p">)</span>
</pre></div>
</div>
<p>On calcule <span class="math notranslate nohighlight">\(x_t\)</span> √† un temps quelconque donn√©.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">q_x</span><span class="p">(</span><span class="n">x_0</span><span class="p">,</span><span class="n">t</span><span class="p">):</span>
    <span class="n">noise</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn_like</span><span class="p">(</span><span class="n">x_0</span><span class="p">)</span>
    <span class="n">alphas_t</span> <span class="o">=</span> <span class="n">alphas_bar_sqrt</span><span class="p">[</span><span class="n">t</span><span class="p">]</span>
    <span class="n">alphas_1_m_t</span> <span class="o">=</span> <span class="n">one_minus_alphas_bar_sqrt</span><span class="p">[</span><span class="n">t</span><span class="p">]</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">alphas_t</span> <span class="o">*</span> <span class="n">x_0</span> <span class="o">+</span> <span class="n">alphas_1_m_t</span> <span class="o">*</span> <span class="n">noise</span><span class="p">)</span>

</pre></div>
</div>
<p>Le processus avant <span class="math notranslate nohighlight">\(q\)</span> est illustr√© sur la (<a class="reference internal" href="#q"><span class="std std-numref">Fig. 73</span></a>), √† partir du code suivant :</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">nb_courbes</span> <span class="o">=</span> <span class="mi">20</span>
<span class="n">fig</span><span class="p">,</span><span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">5</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">nb_courbes</span><span class="p">):</span>
    <span class="n">j</span> <span class="o">=</span> <span class="n">i</span><span class="o">//</span><span class="mi">5</span>
    <span class="n">k</span> <span class="o">=</span> <span class="n">i</span><span class="o">%</span><span class="mi">5</span>
    <span class="n">n</span> <span class="o">=</span> <span class="n">i</span><span class="o">*</span><span class="n">T</span><span class="o">//</span><span class="n">nb_courbes</span>
    <span class="n">q_i</span> <span class="o">=</span> <span class="n">q_x</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">n</span><span class="p">]))</span>
    <span class="n">axs</span><span class="p">[</span><span class="n">j</span><span class="p">,</span><span class="n">k</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">q_i</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span><span class="n">q_i</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span><span class="n">s</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
    <span class="n">axs</span><span class="p">[</span><span class="n">j</span><span class="p">,</span><span class="n">k</span><span class="p">]</span><span class="o">.</span><span class="n">set_axis_off</span><span class="p">()</span>
    <span class="n">axs</span><span class="p">[</span><span class="n">j</span><span class="p">,</span><span class="n">k</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;$q(\mathbf</span><span class="si">{x}</span><span class="s1">_{&#39;</span><span class="o">+</span><span class="nb">str</span><span class="p">(</span><span class="n">n</span><span class="p">)</span><span class="o">+</span><span class="s1">&#39;})$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
<figure class="align-default" id="q">
<img alt="_images/q.png" src="_images/q.png" />
<figcaption>
<p><span class="caption-number">Fig. 73 </span><span class="caption-text">Application du processus avant <span class="math notranslate nohighlight">\(q\)</span> au cours du temps</span><a class="headerlink" href="#q" title="Lien vers cette image">#</a></p>
</figcaption>
</figure>
<p>On construit ensuite le processus de diffusion. Le r√©seau utilis√© est un perceptron multicouches √† trois couches cach√©es de <code class="docutils literal notranslate"><span class="pre">n_hidden</span></code>neurones et √† activations ReLU.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">n_hidden</span> <span class="o">=</span> <span class="mi">64</span>

<span class="k">class</span> <span class="nc">DDPM</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">n_steps</span><span class="p">,</span><span class="n">n_hidden</span><span class="o">=</span><span class="mi">64</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">DDPM</span><span class="p">,</span><span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">linears</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">(</span>
            <span class="p">[</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="n">n_hidden</span><span class="p">),</span><span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">n_hidden</span><span class="p">,</span><span class="n">n_hidden</span><span class="p">),</span><span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">n_hidden</span><span class="p">,</span><span class="n">n_hidden</span><span class="p">),</span><span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">n_hidden</span><span class="p">,</span><span class="mi">2</span><span class="p">),</span>
            <span class="p">]</span>
        <span class="p">)</span>
        <span class="c1"># Gestion des pas de temps</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">step_embeddings</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">(</span>
            <span class="p">[</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">n_steps</span><span class="p">,</span><span class="n">n_hidden</span><span class="p">),</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">n_steps</span><span class="p">,</span><span class="n">n_hidden</span><span class="p">),</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">n_steps</span><span class="p">,</span><span class="n">n_hidden</span><span class="p">),</span>
            <span class="p">]</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">x</span><span class="p">,</span><span class="n">t</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">idx</span><span class="p">,</span><span class="n">embedding_layer</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">step_embeddings</span><span class="p">):</span>
            <span class="n">t_embedding</span> <span class="o">=</span> <span class="n">embedding_layer</span><span class="p">(</span><span class="n">t</span><span class="p">)</span>
            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linears</span><span class="p">[</span><span class="mi">2</span><span class="o">*</span><span class="n">idx</span><span class="p">](</span><span class="n">x</span><span class="p">)</span>
            <span class="n">x</span> <span class="o">+=</span> <span class="n">t_embedding</span>
            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linears</span><span class="p">[</span><span class="mi">2</span><span class="o">*</span><span class="n">idx</span><span class="o">+</span><span class="mi">1</span><span class="p">](</span><span class="n">x</span><span class="p">)</span>
            
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linears</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">](</span><span class="n">x</span><span class="p">)</span>
    
        <span class="k">return</span> <span class="n">x</span>
</pre></div>
</div>
<p>On √©crit ensuite la fonction de perte du r√©seau</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">diffusion_loss</span><span class="p">(</span><span class="n">model</span><span class="p">,</span><span class="n">x_0</span><span class="p">,</span><span class="n">alphas_bar_sqrt</span><span class="p">,</span><span class="n">one_minus_alphas_bar_sqrt</span><span class="p">,</span><span class="n">n_steps</span><span class="p">):</span>
    <span class="n">batch_size</span> <span class="o">=</span> <span class="n">x_0</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    
    <span class="c1"># G√©n√©reration de temps  al√©atoires pour un √©chantillon de taille batch_size</span>
    <span class="n">t</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">n_steps</span><span class="p">,</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">batch_size</span><span class="o">//</span><span class="mi">2</span><span class="p">,))</span>
    <span class="n">t</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">t</span><span class="p">,</span><span class="n">n_steps</span><span class="o">-</span><span class="mi">1</span><span class="o">-</span><span class="n">t</span><span class="p">],</span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">t</span> <span class="o">=</span> <span class="n">t</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    
    <span class="c1"># Coefficient de x_0</span>
    <span class="n">a</span> <span class="o">=</span> <span class="n">alphas_bar_sqrt</span><span class="p">[</span><span class="n">t</span><span class="p">]</span>
    
    <span class="c1"># G√©n√©ration, d&#39;un bruit al√©atoire</span>
    <span class="n">e</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn_like</span><span class="p">(</span><span class="n">x_0</span><span class="p">)</span>

    <span class="c1"># coefficient du bruit</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">one_minus_alphas_bar_sqrt</span><span class="p">[</span><span class="n">t</span><span class="p">]</span>
    
    <span class="c1">#r√©alisation √† l&#39;instant t</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">x_0</span><span class="o">*</span><span class="n">a</span><span class="o">+</span><span class="n">e</span><span class="o">*</span><span class="n">b</span>
    
    <span class="c1"># Pr√©diction du bruit √† l&#39;instant t</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">t</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>
    
    <span class="c1"># Erreur quadratique moyenne</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">e</span> <span class="o">-</span> <span class="n">output</span><span class="p">)</span><span class="o">.</span><span class="n">square</span><span class="p">()</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
</pre></div>
</div>
<p>On √©crit ensuite la fonction d‚Äô√©chantillonnage du processus inverse</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># On r√©cup√®re x[T], x[T-1], x[T-2]|... √† partir de x[T]. . x[0]</span>
<span class="k">def</span> <span class="nf">p_sample_loop</span><span class="p">(</span><span class="n">model</span><span class="p">,</span><span class="n">shape</span><span class="p">,</span><span class="n">n_steps</span><span class="p">,</span><span class="n">betas</span><span class="p">,</span><span class="n">one_minus_alphas_bar_sqrt</span><span class="p">):</span>
    <span class="n">cur_x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span>
    <span class="n">x_seq</span> <span class="o">=</span> <span class="p">[</span><span class="n">cur_x</span><span class="p">]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">reversed</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">n_steps</span><span class="p">)):</span>
        <span class="n">cur_x</span> <span class="o">=</span> <span class="n">p_sample</span><span class="p">(</span><span class="n">model</span><span class="p">,</span><span class="n">cur_x</span><span class="p">,</span><span class="n">i</span><span class="p">,</span><span class="n">betas</span><span class="p">,</span><span class="n">one_minus_alphas_bar_sqrt</span><span class="p">)</span>
        <span class="n">x_seq</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">cur_x</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">x_seq</span>

<span class="c1"># On √©chantillonne la valeur reconstruite √† l&#39;instant t √† partir de x[T]</span>
<span class="k">def</span> <span class="nf">p_sample</span><span class="p">(</span><span class="n">model</span><span class="p">,</span><span class="n">x</span><span class="p">,</span><span class="n">t</span><span class="p">,</span><span class="n">betas</span><span class="p">,</span><span class="n">one_minus_alphas_bar_sqrt</span><span class="p">):</span>
    <span class="n">t</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">t</span><span class="p">])</span>
    
    <span class="n">coeff</span> <span class="o">=</span> <span class="n">betas</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="o">/</span> <span class="n">one_minus_alphas_bar_sqrt</span><span class="p">[</span><span class="n">t</span><span class="p">]</span>
    <span class="n">eps_theta</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">t</span><span class="p">)</span>
    <span class="n">mean</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">betas</span><span class="p">[</span><span class="n">t</span><span class="p">])</span><span class="o">.</span><span class="n">sqrt</span><span class="p">())</span><span class="o">*</span><span class="p">(</span><span class="n">x</span><span class="o">-</span><span class="p">(</span><span class="n">coeff</span><span class="o">*</span><span class="n">eps_theta</span><span class="p">))</span>
    
    <span class="n">z</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn_like</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">sigma_t</span> <span class="o">=</span> <span class="n">betas</span><span class="p">[</span><span class="n">t</span><span class="p">]</span><span class="o">.</span><span class="n">sqrt</span><span class="p">()</span>
    <span class="n">sample</span> <span class="o">=</span> <span class="n">mean</span> <span class="o">+</span> <span class="n">sigma_t</span> <span class="o">*</span> <span class="n">z</span>
    
    <span class="k">return</span> <span class="p">(</span><span class="n">sample</span><span class="p">)</span>
</pre></div>
</div>
<p>Et on entra√Æne enfin le mod√®le</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">batch_size</span> <span class="o">=</span> <span class="mi">128</span>
<span class="n">dataloader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span><span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span><span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">nb_epochs</span> <span class="o">=</span> <span class="mi">4000</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">DDPM</span><span class="p">(</span><span class="n">T</span><span class="p">,</span><span class="n">n_hidden</span><span class="p">)</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span><span class="n">lr</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">)</span>

<span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">nb_epochs</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">idx</span><span class="p">,</span><span class="n">batch_x</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">dataloader</span><span class="p">):</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">diffusion_loss</span><span class="p">(</span><span class="n">model</span><span class="p">,</span><span class="n">batch_x</span><span class="p">,</span><span class="n">alphas_bar_sqrt</span><span class="p">,</span><span class="n">one_minus_alphas_bar_sqrt</span><span class="p">,</span><span class="n">T</span><span class="p">)</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">clip_grad_norm_</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span><span class="mf">1.</span><span class="p">)</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">t</span><span class="o">%</span><span class="mi">100</span><span class="o">==</span><span class="mi">0</span><span class="p">:</span>
            <span class="n">x_seq</span> <span class="o">=</span> <span class="n">p_sample_loop</span><span class="p">(</span><span class="n">model</span><span class="p">,</span><span class="n">dataset</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span><span class="n">T</span><span class="p">,</span><span class="n">betas</span><span class="p">,</span><span class="n">one_minus_alphas_bar_sqrt</span><span class="p">)</span>
</pre></div>
</div>
<p>Les figures suivantes pr√©sentent l‚Äôaction du  processus inverse au d√©but (<a class="reference internal" href="#q0"><span class="std std-numref">Fig. 74</span></a>), au milieu (<a class="reference internal" href="#q1"><span class="std std-numref">Fig. 75</span></a>) et √† la fin (<a class="reference internal" href="#q2"><span class="std std-numref">Fig. 76</span></a>) de l‚Äôentra√Ænement.</p>
<figure class="align-default" id="q0">
<img alt="_images/curve_150_1.png" src="_images/curve_150_1.png" />
<figcaption>
<p><span class="caption-number">Fig. 74 </span><span class="caption-text">Processus inverse au d√©but de l‚Äôentra√Ænement</span><a class="headerlink" href="#q0" title="Lien vers cette image">#</a></p>
</figcaption>
</figure>
<figure class="align-default" id="q1">
<img alt="_images/curve_150_2.png" src="_images/curve_150_2.png" />
<figcaption>
<p><span class="caption-number">Fig. 75 </span><span class="caption-text">Processus inverse au milieu de l‚Äôentra√Ænement</span><a class="headerlink" href="#q1" title="Lien vers cette image">#</a></p>
</figcaption>
</figure>
<figure class="align-default" id="q2">
<img alt="_images/curve_150_3.png" src="_images/curve_150_3.png" />
<figcaption>
<p><span class="caption-number">Fig. 76 </span><span class="caption-text">Processus inverse √† la fin de l‚Äôentra√Ænement</span><a class="headerlink" href="#q2" title="Lien vers cette image">#</a></p>
</figcaption>
</figure>
<p>On peut alors partir d‚Äôun bruit gaussien 2D et appliquer le processus inverse pour g√©n√©rer une r√©alisation des donn√©es d‚Äôentr√©e.</p>
<p><img alt="" src="_images/reversediffusion.gif" /></p>
<p>On peut √©galement partir d‚Äôautres nuages de points</p>
<table class="table">
<thead>
<tr class="row-odd"><th class="head"><p><img alt="" src="_images/reversediffusioncircles.gif" /></p></th>
<th class="head"><p><img alt="" src="_images/reversediffusionmoons.gif" /></p></th>
</tr>
</thead>
</table>
<div class="docutils container" id="id6">
<div role="list" class="citation-list">
<div class="citation" id="id53" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>1<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id1">1</a>,<a role="doc-backlink" href="#id2">2</a>,<a role="doc-backlink" href="#id3">3</a>)</span>
<p>Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In H.¬†Larochelle, M.¬†Ranzato, R.¬†Hadsell, M.F. Balcan, and H.¬†Lin, editors, <em>Advances in Neural Information Processing Systems</em>, volume¬†33, 6840‚Äì6851. Curran Associates, Inc., 2020.</p>
</div>
<div class="citation" id="id54" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>2<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id4">1</a>,<a role="doc-backlink" href="#id5">2</a>)</span>
<p>Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: convolutional networks for biomedical image segmentation. <em>CoRR</em>, 2015.</p>
</div>
</div>
</div>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="gan.html"
       title="page pr√©c√©dente">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">pr√©c√©dent</p>
        <p class="prev-next-title">R√©seaux antagonistes g√©n√©rateurs</p>
      </div>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contenu
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction">Introduction</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#modele-de-diffusion">Mod√®le de diffusion</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#modele">Mod√®le</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#apprentissage">Apprentissage</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#reseau-de-neurones">R√©seau de neurones</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#implementation">Impl√©mentation</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
Par Vincent BARRA
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      ¬© Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=8d27b9dea8ad943066ae"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=8d27b9dea8ad943066ae"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>