
<!DOCTYPE html>


<html lang="fr" data-content_root="./" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Auto-encodeurs &#8212; Apprentissage profond</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.5.1/css/all.min.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.1/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.1/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.1/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="_static/styles/sphinx-book-theme.css?v=384b581d" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="_static/proof.css?v=ca93fcec" />
    <link rel="stylesheet" type="text/css" href="_static/design-style.1e8bd061cd6da7fc9cf755528e8ffc24.min.css?v=0a3b3ea7" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=8d27b9dea8ad943066ae" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=8d27b9dea8ad943066ae" />
  <script src="_static/vendor/fontawesome/6.5.1/js/all.min.js?digest=8d27b9dea8ad943066ae"></script>

    <script src="_static/documentation_options.js?v=72dce1d2"></script>
    <script src="_static/doctools.js?v=9a2dae69"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="_static/copybutton.js?v=f281be69"></script>
    <script src="_static/scripts/sphinx-book-theme.js?v=efea14e4"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js?v=4a39c7ea"></script>
    <script src="_static/translations.js?v=bf059b8c"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js?v=36754332"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'ae';</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Recherche" href="search.html" />
    <link rel="next" title="Réseaux récurrents" href="rnn.html" />
    <link rel="prev" title="Réseaux convolutifs" href="cnn.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="fr"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a id="pst-skip-link" class="skip-link" href="#main-content">Passer au contenu principal</a>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Haut de page
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <header class="bd-header navbar navbar-expand-lg bd-navbar">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  

<a class="navbar-brand logo" href="intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="_static/logo.png" class="logo__image only-light" alt="Apprentissage profond - Home"/>
    <script>document.write(`<img src="_static/logo.png" class="logo__image only-dark" alt="Apprentissage profond - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn navbar-btn search-button-field search-button__button" title="Recherche" aria-label="Recherche" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Recherche</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Introduction</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="NN.html">Introduction aux réseaux de neurones</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Modèles classiques</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="perceptron.html">Perceptron</a></li>
<li class="toctree-l1"><a class="reference internal" href="PMC.html">Perceptrons multicouches</a></li>
<li class="toctree-l1"><a class="reference internal" href="cnn.html">Réseaux convolutifs</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Auto-encodeurs</a></li>
<li class="toctree-l1"><a class="reference internal" href="rnn.html">Réseaux récurrents</a></li>
<li class="toctree-l1"><a class="reference internal" href="transferLearning.html">Utilisation de réseaux existants</a></li>
<li class="toctree-l1"><a class="reference internal" href="transformers.html">Transformers</a></li>
<li class="toctree-l1"><a class="reference internal" href="gnn.html">Graph Neural Networks</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Modèles génératifs</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="vae.html">Autoencodeurs variationnels</a></li>
<li class="toctree-l1"><a class="reference internal" href="gan.html">Réseaux antagonistes générateurs</a></li>
<li class="toctree-l1"><a class="reference internal" href="diffusion.html">Modèles de diffusion</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">



<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Mode plein écran"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="clair/sombre" aria-label="clair/sombre" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Recherche" aria-label="Recherche" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Auto-encodeurs</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contenu </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction">Introduction</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#influence-de-la-taille-de-l-espace-de-codage">Influence de la taille de l’espace de codage</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#le-cas-mathcal-h-mathcal-x">Le cas <span class="math notranslate nohighlight">\(|\mathcal{H}|&lt;|\mathcal{X}|\)</span></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#le-cas-mathcal-h-geq-mathcal-x">Le cas <span class="math notranslate nohighlight">\(|\mathcal{H}|\geq|\mathcal{X}|\)</span></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#autoencodeurs-regularises">Autoencodeurs régularisés</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#autoencodeurs-parcimonieux">Autoencodeurs parcimonieux</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#autoencodeurs-contractifs">Autoencodeurs contractifs</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#autoencodeurs-de-debruitage">Autoencodeurs de débruitage</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#autoencodeurs-variationnels">Autoencodeurs variationnels</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#implementation">Implémentation</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="auto-encodeurs">
<h1>Auto-encodeurs<a class="headerlink" href="#auto-encodeurs" title="Lien vers cette rubrique">#</a></h1>
<section id="introduction">
<h2>Introduction<a class="headerlink" href="#introduction" title="Lien vers cette rubrique">#</a></h2>
<p>Un autoencodeur est algorithme entraîné de manière non supervisée à
reproduire son entrée <span class="math notranslate nohighlight">\(\mathbf{x}\in \mathcal{X}\)</span>. Il peut être vu
(<a class="reference internal" href="#ae1"><span class="std std-numref">Fig. 21</span></a>) comme
composé de deux parties : un <em>encodeur</em> <code class="docutils literal notranslate"><span class="pre">E</span></code> qui transforme <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>
en un code déterministe
<span class="math notranslate nohighlight">\(\mathbf{h} = f(\mathbf{x} ; \mathbf{w_E})\in \mathcal{H}\)</span> ou une
distribution
<span class="math notranslate nohighlight">\(\textbf p_{encodeur} (\mathbf{h}|\mathbf{x},\mathbf{w_E})\)</span>,
qui représente l’entrée ; et un <em>décodeur</em> <code class="docutils literal notranslate"><span class="pre">D</span></code> qui produit une
reconstruction déterministe
<span class="math notranslate nohighlight">\(\mathbf{\hat{x}} = g(\mathbf{h} ; \mathbf{w_D})\)</span> de <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> ou une
distribution
<span class="math notranslate nohighlight">\(\textbf p_{decodeur} (\mathbf{x}|\mathbf{h},\mathbf{w_D})\)</span>.
Les vecteurs <span class="math notranslate nohighlight">\(\mathbf{w_E}\)</span> et <span class="math notranslate nohighlight">\(\mathbf{w_D}\)</span> sont les paramètres de <code class="docutils literal notranslate"><span class="pre">E</span></code>
et <code class="docutils literal notranslate"><span class="pre">D</span></code>. Le plus souvent, l’encodeur et le décodeur sont des réseaux de
neurones (perceptrons multicouches plus ou moins profonds, réseaux
convolutifs ou récurrents,…) et les paramètres sont donc les poids de ces réseaux. À ce
titre, l’entraînement peut être réalisé avec les mêmes algorithmes que
ceux utilisés dans les réseaux de neurones classiques.<br />
Entraîner un autoencodeur à reconstruire
<span class="math notranslate nohighlight">\(g\circ f(\mathbf{x})=\mathbf{x}\)</span> pour tout <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> n’est pas utile
(on apprend l’identité). On contraint donc le réseau à ne pas reproduire
parfaitement l’entrée, et à ne s’intéresser qu’à certains aspects de la
reconstruction, ce qui lui permet d’apprendre des propriétés utilies des
données.</p>
<figure class="align-default" id="ae1">
<img alt="_images/ae1.png" src="_images/ae1.png" />
<figcaption>
<p><span class="caption-number">Fig. 21 </span><span class="caption-text">Architecture générale d’un autoencodeur</span><a class="headerlink" href="#ae1" title="Lien vers cette image">#</a></p>
</figcaption>
</figure>
</section>
<section id="influence-de-la-taille-de-l-espace-de-codage">
<h2>Influence de la taille de l’espace de codage<a class="headerlink" href="#influence-de-la-taille-de-l-espace-de-codage" title="Lien vers cette rubrique">#</a></h2>
<section id="le-cas-mathcal-h-mathcal-x">
<h3>Le cas <span class="math notranslate nohighlight">\(|\mathcal{H}|&lt;|\mathcal{X}|\)</span><a class="headerlink" href="#le-cas-mathcal-h-mathcal-x" title="Lien vers cette rubrique">#</a></h3>
<p>Lorsque la dimension du code <span class="math notranslate nohighlight">\(\mathbf{h}\)</span> est inférieure à celle de
<span class="math notranslate nohighlight">\(\mathbf{x}\)</span>, l’encodeur <code class="docutils literal notranslate"><span class="pre">E</span></code> apprend à réduire la dimension. Le
décodeur, une fois appris, permet de créer une donnée dans <span class="math notranslate nohighlight">\(\mathcal{X}\)</span>
à partir d’un point de <span class="math notranslate nohighlight">\(\mathcal{H}\)</span> : il agit donc comme un modèle
génératif.<br />
L’apprentissage (la recherche des valeurs de <span class="math notranslate nohighlight">\(\mathbf{w_E}\)</span> et
<span class="math notranslate nohighlight">\(\mathbf{w_D}\)</span>) s’effectue par minimisation d’une fonction de perte</p>
<div class="math notranslate nohighlight">
\[\ell(\mathbf{x} , g\left [f(\mathbf{x} ; \mathbf{w_E}); \mathbf{w_D})\right ]\]</div>
<p>Si <span class="math notranslate nohighlight">\(g\)</span> est linéaire et <span class="math notranslate nohighlight">\(\ell\)</span> est la fonction de perte quadratique,
alors l’autoencodeur agit comme l’analyse en composantes principales.
Dans le cas plus général, l’autoencodeur apprend une représentation plus
complexe des données. Il faut cependant prendre garde à ce que <span class="math notranslate nohighlight">\(f\)</span> et
<span class="math notranslate nohighlight">\(g\)</span> ne soient pas trop complexes, auquel cas l’autoencodeur ne saura
faire que copier exactement l’entrée, sans extraire dans <span class="math notranslate nohighlight">\(\mathcal{H}\)</span>
d’information utile sur les données.</p>
<p>L’espace <span class="math notranslate nohighlight">\(\mathcal{H}\)</span> peut être utilisé pour de la visualisation en
dimension réduite des données, pour des tâches de classification, ou
plus simplement pour un espace de représentation plus compact des
données de <span class="math notranslate nohighlight">\(\mathcal{X}\)</span> (<a class="reference internal" href="#ae2"><span class="std std-numref">Fig. 22</span></a>).</p>
<figure class="align-default" id="ae2">
<img alt="_images/ae2.png" src="_images/ae2.png" />
<figcaption>
<p><span class="caption-number">Fig. 22 </span><span class="caption-text">Utilisation d’un autoencodeur pour la compression et la génération de
chiffres manuscrits. Les images MNIST (28<span class="math notranslate nohighlight">\(\times\)</span>28,
ligne du haut) sont encodées par un simple perceptron multicouche à
activation sigmoïde et une seule couche cachée de taille 36. Le code est
visualisé (ligne du milieu) sous la forme d’images
6<span class="math notranslate nohighlight">\(\times\)</span>6. Le décodeur produit des images
reconstruites (ligne du bas) à partir de ce
code</span><a class="headerlink" href="#ae2" title="Lien vers cette image">#</a></p>
</figcaption>
</figure>
</section>
<section id="le-cas-mathcal-h-geq-mathcal-x">
<h3>Le cas <span class="math notranslate nohighlight">\(|\mathcal{H}|\geq|\mathcal{X}|\)</span><a class="headerlink" href="#le-cas-mathcal-h-geq-mathcal-x" title="Lien vers cette rubrique">#</a></h3>
<p>Si la taille de l’espace de représentation <span class="math notranslate nohighlight">\(\mathcal{H}\)</span> est supérieure
à celle de l’espace d’entrée, on comprend assez facilement qu’il est
très aisé pour l’autoencodeur d’apprendre l’identité, sans extraire
d’information utile des données initiales (il suffit de propager
<span class="math notranslate nohighlight">\(\mathcal{X}\)</span> dans <span class="math notranslate nohighlight">\(\mathcal{H}\)</span>). Il est donc nécessaire de contraindre
le modèle.</p>
</section>
</section>
<section id="autoencodeurs-regularises">
<h2>Autoencodeurs régularisés<a class="headerlink" href="#autoencodeurs-regularises" title="Lien vers cette rubrique">#</a></h2>
<p>Régulariser un autoencodeur permet d’entraîner efficacement
l’algorithme, en choisissant de plus la dimension de <span class="math notranslate nohighlight">\(\mathcal H\)</span> et la
complexité de <span class="math notranslate nohighlight">\(f\)</span> et <span class="math notranslate nohighlight">\(g\)</span> en fonction de la complexité de la distribution
à modéliser. Plutôt que de limiter la capacité du modèle (en imposant
par exemple que <code class="docutils literal notranslate"><span class="pre">E</span></code> et <code class="docutils literal notranslate"><span class="pre">D</span></code> soient des réseaux multicouches à faible
profondeur et/ou que <span class="math notranslate nohighlight">\(\mathcal H\)</span> soit de faible dimension), la
régularisation construit une fonction de perte qui encourage
l’autoencodeur à avoir des propriétés supplémentaires, en plus de celle
de reproduire son entrée.</p>
<p>Un autoencodeur régularisé minimise la fonction</p>
<div class="math notranslate nohighlight">
\[\ell(\mathbf{x} , g\left [f(\mathbf{x} ; \mathbf{w_E}); \mathbf{w_D})\right ] + \beta \Omega(\mathbf{h})\]</div>
<p>où <span class="math notranslate nohighlight">\(\Omega(\mathbf{h})\)</span> est un terme de pénalisation permettant de
contraindre les paramètres du modèle et <span class="math notranslate nohighlight">\(\beta\in\mathbb{R}\)</span> contrôle le
poids du terme de pénalité dans l’optimisation.</p>
<section id="autoencodeurs-parcimonieux">
<h3>Autoencodeurs parcimonieux<a class="headerlink" href="#autoencodeurs-parcimonieux" title="Lien vers cette rubrique">#</a></h3>
<p>Les autoencodeurs parcimonieux (ou épars) sont typiquement utilisés pour
apprendre des caractéristiques pertinentes des données d’entrée, qui
sont ensuite utilisées comme entrées d’algorithmes de classification ou
de régression.</p>
<p>Supposons que <code class="docutils literal notranslate"><span class="pre">E</span></code> et <code class="docutils literal notranslate"><span class="pre">D</span></code> soient des perceptrons multicouches. Il est
alors par exemple possible d’imposer aux neurones d’être « inactifs » la
plupart du temps, en définissant l’inactivité comme une valeur de sortie
du neurone proche de zéro (pour une sigmoïde, ou -1 pour une tangente
hyperbolique). Pour cela, on dispose de <span class="math notranslate nohighlight">\(m\)</span> exemples
<span class="math notranslate nohighlight">\(\mathcal{S} =  \{\mathbf{x_1}\cdots \mathbf{x_m}\}\)</span>. On note
<span class="math notranslate nohighlight">\(y^{(l)}_j(\mathbf{x})\)</span> l’activation du neurone caché <span class="math notranslate nohighlight">\(j\)</span> de la couche
<span class="math notranslate nohighlight">\(l\)</span> lorsque l’entrée <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> est présentée au réseau. On note
également</p>
<div class="math notranslate nohighlight">
\[\hat\rho_j = \frac{1}{m} \sum_{i=1}^m \left[ y^{(l)}_j(\mathbf{x_i}) \right]\]</div>
<p>l’activation moyenne du neurone caché <span class="math notranslate nohighlight">\(j\)</span> sur présentation de
<span class="math notranslate nohighlight">\(\mathcal{S}\)</span>. L’objectif est alors d’imposer <span class="math notranslate nohighlight">\(\hat\rho_j = \rho\)</span>, où
<span class="math notranslate nohighlight">\(\rho\)</span> est une valeur proche de zéro (ainsi l’activation moyenne de
chaque neurone caché doit être faible), par l’intermédiaire d’une
définition adaptée de <span class="math notranslate nohighlight">\(\Omega\)</span>. De nombreux choix sont possibles. Par
exemple pour un réseau à une couche cachée :</p>
<div class="math notranslate nohighlight">
\[\Omega(\mathbf{h}) = \displaystyle\sum_{j=1}^{n^{(2)}} \rho \log \frac{\rho}{\hat\rho_j} + (1-\rho) \log \frac{1-\rho}{1-\hat\rho_j} = \displaystyle\sum_{j=1}^{n^{(2)}} {KL}(\rho || \hat\rho_j),\]</div>
<p>où <span class="math notranslate nohighlight">\({KL}(\rho || \hat\rho_j)\)</span> est la divergence de Kullback-Leibler (KL)
entre une variable aléatoire de loi de Bernoulli de moyenne <span class="math notranslate nohighlight">\(\rho\)</span> et
une variable aléatoire de loi de Bernoulli de moyenne <span class="math notranslate nohighlight">\(\hat\rho_j\)</span>.<br />
On peut alors montrer que <span class="math notranslate nohighlight">\({KL}(\rho || \hat\rho_j) = 0\)</span> si
<span class="math notranslate nohighlight">\(\hat\rho_j = \rho\)</span>, et <span class="math notranslate nohighlight">\({KL}\)</span> croît de façon monotone lorsque
<span class="math notranslate nohighlight">\(\hat\rho_j\)</span> s’éloigne de <span class="math notranslate nohighlight">\(\rho\)</span>.</p>
<p>Le calcul des dérivées partielles et la descente de gradient changent
peu pour l’algorithme d’optimisation. Il faut cependant connaître au
préalable les <span class="math notranslate nohighlight">\(\textstyle \hat\rho_j\)</span> et donc faire dans un premier
temps une propagation avant sur tous les exemples de <span class="math notranslate nohighlight">\(\mathcal{S}\)</span>
permettant de calculer les activations moyennes.<br />
Dans le cas où la base d’apprentissage est suffisamment petite, elle
tient entièrement en mémoire et les activations peuvent être stockées
pour calculer <span class="math notranslate nohighlight">\(\textstyle \hat\rho_j\)</span>. Les activations stockées peuvent
alors être utilisées dans l’étape de rétropropagation sur l’ensemble des
exemples.<br />
Dans le cas contraire, le calcul de <span class="math notranslate nohighlight">\(\textstyle \hat\rho_j\)</span> peut être
fait en accumulant les activations calculées exemple par exemple, mais
sans sauvegarder les valeurs de ces activations. Une seconde propagation
sur chaque exemple sera alors nécessaire pour permettre la
rétropropagation.</p>
<p>Les autoencodeurs parcimonieux peuvent également être
vus d’un point de vue probabiliste comme des algorithmes maximisant la
vraisemblance maximale d’un modèle génératif à variables latentes
<span class="math notranslate nohighlight">\(\mathbf{h}\)</span>. Supposons disposer d’une distribution jointe explicite</p>
<div class="math notranslate nohighlight">
\[\textbf{p}_{modele}(\mathbf{x},\mathbf{h}) = \textbf{p}_{modele}( \mathbf{h})\textbf{p}_{modele}(\mathbf{x}|\mathbf{h})\]</div>
<p>La log vraisemblance peut alors s’écrire</p>
<div class="math notranslate nohighlight">
\[log(\textbf{p}_{modele}(\mathbf{x})) = log \left (\displaystyle\sum_{\mathbf{h}}  \textbf{p}_{modele}(\mathbf{x},\mathbf{h})\right )\]</div>
<p>L’autoencodeur approche cette somme juste pour une une valeur de
<span class="math notranslate nohighlight">\(\mathbf{h}\)</span> fortement probable. Pour cette valeur, on maximise alors
$<span class="math notranslate nohighlight">\(log(\textbf{p}_{modele}(\mathbf{x},\mathbf{h})) = log(\textbf{p}_{modele}(\mathbf{h})) + log(\textbf{p}_{modele}(\mathbf{x}|\mathbf{h}))\)</span><span class="math notranslate nohighlight">\(
et \)</span>log(\textbf{p}_{modele}(\mathbf{h}))$ peut être
utilisée pour introduire de la parcimonie.</p>
<p>Par exemple si
<span class="math notranslate nohighlight">\(\textbf{p}_{modele}(h_i) = \frac{\lambda}{2} e^{-\beta|h_i| }\)</span>
(Laplace prior), alors</p>
<div class="math notranslate nohighlight">
\[-log(\textbf{p}_{modele}(\mathbf{h})) = \displaystyle\sum_{i=1}^{|\mathcal{H}|}\left (\lambda |h_i| -log\frac{\lambda}{2} \right) = \Omega(\mathbf{h})  + c\]</div>
<p>et l’on retrouve une régularisation <span class="math notranslate nohighlight">\(\ell_1\)</span> (méthode Lasso).</p>
</section>
<section id="autoencodeurs-contractifs">
<h3>Autoencodeurs contractifs<a class="headerlink" href="#autoencodeurs-contractifs" title="Lien vers cette rubrique">#</a></h3>
<p>Une autre stratégie de régularisation consiste à faire dépendre <span class="math notranslate nohighlight">\(\Omega\)</span>
du gradient du code en fonction des entrées :</p>
<div class="math notranslate nohighlight">
\[\Omega(\mathbf{x},\mathbf{h}) = \displaystyle\sum_{i=1}^{|\mathcal{H}|}\|\nabla_{\mathbf{x}} h_i\|^2 =\left  \|\frac{\partial f(\mathbf{x},\mathbf{w_E})}{\partial \mathbf{x}}\right \|_F^2\]</div>
<p>où <span class="math notranslate nohighlight">\(\|.\|_F\)</span> est la norme de Frobenius. Le modèle apprend alors une
fonction qui change peu lorsque <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> varie peu. Puisque la
pénalité n’est appliquée que sur les exemples de <span class="math notranslate nohighlight">\(\mathcal{S}\)</span>, les
informations capturées dans le code concernent la distribution des
données d’entraînement, et plus précisément la variété sur laquelle
vivent les données de <span class="math notranslate nohighlight">\(\mathcal{S}\)</span>. En ce sens, ces autoencodeurs sont
à rapprocher des méthodes de <em>Manifold Learning</em>.</p>
</section>
<section id="autoencodeurs-de-debruitage">
<h3>Autoencodeurs de débruitage<a class="headerlink" href="#autoencodeurs-de-debruitage" title="Lien vers cette rubrique">#</a></h3>
<p>Plutôt que d’ajouter un terme à la fonction de perte, on peut
directement changer cette dernière pour apprendre des caractéristiques
utiles des données.<br />
Un autoencodeur de débruitage considère la fonction de perte</p>
<div class="math notranslate nohighlight">
\[\ell(\mathbf{x} , g\left [f(\mathbf{\tilde{x}} ; \mathbf{w_E}); \mathbf{w_D})\right ]\]</div>
<p>où <span class="math notranslate nohighlight">\(\mathbf{\tilde{x}}\)</span> est une version de <span class="math notranslate nohighlight">\(\mathbf{{x}}\)</span> bruitée par
une distribution conditionnelle <span class="math notranslate nohighlight">\(C(\mathbf{\tilde{x}},\mathbf{x})\)</span>.
L’autoencodeur apprend alors une distribution de reconstruction
<span class="math notranslate nohighlight">\(\textbf{p}_R(\mathbf{x}\mid \mathbf{\tilde{x}})\)</span>
selon l”<a class="reference internal" href="#AEB">Algorithm 7</a> (<a class="reference internal" href="#ae3"><span class="std std-numref">Fig. 23</span></a>).</p>
<div class="proof algorithm admonition" id="AEB">
<p class="admonition-title"><span class="caption-number">Algorithm 7 </span> (Algorithe d’apprentissage d’un autoencodeur de débruitage)</p>
<section class="algorithm-content" id="proof-content">
<p><strong>Entrée :</strong>  <span class="math notranslate nohighlight">\(\mathcal{S}\)</span>, <span class="math notranslate nohighlight">\(C(\mathbf{\tilde{x}},\mathbf{x})\)</span>, un autoencodeur <span class="math notranslate nohighlight">\((f,g)\)</span></p>
<p><strong>Sortie :</strong> Un autoencodeur de débruitage</p>
<ol class="arabic simple">
<li><p>Tant que (non stop)</p>
<ol class="arabic simple">
<li><p>Tirer un exemple <span class="math notranslate nohighlight">\(\mathbf x\)</span> de <span class="math notranslate nohighlight">\(\mathcal{S}\)</span></p></li>
<li><p>Tirer <span class="math notranslate nohighlight">\(\mathbf {\tilde{x}}\)</span> selon <span class="math notranslate nohighlight">\(C(\mathbf{\tilde{x}},\mathbf{x})\)</span></p></li>
<li><p>Estimer <span class="math notranslate nohighlight">\(\textbf{p}_R(\mathbf{x}\mid \mathbf{\tilde{x}}) = \textbf{p}_{decodeur}(\mathbf{x}\mid \mathbf{h},\mathbf{w_D})=g(\mathbf{h},\mathbf{w_D})\)</span> où <span class="math notranslate nohighlight">\(\mathbf{h} = f(\mathbf{\tilde{x}},\mathbf{w_E})\)</span></p></li>
</ol>
</li>
</ol>
</section>
</div><p>L’apprentissage peut être vu comme une descente de gradient stochastique
de</p>
<div class="math notranslate nohighlight">
\[-\mathbb{E}_{\mathbf{x}\sim \textbf{p}_{\mathcal{S}}(\mathbf{x})} \mathbb{E}_{\mathbf{\tilde{x}}\sim C(\mathbf{\tilde{x}},\mathbf{x})} \left (log\;\textbf{p}_{decodeur} (\mathbf{x}|\mathbf{h}=f(\mathbf{\tilde{x}},\mathbf{w_E}),\mathbf{w_D})\right )\]</div>
<figure class="align-default" id="ae3">
<img alt="_images/ae3.png" src="_images/ae3.png" />
<figcaption>
<p><span class="caption-number">Fig. 23 </span><span class="caption-text">Autoencodeur de débruitage sur les données MNIST. Les images
<span class="math notranslate nohighlight">\(\mathbf{x}\)</span> (ligne du haut) sont corrompues par un bruit gaussien
centré de variance unité (deuxième ligne). Un autoencodeur de débruitage
est ensuite entraîné. Le code <span class="math notranslate nohighlight">\(\mathbf{h}\)</span> de taille 32 est visualisé
(troisième ligne) sous la forme d’images 8<span class="math notranslate nohighlight">\(\times\)</span>4. Le
décodeur produit les images débruitées de la dernière
ligne.</span><a class="headerlink" href="#ae3" title="Lien vers cette image">#</a></p>
</figcaption>
</figure>
</section>
</section>
<section id="autoencodeurs-variationnels">
<h2>Autoencodeurs variationnels<a class="headerlink" href="#autoencodeurs-variationnels" title="Lien vers cette rubrique">#</a></h2>
<p>Le dernier modèle d’autoencodeurs que nous abordons sera traité plus précisément dans un chapitre dédié, dans la partie consacrée aux modèles génératifs.</p>
<p>Les autoencodeurs variationnels (VAE) <span id="id1">[<a class="reference internal" href="#id18" title="D. Kingma and M. Welling. Auto-encoding variational bayes. CoRR, 2013. URL: http://dblp.uni-trier.de/db/journals/corr/corr1312.html#KingmaW13.">1</a>]</span> sont des modèles
génératifs. Ce ne sont pas à proprement parler des autoencodeurs tels
que nous les avons abordés dans les paragraphes précédents, ils
empruntent juste une architecture similaire (<a class="reference internal" href="#ae4"><span class="std std-numref">Fig. 24</span></a>), d’où leur nom.</p>
<figure class="align-default" id="ae4">
<img alt="_images/ae4.png" src="_images/ae4.png" />
<figcaption>
<p><span class="caption-number">Fig. 24 </span><span class="caption-text">Architecture générale d’un autoencodeur
variationnel.</span><a class="headerlink" href="#ae4" title="Lien vers cette image">#</a></p>
</figcaption>
</figure>
<p>Au lieu d’apprendre <span class="math notranslate nohighlight">\(f(.,\mathbf{w_E})\)</span> et <span class="math notranslate nohighlight">\(g(.,\mathbf{w_D})\)</span>, un
autoencodeur variationnel apprend des distributions de probabilité
<span class="math notranslate nohighlight">\(\textbf{p}_{encodeur} (\mathbf{h}|\mathbf{x},\mathbf{w_E})\)</span>
et
<span class="math notranslate nohighlight">\(\textbf{p}_{decodeur} (\mathbf{x}|\mathbf{h},\mathbf{w_D})\)</span>.<br />
Apprendre des distributions plutôt que des fonctions déterministes
présente plusieurs avantages, et notamment :</p>
<ul class="simple">
<li><p>les données d’entrée peuvent être bruitées, et un modèle de
distribution <span class="math notranslate nohighlight">\(\textbf{p}_\mathbf{x}\)</span> peut être
plus utile</p></li>
<li><p>il est possible d’utiliser
<span class="math notranslate nohighlight">\(\textbf{p}_{decodeur} (\mathbf{x}|\mathbf{h},\mathbf{w_D})\)</span>
pour échantillonner <span class="math notranslate nohighlight">\(\mathbf{h}\)</span> puis <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>, et donc de
générer des données ayant des statistiques similaires aux éléments
de <span class="math notranslate nohighlight">\(\mathcal{S}\)</span> (<a class="reference internal" href="#ae5"><span class="std std-numref">Fig. 25</span></a>).</p></li>
</ul>
<p>Abordons ces autoencodeurs sous l’angle des modèles génératifs.
Supposons que nous voulions générer des points suivant la distribution
<span class="math notranslate nohighlight">\(\textbf{p}_\mathbf{x}\)</span>. Plutôt que d’inférer
directement sur cette distribution, nous pouvons utiliser des <em>variables
latentes</em> (le code des autoencodeurs). Les modèles à variables latentes
font l’hypothèse que les données <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> sont issues d’une variable
non observée <span class="math notranslate nohighlight">\(\mathbf{h}\)</span>. S’il peut être difficile de modéliser
directement <span class="math notranslate nohighlight">\(\textbf{p}_\mathbf{x}\)</span>, il peut être plus
facile de choisir<em>a priori</em> une distribution
<span class="math notranslate nohighlight">\(\textbf{p}_\mathbf{h}\)</span> et chercher à modéliser
<span class="math notranslate nohighlight">\(\textbf{p}_{\mathbf{x}|\mathbf{h}}\)</span>.<br />
Pour générer <span class="math notranslate nohighlight">\(\mathbf{x}\sim \textbf{p}_\mathbf{x}\)</span>,
un autoencodeur variationnel tire donc tout d’abord
<span class="math notranslate nohighlight">\(\mathbf{h}\sim \textbf{p}_\mathbf{h}\)</span>. <span class="math notranslate nohighlight">\(\mathbf{h}\)</span>
est ensuite passé à un réseau de neurones et <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> est finalement
tiré selon
<span class="math notranslate nohighlight">\(\textbf{p}_{decodeur} (\mathbf{x}|\mathbf{h},\mathbf{w_D})\)</span>.
L’entraînement est réalisé en maximisant la borne inférieure
variationnelle :</p>
<div class="math notranslate nohighlight">
\[\mathcal{L}(q) = \mathbb{E}_{\mathbf{h}\sim q(\mathbf{h}|\mathbf{x})} log \left (\textbf{p}_{decodeur} (\mathbf{x}|\mathbf{h},\mathbf{w_D})\right ) -KL\left (q(\mathbf{h}|\mathbf{x}) ||\textbf{p}_\mathbf{h}\right )\]</div>
<p>où <span class="math notranslate nohighlight">\(KL\)</span> est la divergence de Kullback Leibler déjà rencontrée dans les
autoencodeurs parcimonieux. Le premier terme de <span class="math notranslate nohighlight">\(\mathcal{L}(q)\)</span> est la
log vraisemblance de la reconstruction trouvée dans les autoencodeurs
classiques, tandis que le second terme tend à rapprocher la distribution
<em>a posteriori</em> <span class="math notranslate nohighlight">\(q(\mathbf{h}|\mathbf{x})\)</span> et le modèle <em>a priori</em>
<span class="math notranslate nohighlight">\(\textbf{p}_\mathbf{h}\)</span>. Dans les techniques
classiques d’inférence, <span class="math notranslate nohighlight">\(q\)</span> est approché par optimisation. Dans les
autoencodeurs variationnels, on entraîne un encodeur paramétrique (un
réseau de neurones paramétré par <span class="math notranslate nohighlight">\(\mathbf{w_E}\)</span>) qui produit les
paramètres de <span class="math notranslate nohighlight">\(q\)</span>. Tant que <span class="math notranslate nohighlight">\(\mathbf{h}\)</span> est continue, il est donc
possible de rétropropager à travers les tirages de <span class="math notranslate nohighlight">\(\mathbf{h}\)</span>
effectués selon
<span class="math notranslate nohighlight">\(q(\mathbf{h}|\mathbf{x})  = q(\mathbf{h}| f(\mathbf{x} ; \mathbf{w_E}))\)</span>
pour obtenir le gradient par rapport à <span class="math notranslate nohighlight">\(\mathbf{w_E}\)</span>. L’apprentissage
consiste alors simplement à maximiser <span class="math notranslate nohighlight">\(\mathcal{L}\)</span> par rapport à
<span class="math notranslate nohighlight">\((\mathbf{w_E},\mathbf{w_D})\)</span>.</p>
<p>Il est courant de choisir comme prior
<span class="math notranslate nohighlight">\(\textbf{p}_\mathbf{h}\)</span> une loi normale centrée
réduite <span class="math notranslate nohighlight">\(\mathcal{N}(\mathbf{0},\mathbf{I})\)</span>. Cette simplicité apparente
ne réduit pas le pouvoir d’expression du modèle si l’effort est fait sur
l’optimisation de la distribution
<span class="math notranslate nohighlight">\(\textbf{p}_{decodeur} (\mathbf{x}|\mathbf{h},\mathbf{w_D})\)</span>.
L’encodeur <code class="docutils literal notranslate"><span class="pre">E</span></code> est alors un réseau de neurones générant des paramètres
de distribution de <span class="math notranslate nohighlight">\(q\)</span> dans <span class="math notranslate nohighlight">\(\mathcal{H}\)</span>, soit un vecteur de moyenne
<span class="math notranslate nohighlight">\(\mu\)</span> et une matrice de covariance <span class="math notranslate nohighlight">\(\mathbf{\Sigma}\)</span>.<br />
Notons enfin que la rétropropagation du gradient nécessite une astuce de
calcul dans <span class="math notranslate nohighlight">\(\mathcal{H}\)</span>, dite astuce de reparamétrisation : la
génération de
<span class="math notranslate nohighlight">\(\mathbf{h}\sim \textbf{p}_{encodeur} (\mathbf{h}|\mathbf{x},\mathbf{w_E})\)</span>
se fait effectivement en tirant une variable aléatoire
<span class="math notranslate nohighlight">\(\epsilon\sim\mathcal{N}(\mathbf{0},\mathbf{I})\)</span>, puis en calculant
<span class="math notranslate nohighlight">\(\mathbf{h}\)</span>=<span class="math notranslate nohighlight">\(\mu\)</span> + <span class="math notranslate nohighlight">\(\Sigma^{1/2} \epsilon\)</span>. L’échantillonnage se fait
alors seulement pour <span class="math notranslate nohighlight">\(\epsilon\)</span>, qui n’a pas besoin d’être rétropropagé.</p>
<figure class="align-default" id="ae5">
<img alt="_images/ae5.png" src="_images/ae5.png" />
<figcaption>
<p><span class="caption-number">Fig. 25 </span><span class="caption-text">Visualisation de l’espace latent <span class="math notranslate nohighlight">\(\mathcal H = \mathbb{R}^2\)</span> appris par un autoencodeur variationnel sur les donneés MNIST. Pour chaque valeur <span class="math notranslate nohighlight">\(\mathbf h_i\)</span> discrétisée sur <span class="math notranslate nohighlight">\(\mathcal H\)</span> est affichée une image <span class="math notranslate nohighlight">\(\mathbf x ∼ \textbf{p}_{decodeur}(\mathbf{x}\mid \mathbf h_i,\mathbf{w_D})\)</span>. Les chiffres de la même classe sont groupés dans cet espace, et les axes de <span class="math notranslate nohighlight">\(\mathcal H\)</span> ont une interprétation (l’axe horizontal semble souligner le caractère « penché » des chiffres).</span><a class="headerlink" href="#ae5" title="Lien vers cette image">#</a></p>
</figcaption>
</figure>
</section>
<section id="implementation">
<h2>Implémentation<a class="headerlink" href="#implementation" title="Lien vers cette rubrique">#</a></h2>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">torch.optim</span> <span class="k">as</span> <span class="nn">optim</span>
<span class="kn">from</span> <span class="nn">torchvision</span> <span class="kn">import</span> <span class="n">datasets</span><span class="p">,</span> <span class="n">transforms</span>
<span class="kn">from</span> <span class="nn">torchvision.utils</span> <span class="kn">import</span> <span class="n">make_grid</span>
</pre></div>
</div>
<p>On travaille sur les données MNIST</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># taille des batchs</span>
<span class="n">train_batch_size</span><span class="o">=</span><span class="mi">128</span>
<span class="n">test_batch_size</span> <span class="o">=</span> <span class="mi">128</span>

<span class="c1"># Paramètres du réseau et de l&#39;apprentissage</span>
<span class="n">lr</span> <span class="o">=</span> <span class="mf">0.001</span>
<span class="n">num_epochs</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">num_hidden_1</span> <span class="o">=</span> <span class="mi">256</span>  
<span class="n">num_hidden_2</span> <span class="o">=</span> <span class="mi">128</span>  
<span class="n">num_input</span> <span class="o">=</span> <span class="mi">784</span>  

<span class="n">transform</span><span class="o">=</span><span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span>
        <span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(),</span>
        <span class="n">transforms</span><span class="o">.</span><span class="n">Normalize</span><span class="p">((</span><span class="mf">0.1307</span><span class="p">,),</span> <span class="p">(</span><span class="mf">0.3081</span><span class="p">,))</span>
        <span class="p">])</span>
<span class="n">dataset1</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">MNIST</span><span class="p">(</span><span class="s1">&#39;../data&#39;</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">download</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                       <span class="n">transform</span><span class="o">=</span><span class="n">transform</span><span class="p">)</span>
<span class="n">dataset2</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">MNIST</span><span class="p">(</span><span class="s1">&#39;../data&#39;</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                       <span class="n">transform</span><span class="o">=</span><span class="n">transform</span><span class="p">)</span>
<span class="n">train_kwargs</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;batch_size&#39;</span><span class="p">:</span> <span class="n">train_batch_size</span><span class="p">}</span>
<span class="n">test_kwargs</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;batch_size&#39;</span><span class="p">:</span> <span class="n">test_batch_size</span><span class="p">}</span>
<span class="n">train_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">dataset1</span><span class="p">,</span><span class="o">**</span><span class="n">train_kwargs</span><span class="p">)</span>
<span class="n">test_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">dataset2</span><span class="p">,</span> <span class="o">**</span><span class="n">test_kwargs</span><span class="p">)</span>
</pre></div>
</div>
<p>et on implémente un autoencodeur :</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">AE</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_dim</span><span class="p">,</span> <span class="n">h_dim1</span><span class="p">,</span> <span class="n">h_dim2</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">AE</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="c1"># encodeur </span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">x_dim</span><span class="p">,</span> <span class="n">h_dim1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">h_dim1</span><span class="p">,</span> <span class="n">h_dim2</span><span class="p">)</span>
        <span class="c1"># decodeur part</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc3</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">h_dim2</span><span class="p">,</span> <span class="n">h_dim1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc4</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">h_dim1</span><span class="p">,</span> <span class="n">x_dim</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">encodeur</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc2</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">x</span>

    <span class="k">def</span> <span class="nf">decodeur</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc3</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc4</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">x</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encodeur</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">decodeur</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>
</pre></div>
</div>
<p>On instantie le modèle</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">AE</span><span class="p">(</span><span class="n">num_input</span><span class="p">,</span> <span class="n">num_hidden_1</span><span class="p">,</span> <span class="n">num_hidden_2</span><span class="p">)</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span>
<span class="n">loss_function</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">()</span>
</pre></div>
</div>
<p>Et on entraîne</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">):</span>
    <span class="n">train_loss</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">_</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">train_loader</span><span class="p">):</span>
        <span class="n">inputs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">data</span><span class="p">,(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">784</span><span class="p">))</span> 
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">recons</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_function</span><span class="p">(</span><span class="n">recons</span><span class="p">,</span> <span class="n">inputs</span><span class="p">)</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">train_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
        
    <span class="k">if</span> <span class="n">i</span><span class="o">%</span><span class="mi">10</span><span class="o">==</span><span class="mi">0</span><span class="p">:</span>    
        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Epoch: </span><span class="si">{}</span><span class="s1"> perte moyenne: </span><span class="si">{:.9f}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">train_loss</span><span class="p">))</span>
</pre></div>
</div>
<p>La figure (<a class="reference internal" href="#ae5b"><span class="std std-numref">Fig. 26</span></a>) montre l’évolution de la reconstruction au cours des itérations.</p>
<figure class="align-center" id="ae5b">
<img alt="_images/ae5b.png" src="_images/ae5b.png" />
<figcaption>
<p><span class="caption-number">Fig. 26 </span><span class="caption-text">Reconstruction aux itérations 0,10,20,30 et 40 d’un sous-ensemble de l’ensemble d’apprentissage.</span><a class="headerlink" href="#ae5b" title="Lien vers cette image">#</a></p>
</figcaption>
</figure>
<p>Pour finalement visualiser les résultats de la reconstruction sur un ensemble de test (<a class="reference internal" href="#ae6"><span class="std std-numref">Fig. 27</span></a>).</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">imshow</span><span class="p">(</span><span class="n">img</span><span class="p">):</span>
    <span class="n">npimg</span> <span class="o">=</span> <span class="n">img</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
    <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">npimg</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">)))</span> 
    <span class="n">ax</span><span class="o">.</span><span class="n">get_xaxis</span><span class="p">()</span><span class="o">.</span><span class="n">set_visible</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">get_yaxis</span><span class="p">()</span><span class="o">.</span><span class="n">set_visible</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>

<span class="n">inputs</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">test_loader</span><span class="p">))</span>
<span class="n">inputs_example</span> <span class="o">=</span> <span class="n">make_grid</span><span class="p">(</span><span class="n">inputs</span><span class="p">[:</span><span class="mi">16</span><span class="p">,:,:,:],</span><span class="mi">4</span><span class="p">)</span>
<span class="n">inputs</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">inputs</span><span class="p">,(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">784</span><span class="p">))</span>

<span class="n">outputs</span><span class="o">=</span><span class="n">model</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
<span class="n">outputs</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">outputs</span><span class="p">,(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">28</span><span class="p">,</span><span class="mi">28</span><span class="p">))</span>
<span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span>
<span class="n">outputs_example</span> <span class="o">=</span> <span class="n">make_grid</span><span class="p">(</span><span class="n">outputs</span><span class="p">[:</span><span class="mi">16</span><span class="p">,:,:,:],</span><span class="mi">4</span><span class="p">)</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="mi">122</span><span class="p">)</span>
<span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">121</span><span class="p">)</span>
<span class="n">fig</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Images originales&quot;</span><span class="p">)</span>
<span class="n">imshow</span><span class="p">(</span><span class="n">inputs_example</span><span class="p">)</span>
<span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">122</span><span class="p">)</span>
<span class="n">fig</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Images reconstruites&quot;</span><span class="p">)</span>
<span class="n">imshow</span><span class="p">(</span><span class="n">outputs_example</span><span class="p">)</span>
</pre></div>
</div>
<figure class="align-center" id="ae6">
<img alt="_images/ae6.png" src="_images/ae6.png" />
<figcaption>
<p><span class="caption-number">Fig. 27 </span><span class="caption-text">Images originales de test (gauche) et reconstruites (droite) par l’autoencodeur.</span><a class="headerlink" href="#ae6" title="Lien vers cette image">#</a></p>
</figcaption>
</figure>
<p>L’Autoencodeur peut ensuite être utilisé par exemple en reconnaissance de chiffres. Une manière simple consiste à choisir au hasard 10 échantillons d’apprentissage de chaque classe et à leur attribuer une étiquette. Ensuite, étant donné les données de test, il est possible de prédire à quelles classes elles appartiennent en trouvant les échantillons d’apprentissage étiquetés les plus similaires dans l’espace latent <span class="math notranslate nohighlight">\(\mathcal H\)</span>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Données d&#39;entraînement</span>
<span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">train_loader</span><span class="p">))</span>
<span class="n">candidates</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">train_batch_size</span><span class="p">,</span> <span class="mi">10</span><span class="o">*</span><span class="mi">10</span><span class="p">)</span>
<span class="n">x_train</span> <span class="o">=</span> <span class="n">x_train</span><span class="p">[</span><span class="n">candidates</span><span class="p">]</span>
<span class="n">y_train</span> <span class="o">=</span> <span class="n">y_train</span><span class="p">[</span><span class="n">candidates</span><span class="p">]</span>

<span class="c1"># Données test à étiqueter</span>
<span class="n">x_test</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">test_loader</span><span class="p">))</span>
<span class="n">candidates</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">test_batch_size</span><span class="p">,</span> <span class="mi">10</span><span class="o">*</span><span class="mi">10</span><span class="p">)</span>
<span class="n">x_test</span> <span class="o">=</span> <span class="n">x_test</span><span class="p">[</span><span class="n">candidates_test</span><span class="p">]</span>
<span class="n">y_test</span> <span class="o">=</span> <span class="n">y_test</span><span class="p">[</span><span class="n">candidates</span><span class="p">]</span>

<span class="c1">#Représentation des données dans l&#39;espace latent</span>
<span class="n">h_train</span><span class="o">=</span><span class="n">model</span><span class="o">.</span><span class="n">encodeur</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">x_train</span><span class="p">,(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">784</span><span class="p">)))</span>
<span class="n">h_test</span><span class="o">=</span><span class="n">model</span><span class="o">.</span><span class="n">encodeur</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">x_test</span><span class="p">,(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">784</span><span class="p">)))</span>

<span class="c1"># Données d&#39;entraînement les plus proches (MSE) de chaque exemple de test</span>
<span class="n">MSEs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">power</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">h_test</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">cpu</span><span class="p">(),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">h_train</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">cpu</span><span class="p">(),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span> <span class="mi">2</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">neighbours</span> <span class="o">=</span> <span class="n">MSEs</span><span class="o">.</span><span class="n">argmin</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">predicts</span> <span class="o">=</span> <span class="n">y_train</span><span class="p">[</span><span class="n">neighbours</span><span class="p">]</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Taux de reconnaissance des chiffres manuscrits sur l&#39;ensemble de test :  </span><span class="si">%.1f%%</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="mi">100</span> <span class="o">*</span> <span class="p">(</span><span class="n">y_test</span> <span class="o">==</span> <span class="n">predicts</span><span class="p">)</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()))</span>
</pre></div>
</div>
<div class="docutils container" id="id2">
<div role="list" class="citation-list">
<div class="citation" id="id18" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id1">1</a><span class="fn-bracket">]</span></span>
<p>D. Kingma and M. Welling. Auto-encoding variational bayes. <em>CoRR</em>, 2013. URL: <a class="reference external" href="http://dblp.uni-trier.de/db/journals/corr/corr1312.html#KingmaW13">http://dblp.uni-trier.de/db/journals/corr/corr1312.html#KingmaW13</a>.</p>
</div>
</div>
</div>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="cnn.html"
       title="page précédente">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">précédent</p>
        <p class="prev-next-title">Réseaux convolutifs</p>
      </div>
    </a>
    <a class="right-next"
       href="rnn.html"
       title="page suivante">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">suivant</p>
        <p class="prev-next-title">Réseaux récurrents</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contenu
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction">Introduction</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#influence-de-la-taille-de-l-espace-de-codage">Influence de la taille de l’espace de codage</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#le-cas-mathcal-h-mathcal-x">Le cas <span class="math notranslate nohighlight">\(|\mathcal{H}|&lt;|\mathcal{X}|\)</span></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#le-cas-mathcal-h-geq-mathcal-x">Le cas <span class="math notranslate nohighlight">\(|\mathcal{H}|\geq|\mathcal{X}|\)</span></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#autoencodeurs-regularises">Autoencodeurs régularisés</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#autoencodeurs-parcimonieux">Autoencodeurs parcimonieux</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#autoencodeurs-contractifs">Autoencodeurs contractifs</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#autoencodeurs-de-debruitage">Autoencodeurs de débruitage</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#autoencodeurs-variationnels">Autoencodeurs variationnels</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#implementation">Implémentation</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
Par Vincent BARRA
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=8d27b9dea8ad943066ae"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=8d27b9dea8ad943066ae"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>